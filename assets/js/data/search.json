[ { "title": "Exploring Parallel Strategies with Jax", "url": "/posts/exploring-parallel-strategies-with-jax/", "categories": "ML Engineering", "tags": "jax, data-parallel, model-parallel, pipeline-paralell, tensor-parallel, mixture-of-experts", "date": "2024-01-27 06:00:00 +0300", "snippet": " Training large language models either like GPT, LlaMa or Mixtral requires immense computational resources. With model sizes ballooning into the billions or sometimes even trillions of parameters, specialized parallelization techniques are essential to make training feasible. In this post, we’ll explore implementing some of these scaling strategies in Jax - a Python framework designed for high-performance numerical computing with support for accelerators like GPU and TPU.Tensors shardingJax is a great fit for implementing parallel LLM training thanks to its high-level APIs for composing parallel functions and its seamless acceleration on GPU/TPU hardware. We’ll walk through code examples for data, tensor, pipeline and expert parallelisms in Jax while training a “toy” FFN model for demonstration. The insights from this exercise will help us understand how state-of-the-art systems actually parallelize and distribute LLM training in practice.Device placementLet’s discover now how to run particular operations on a device of your choice. Don’t worry if you don’t have multiple GPUs, an arbitrary number of devices can be emulated even with single CPU by setting --xla_force_host_platform_device_count flag:import osimport jaximport jax.numpy as jnp# Use 8 CPU devicesos.environ[&quot;XLA_FLAGS&quot;] = &#39;--xla_force_host_platform_device_count=8&#39;With this little trick jax.devices() now shows us eight “different” devices:[CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]Let’s create a small empty tensor x and observe its physical location:batch_size, embed_dim = 16, 8x = jnp.zeros((batch_size, embed_dim))print(x.device()) # will output default device, e.g. TFRT_CPU_0To put tensor x on specific device one can simply use device_put function:jax.device_put(x, jax.devices()[1]).device() # TFRT_CPU_1What if we want to place different parts of our tensor on different devices? There is a technique called tensor sharding: we split x by multiple sub-tensors and place each on its own device. But firstly, we need to create a sharding object, which is basically a device placement configuration:from jax.sharding import PositionalShardingsharding = PositionalSharding(jax.devices())We can split our tensor x in multiple ways. For example, we can split it column-wise along embedding dimension:G = jax.local_device_count()sharded_x = jax.device_put(x, sharding.reshape(1, G))If we print sharded_x.devices() it will give us a list of all devices, which is not very informative since it tells us nothing about our tensor sharding. Luckily, we have visualize_array_sharding function from jax.debug which gives us a pretty visual idea on how x is sharded:from jax.debug import visualize_array_shardingimport matplotlib as mpldef visualize(tensor, color_map=&quot;Set3&quot;): visualize_array_sharding(tensor, color_map=mpl.colormaps[color_map]) visualize(sharded_x)Column-wise sharding of tensor with 8 emulated devices.There are various other ways to shard our tensor: we can split it along batch dimension or, even more, arrange our devices in 4x2 mesh and mesh both axes:Some other ways to shard tensor.Another way to place a tensor on devices that we need to look at before moving forward is tensor replication. Replicating tensor means that several devices will store their own copies of the whole tensor x:replicated_x = jax.device_put(x, sharding.replicate(0))visualize(replicated_x, color_map=&quot;Pastel2_r&quot;)Device placement for replicated tensor $x$.One can also combine sharding with replicating:combined_x = jax.device_put(x, sharding.reshape(2, G // 2).replicate(0))visualize(combined_x)Device placement for sharded and replicated tensor $x$.We will follow the similar way for visualizationVisualization of tensors locations. On the left side - tensor is split column-wise by 4 subtensors, each located on its designated device. On the right side - tensor is copied on 2 devices.Parallel processingLet’s create a simple feed-forward layer (FFN), which is one of the core components in modern LLMs. It consists of two linear layers and an activation function between them. If we omit bias and use ReLU as activation between layers, then FFN can be written as\\[\\operatorname{FFN}(x) = \\max(0, x\\mathbf{W}_1)\\mathbf{W}_2,\\]where $x \\in \\mathbb{R}^{B \\times d}$, $\\mathbf{W}_1 \\in \\mathbb{R}^{d \\times h}$ and $\\mathbf{W}_2 \\in \\mathbb{R}^{h \\times d}$. We will refer to $B$, $d$ and $h$ as to batch size, embedding and hidden dimensions respectively.from jax import jit, randomfrom typing import NamedTuplefrom jax._src.typing import ArrayLikeclass Params(NamedTuple): w1: jnp.ndarray w2: jnp.ndarray@jitdef ffn(x: jnp.array, params: Params): z = jnp.maximum(x @ params.w1, 0) return z @ params.w2Let us also define additional mock functions for data sampling (both features and labels) and FFN weights initialization:def init_ffn_weights(embed_dim: int, hidden_dim: int, rng: ArrayLike): &#39;&#39;&#39; Create FFN weights with Xavier initialization &#39;&#39;&#39; std = jnp.sqrt(2 / (embed_dim + hidden_dim)) w1_key, w2_key = random.split(rng) w1 = std * random.normal(w1_key, (embed_dim, hidden_dim)) w2 = std * random.normal(w2_key, (hidden_dim, embed_dim)) return Params(w1, w2)def sample_data(batch_size: int, embed_dim: int, rng): &#39;&#39;&#39; Create random features `x` and dependable random targets `y` &#39;&#39;&#39; x = random.normal(rng, (batch_size, embed_dim)) w = random.normal(random.PRNGKey(1), (embed_dim, embed_dim)) y = jnp.sin(x @ w) return x, yNow we can run forward pass through FFN layer in Jax simply like this:# set up toy example hyper-parametersB, d, h = 16, 8, 32# create random keysdata_key = random.PRNGKey(0)weight_key = random.PRNGKey(42)x, y = sample_data(B, d, data_key)params = init_ffn_weights(d, h, weight_key)y_pred = ffn(x, params)Here x is stored on one single device and so does y_pred. Recall that x is basically a stack of B features with size d. It means that we can split this stack along batch dimension and send each part to its own device and process them in parallel. And we already know how to accomplish that:sharded_x = jax.device_put(x, sharding.reshape(G, 1))visualize(ffn(sharded_x, params))Data ParallelismData Parallel (DP) is a relatively simple strategy, but it allows scaling to large data batches: the training data is partitioned across $G$ distributed workers and fed to the model, which is replicated. The training process is done in parallel: dataloader spits out a batch of the total size $B$, then each worker computes activations, loss values $\\ell$ and model gradients with its independent data split of size $S=\\frac{B}{G}$. Gradients are then synchronized at the end of each training step before the weights update, so that all workers observe consistent model parameters throughout training.Data Parallel strategy with 4 devices. Embedding and hidden dimensions $d$ and $h$ are equal to 2 and 4 respectively. Each device runs computations with its own separate shard of size $S$ equal to 5Let’s build and example of a regression training loop with data parallelism. First, we build a deep neural network, consisting of $L$ FFN layers with residual connections (to prevent outputs degenerating to zeros due to ReLU activation). We also set up loss criteria and dataset and weights initialization functions.@jitdef model(x: jnp.array, params: Params): for p in params: x += ffn(x, p) return x @jit def criterion(y_pred: jnp.ndarray, y_true: jnp.ndarray): return jnp.mean((y_pred - y_true) ** 2)@jitdef loss_fn(params: Params, x: jnp.ndarray, y: jnp.ndarray): y_pred = model(x, params) return criterion(y_pred, y) def create_dataset(num_samples: int, batch_size: int, embed_dim: int): return jnp.array([ sample_data(batch_size, embed_dim, random.PRNGKey(i)) for i in range(num_samples) ])def init_weights(embed_dim: int, hidden_dim: int, layer_num: int, rng: ArrayLike): &#39;&#39;&#39; Create weights for a stack of `layer_num` FFN layers &#39;&#39;&#39; layer_keys = random.split(rng, layer_num) return [init_ffn_weights(embed_dim, hidden_dim, layer_keys[l]) for l in range(layer_num)]We’ve seen how to perform simple parallel operations manually, e.g. batching a simple FFN forward pass across several devices. JAX also supports automatic device parallelism: we can use jax.pmap to transform a function written for one device into a function that runs in parallel on multiple devices.x = jnp.arange(G * d).reshape(G, d) # dummy sample# replicate model weightsparams = jax.tree_map(lambda p: jnp.tile(p, (G, 1, 1)), params)visualize(jax.pmap(ffn, axis_name=&#39;G&#39;)(x, params))Here’s how pmap works: ffn() takes data tensors of shape [B, ...] and computes the output of FFN layer on that batch. We want to spread the batch dimension across all available devices. To do that, we add a new axis. The arguments to the wrapped ffn() thus need to have shape [G, B/G, ...]. So, to call ffn(), we’ll need to reshape data batches so that what used to be batch is reshaped to [G, B/G, ...]. That’s what split() does below.def split(arr: jnp.ndarray, num_sections: int=None, axis: int=0): return jnp.array(jnp.split(arr, num_sections, axis=axis))Additionally, we’ll need to replicate our model parameters, adding the G axis. This reshaping is how a pmapped function knows which devices to send which data.With all that being said, we still need to send gradient information between the devices. For that, we can use special collective ops such as the jax.lax.p* ops psum, pmean, pmax, etc. In order to use the collective ops we must specify the name of the pmap-ed axis through axis_name argument, and then refer to it when calling the op. Here is a function update(), which runs forward and backward calculations, updates model parameters and all of it is done in parallel on different devices:import functools# Remember that the &#39;G&#39; is just an arbitrary string label used# to later tell &#39;jax.lax.pmean&#39; which axis to reduce over. Here, we call it# &#39;G&#39;, but could have used anything, so long as &#39;pmean&#39; used the same.@functools.partial(jax.pmap, axis_name=&#39;G&#39;)def update(params: Params, x: jnp.ndarray, y: jnp.ndarray): # Compute the gradients on the given minibatch (individually on each device) loss, grads = jax.value_and_grad(loss_fn)(params, x, y) # Combine the gradient across all devices (by taking their mean) grads = jax.lax.pmean(grads, axis_name=&#39;G&#39;) # Also combine the loss. Unnecessary for the update, but useful for logging loss = jax.lax.pmean(loss, axis_name=&#39;G&#39;) # Each device performs its own update, but since we start with the same params # and synchronise gradients, the params stay in sync LEARNING_RATE = 1e-3 new_params = jax.tree_map( lambda param, g: param - g * LEARNING_RATE, params, grads) return new_params, lossDuring the update step, we need to combine the gradients computed by each device – otherwise, the updates performed by each device would be different. That’s why we use jax.lax.pmean to compute the mean across the G axis, giving us the average gradient of the batch. That average gradient is what we use to compute the update.Combining all together, we can now create a full train cycle with data parallel strategy:def train_with_data_parallel(dataset, params, num_epochs): G = jax.local_device_count() # replicate model weights replicated_params = [ jax.tree_map(lambda param: jnp.tile(param, (G, 1, 1)), p) for p in params ] for epoch in range(num_epochs): avg_loss = 0 for (x, y) in tqdm(dataset, leave=False): # shard data batch x, y = split(x, G), split(y, G) replicated_params, loss = update(replicated_params, x, y) # note that loss is actually an array of shape [G], with identical # entries, because each device returns its copy of the loss # visualize(loss) will show [CPU 0, CPU 1, ..., CPU G] avg_loss += loss.mean().item() if (epoch + 1) % 5 == 0: print(f&quot;Step {epoch + 1:3d}, loss: {avg_loss / dataset.shape[0]:.3f}&quot;) return replicated_paramsWe only need to set hyper-parameters, define training dataset and initialize weights to finally call the training function.# set G = 4 devicesos.environ[&quot;XLA_FLAGS&quot;] = &#39;--xla_force_host_platform_device_count=4&#39;num_epochs = 50num_samples = 500B, d, h, L = 20, 2, 4, 16dataset = create_dataset(num_samples, B, d)print(&#39;Dataset size:&#39;, dataset.shape) # [N, 2, B, d]params = init_weights(d, h, L, random.PRNGKey(42))train_with_data_parallel(dataset, params, num_epochs)And we can observe the process of our model being trained:Dataset size: (500, 2, 20, 2)Step 5, loss: 0.233Step 10, loss: 0.184Step 15, loss: 0.127Step 20, loss: 0.117Step 25, loss: 0.111Step 30, loss: 0.106Step 35, loss: 0.103Step 40, loss: 0.100Step 45, loss: 0.098Step 50, loss: 0.097The main problem with DP approach is that during the backward pass all the gradients must be transferred to the all other devices. For example, with $\\mathbf{W}_1 \\in \\mathbb{R}^{d \\times h}$ weight matrix in float32 the number of $32 \\cdot d \\cdot h$ bits have to be sent between each pair of devices. The same amount is needed for $\\mathbf{W}_2$. If we work in a multi-node setup with $v$ GBit/s of network card bandwidth, we’ll need\\[t = \\frac{64 \\cdot d \\cdot h}{v \\cdot 1024^3}\\]seconds to send the gradients for FFN layer from one node to another (plus an additional overhead $\\delta t$ that is neglected here). Given the substantial amount of data communication required in DP, a fast connection (interconnect) between computing devices is necessary. While DP may work for TPU device networks scaling up to pod levels, modern GPUs predominantly have fast interconnectivity only within a group of 8 devices hosted on the same system. Inter-GPU communication is considerably slower across separate hosts.There is an example: imagine that we use data parallelism for LLM pretraining and we need to give a rough estimation of the time required for backward calculations. In GPT-3 the embedding size $d$ is 12⋅1024 with hidden size $h=4d$. Microsoft built a supercomputer exclusively for OpenAI with 10,000 GPUs and 400 GBit/s of network connectivity between nodes. Plugging in these numbers we get\\[t = \\frac{64 \\cdot 12 \\cdot 4 \\cdot 12 \\cdot 1024^2}{400 \\cdot 1024^3} = 90\\mbox{ms}\\]just to transfer FFN gradients. As there are 96 FFN layers in GPT-3, it’ll take about 9 seconds for this part of gradient synchronization. And this is just to send data from one node to another, while there might be dozens, hundreds or even thousands nodes with all-to-all communication cost growing quadratically. Easily we can see that data parallelism does not scale with size of the cluster and cannot be used in isolation for large models.The described strategy above is also called Distributed Data Parallel (DDP) and it is different from HuggingFace definition of data paralelism. HuggingFace version of DP helps to overcome slow intra-node connectivity by minimizing the amount of synchronized data and delegating a lot of data/gradient processing to one leading GPU. This, in turn, results in under-utilization of other devices.Another common strategy for amortizing communication cost is gradient accumulation. We can run multiple forward and backward propagations and accumulate local gradients on each device in parallel before launching data synchronization and taking optimizer step. Additionally, performance can be improved by synchronizing the computed gradients for some tensors while simultaneously computing gradients for anothers.Model parallelismWith the advent of large neural networks that do not fit on one device, the need to parallelize models has increased. This is especially true in the case of LLMs, whose number of parameters can exceed the already considerable amount of input data. To distribute computation and memory we can split the model across multiple devices and across multiple dimensions.Tensor ParallelismThe idea of sharding, the way it was applied to data tensors, can be used in a similar way with respect to the model weights. We can divide each tensor $\\mathbf{W}$ into chunks distributed across multiple devices, so instead of having the whole tensor reside on a single device, each shard of the tensor resides on its own accelerator. Each part gets processed separately in parallel on different devices and after processing the results are synced at the end of the step.Such strategy is called Tensor Parallel (TP) or horizontal parallelism, as the splitting happens on horizontal level (we will get to the vertical/pipeline parallelism later). A simple and efficient way to parallelize FFN calculations was proposed in Megatron-LM paper. Let’s represent matrices $\\mathbf{W}_1$ and $\\mathbf{W}_2$ as concatenation of $G$ sub-tensors along rows and columns respectively:\\[\\mathbf{W}_1 = \\begin{pmatrix} \\color{#8ED3C7}{\\mathbf{W}_1^1} &amp;amp; \\color{#D9D9D9}{\\mathbf{W}_1^2} &amp;amp; \\cdots &amp;amp; \\color{#FDBFB9}{\\mathbf{W}_1^G} \\end{pmatrix}, \\quad \\mathbf{W}_2 = \\begin{pmatrix} \\color{#8ED3C7}{\\mathbf{W}_2^1} \\\\ \\color{#D9D9D9}{\\mathbf{W}_2^2} \\\\ \\vdots \\\\ \\color{#FDBFB9}{\\mathbf{W}_2^G} \\end{pmatrix}\\]with $\\mathbf{W}_1^k \\in \\mathbb{R}^{d \\times \\frac{h}{G}}$, $\\mathbf{W}_2^k \\in \\mathbb{R}^{\\frac{h}{G}\\times d}$ for $k = 1, \\dots, G$. Then with $x$ replicated over devices we can perform sub-tensors multiplications in parallel:\\[x\\mathbf{W}_1=\\begin{pmatrix} x\\color{#8ED3C7}{\\mathbf{W}_1^1} &amp;amp; x\\color{#D9D9D9}{\\mathbf{W}_1^2} &amp;amp;\\cdots &amp;amp; x\\color{#FDBFB9}{\\mathbf{W}_1^G} \\end{pmatrix}\\]and for $z = \\max(x\\mathbf{W}_1, 0)$ we have\\[z\\mathbf{W}_2=z {\\color{#8ED3C7}{\\mathbf{W}_2^1}} + z {\\color{#D9D9D9}{\\mathbf{W}_2^2}} + \\cdots + z {\\color{#FDBFB9}{\\mathbf{W}_2^G}}.\\]The model weights on different devices do not overlap, and the only communication between devices occurs at the end of the FFN layer when we need to sum all the outputs. We can already see the advantage of TP over DP here: computational costs for each device decreases drastically with growing number of devices. On the other hand, the deficiency of TP is that the input data is replicated, so that the batch size per device is now equal to the total batch size. Hence if we are restricted by GPU memory we have to reduce our $B$. Otherwise, we can increase our $S$ by a factor of $G$ to keep up with the same batch size $B=S \\times G$ as in DP strategy.Tensor Parallel strategy with batch size $B$ set to 5. Each device stores its own part of model weights. Activations are aggregated after each FFN layer.def train_with_tensor_parallel(dataset, params, num_epochs): G = jax.local_device_count() sharded_params = [ Params(w1=split(p.w1, num_sections=G, axis=1), w2=split(p.w2, num_sections=G, axis=0)) for p in params ] for epoch in range(num_epochs): avg_loss = 0 for (x, y) in tqdm(dataset, leave=False): # replicate data batch x, y = jnp.array([x] * G), jnp.array([y] * G) sharded_params, loss = update(sharded_params, x, y) avg_loss += loss.mean().item() if (epoch + 1) % 5 == 0: print(f&quot;Step {epoch + 1:3d}, loss: {avg_loss / dataset.shape[0]:.3f}&quot;) return sharded_paramsSince in TP the size of synchronized weights equals to the batch size $B$ multiplied by embedding size $d$, when we operate with float32, each device sends $32 \\cdot d \\cdot B$ bits to each other device. Thus, the amount of memory transfer between each pair of devices is $O(dh)$ for DP versus $O(dB)$ for TP. We can conclude, that DP is a preferable strategy for small networks (e.g. model can fit onto one device), while TP works better with larger models and smaller batches.Hybrid data and model tensor parallelismIt is common to mix both data and tensor parallelism for large scale models. With a total of $G=\\operatorname{TP}\\times \\operatorname{DP}$ devices, each device stores $\\frac{B}{\\operatorname{DP}}$ embedding vectors and $\\frac{h}{\\operatorname{TP}}$ of both the weights and intermediate activations.def train_with_hybrid_parallel(dataset, params, num_epochs, DP, TP): sharded_params = [ Params(w1=split(p.w1, num_sections=TP, axis=1), w2=split(p.w2, num_sections=TP, axis=0)) for p in params ] hybrid_params = [ jax.tree_map(lambda param: jnp.tile(param, (DP, 1, 1)), p) for p in sharded_params ] for epoch in range(num_epochs): avg_loss = 0 for (x, y) in tqdm(dataset, leave=False): # shard and then replicate data batch x, y = split(x, DP), split(y, DP) x, y = jnp.repeat(x, TP, axis=0), jnp.repeat(y, TP, axis=0) hybrid_params, loss = update(hybrid_params, x, y) avg_loss += loss.mean().item() if (epoch + 1) % 5 == 0: print(f&quot;Step {epoch + 1:3d}, loss: {avg_loss / dataset.shape[0]:.3f}&quot;) return hybrid_paramsPipeline ParallelismSuppose our neural network, a stack of $L$ FFN layers, is so deep, that it doesn’t fit on a single device. This scenario is practical because a common way to scale up models is to stack layers of the same pattern. It might feel straightforward for us to split our model by layer and that is what Pipeline Parallel (PP) strategy does. It splits up the model weights vertically, so that only a small group of consecutive layers of the model are placed on a single device.Naive Pipeline Parallel strategy. Each stage represent forward/backward pass through its own sequence of $\\frac{L}{G}$ FFN layers on each device. It can be seen that running every data batch through multiple workers with sequential dependencies leads to large idle bubbles and severe under-utilization of computation resources.To implement naive PP strategy in Jax we just have to place layers to their corresponding devices and whenever the data goes in each layer it must be switched to the same device.Clearly, the main disadvantage of this type of parallelization is that all but one device is idle at any given moment. In addition, at the end of each stage there is a serious communication overhead for transferring data between devices. To reduce idling problem we have to explore other approaches.Pipeline Parallel reduced to tensor shardingSince our model only consists of $L$ equal (not shared) layers, when we split it vertically, our pipelining scenario looks like $G$ stages of the same subcomputation except for having different weight values. The similar picture we could’ve seen in TP strategy - every device is doing the same operations but with different operands.Let’s reorganize the way we look at our model architecture. If we stack weights from different stages and represent them all together as tensors $\\mathbf{W}_1 \\in \\mathbb{R}^{G \\times d \\times h}$ and $\\mathbf{W}_2 \\in \\mathbb{R}^{G \\times h \\times d}$ in each FFN layer, we can shard them across stacked axis, so that these stages can be run in parallel, but with different batches. Although, $k$-th stage still have to wait until output activations from $(k-1)$-th stage are calculated and transferred to the $k$-th device. It means that first $k$ runs for $k$-th stage must be done with emulated input data, e.g. filled with zeros or random values.Pipeline Parallel inference with inter-batch parallelism. White cells represent zero-paddings.The extra iterations are equivalent to the bubbles that describe the idle time due to data dependency, although the waiting devices compute on padded data instead of being idle. One can notice that if we split our batch into multiple micro-batches and enable each stage worker to process one micro-batch simultaneously, idle bubbles become much smaller, compared to naive PP.def stack_stage_weights(params: list): &#39;&#39;&#39; Stack G stages, each containing L/G FFN layers &#39;&#39;&#39; L = len(params) G = jax.local_device_count() stage_layers = L // G out_params = [] for l in range(stage_layers): w1 = jnp.stack([params[l + g * stage_layers].w1 for g in range(G)]) w2 = jnp.stack([params[l + g * stage_layers].w2 for g in range(G)]) out_params.append(Params(w1, w2)) return out_paramsdef pipeline_inference(params: list[Params], x: jnp.ndarray, M: int): &#39;&#39;&#39; Split input batch to M micro-batches and run PP forward pass &#39;&#39;&#39; B = x.shape[0] micro_batch_size = B // M # re-organize weights params = stack_stage_weights(params) # split input data to micro-batches x = split(x, M) # create shifting buffer state = jnp.zeros((G, micro_batch_size, d)) y_pred = [] for i in range(M + G - 1): from_prev_stage = jnp.concatenate([jnp.expand_dims(x[i], 0), state[:-1]]) state = jax.pmap(model)(from_prev_stage, params) if i &amp;gt;= G - 1: # first micro-batch has passed through the last stage y_pred.append(state[-1]) return jnp.array(y_pred).reshape(B, d)This is the main idea in GPipe (Huang et al. 2019) paper. During training stage, backward calculations are scheduled in reverse order. Gradients from multiple micro-batches are aggregated and applied synchronously at the end, which guarantees learning consistency and efficiency. Given $M$ evenly split micro-batches, the idle time is $O \\big( \\frac{G - 1}{M + G - 1} \\big)$ amortized over the number of micro-steps.To reduce memory footprint gradient checkpointing can be applied, meaning that during forward computation, each device only stores output activations at the stage boundaries. During the backward pass on $k$-th device the $k$-th stage forward pass re-computes the rest of activations. While it doubles time, required for forward calculations, it helps to reduce peak activation memory requirement to $O \\big(B + \\frac{L}{G} \\times \\frac{B}{M}\\big)$. In comparison, memory requirement without PP and gradient checkpointing would be $O(B \\times L)$, since computing the gradients requires both the next layer gradients and the cached activations.Expert ParallelismWith Mixture-of-Experts (MoE) models, different sub-networks (FFN layers in our case) or so-called “experts” specialize in different parts of the input space. For example, in a language model, some experts may specialize in grammar while others focus on semantic understanding. The key to a mixture of experts is having a gating network $\\mathcal{G}$ that assigns different parts of each input to the most relevant experts.During training, only the experts assigned to a given input have their parameters updated. This sparse update allows mixture of experts models to scale to thousands or even tens of thousands of experts. Each expert can be updated in parallel by a different set of accelerators without heavy communication overhead.Mixture of Expert RoutingMoE layer was proposed by Shazeer et al. (2017). It takes a token representation $x$ and then routes it through gating network $\\mathcal{G}$ to determined experts. Say, we have $E$ experts in total, then the output of the MoE layer $y$ is the linearly weighted combination of each expert’s output by the gate value\\[y = \\sum_{i=1}^E \\mathcal{G}(x)_i \\cdot \\operatorname{FFN}_i(x).\\]The simple choice of gating function is to create trainable weight matrix $\\mathbf{W}_G$ to produce logits, which are normalized via a softmax distribution over the available experts at that layer:\\[\\mathcal{G}(x)=\\operatorname{softmax}(x\\mathbf{W}_{\\text{G}}).\\]def dense_gating(x: jnp.ndarray, gate_params: jnp.ndarray): return jax.nn.softmax(x @ gate_params)However, this choice raises two problems: MoE layer with dense control vector $\\mathcal{G}(x)$ requires computation of all $E$ experts, even those whose impact to the output may be negligible. It would be more efficient if we didn’t have to compute $\\operatorname{FFN}_i(x)$ when $\\mathcal{G}(x)_i=0$. Self-reinforcing effect: gating network might favor a few strong experts everytime, leaving the rest of the layers redundant.To overcome these issues we introduce sparsity through $\\operatorname{topk}$ function and noise via standard Gaussian variable $\\epsilon \\sim \\mathcal{N}(0, 1)$. The amount of noise per component is controlled by a second trainable weight matrix $\\mathbf{W}_{\\text{noise}}$. Modified gating function would like like this:\\[\\mathcal{G}(x)=\\operatorname{softmax}(\\operatorname{topk}(H(x), k)),\\]where\\[H(x)_i = (x\\mathbf{W}_{\\text{G}})_i + \\epsilon \\cdot \\operatorname{softplus}(x \\mathbf{W}_{\\text{noise}})_i\\]and\\[\\operatorname{topk}(v, k)_i = \\begin{cases} v, &amp;amp; \\text{if } v_i \\text{ is in top } k \\text{ elements of } v \\\\ -\\infty, &amp;amp; \\text{otherwise.} \\end{cases}\\]def scatter(input: jnp.ndarray, dim: int, index: jnp.ndarray, src: int): &#39;&#39;&#39; Scatter function analogous to PyTorch `scatter_` &#39;&#39;&#39; idx = jnp.meshgrid(*(jnp.arange(n) for n in input.shape), sparse=True, indexing=&#39;ij&#39;) idx[dim] = index return input.at[tuple(idx)].set(src)def index_to_mask(index: jnp.ndarray, input_shape: tuple): &#39;&#39;&#39; Transform given indices to mask of input shape, where mask[index] = True and False otherwise &#39;&#39;&#39; zeros = jnp.zeros(input_shape, dtype=bool) return scatter(zeros, 1, index, True)def sparse_gating(x: jnp.ndarray, gate_params: jnp.ndarray, topk: int, noise_weights: jnp.ndarray=None, rng: ArrayLike=None): h = x @ gate_params if noise_weights is not None: assert rng is not None, &quot;Random seed is required to use noisy gating&quot; eps = random.normal(rng, h.shape) noise = eps * jax.nn.softplus(x @ noise_weights) h += noise _, top_k_ids = jax.lax.top_k(h, topk) mask = index_to_mask(top_k_ids, h.shape) h = jnp.where(mask, h, -jnp.inf) return jax.nn.softmax(h)To help load-balancing and to avoid collapse to using a small number of experts an auxiliary importance loss was proposed. Let’s define an importance of an expert $i$ relative to the batch $\\mathcal{B}$ as batchwise sum of the gate values for that expert: $\\sum_{x \\in \\mathcal{B}} \\mathcal{G}(x)_i$. Importance loss minimizes the squared coefficient of variation of importance over experts:\\[\\ell_{\\text{aux}}(\\mathcal{B}) = \\operatorname{CV} \\big( \\sum_{x \\in \\mathcal{B}} \\mathcal{G}(x) \\big)^2.\\]Such constraint encourages all experts to have equal importance values.Another important detail is that since each expert network receives only a portion of the training samples, we should try to use as large a batch size as possible in MoE. To improve the throughput MoE can be combined with DP strategy.GShardAdditional experts increase the amount of model parameters significantly. Basically MoE layer requires $E$ times more parameters than a single FFN layer (plus gating tensor $\\mathbf{W}_{\\text{G}}$). But what if we had each expert reside on its own device? GShard (Lepikhin et al., 2020) uses the idea of sharding across expert dimension to scale up the MoE transformer model up to 600B parameters. The MoE transformer replaces every other FFN with a MoE layer. All MoE layers are different across devices, while other layers are duplicated.Illustration of scaling of Transformer Encoder with MoE Layers. (a) The encoder of a standard Transformer model is a stack of self-attention and feed forward layers interleaved with residual connections and layer normalization. (b) By replacing every other feed forward layer with a MoE layer, we get the model structure of the MoE Transformer Encoder. (c) When scaling to multiple devices, the MoE layer is sharded across devices, while all other layers are replicated.Authors chose to let each token $x$ dispatched to at most two experts. Besides that, there are several improvements for the gating function in GShard: To ensure that expert load is balanced, the number of tokens processed by one expert is restricted by threshold $C$ named expert capacity. If a token $x$ is routed to an expert that has reached its capacity, the token be considered overflowed and gating output $\\mathcal{G}(x)$ degenerates into a zero vector. Such token has its representation $x$ passed on to the next layer via residual connection. Local group dispatching: $\\mathcal{G}(\\cdot)$ partitions all tokens in a training batch evenly into $G$ groups, each of size $S$ and the expert capacity is enforced on the group level. Random routing: since MoE layer output $y$ is a weighted average, if the 2nd best expert gating weight is small enough, we can skip expert computation. To comply with the capacity constraint, token is dispatched to the expert with with a probability proportional to its weight. Auxiliary loss to avoid experts under-utilization is modified. Let $p(x)$ be dense gating function:\\[p(x) = \\operatorname{softmax}(x\\mathbf{W}_{\\text{G}}).\\]Also let $f_i$ be the fraction of tokens in a group $\\mathcal{S}$ dispatched to expert $i$:\\[f_i = \\frac{1}{S} \\sum_{x \\in \\mathcal{S}} \\mathbb{1}_{ \\lbrace \\operatorname{argmax} \\mathcal{G}(x) = i \\rbrace },\\]The goal is to minimize mean squared ratio of tokens per expert: $\\frac{1}{E} \\sum_{i=1}^E f_i^2$. But since this value is derived from $\\operatorname{topk}$ function, it’s non-differentiable, so authors propose to use mean gating weights as differentiable approximation:\\[\\bar{p}_i = \\frac{1}{S} \\sum_{x \\in \\mathcal{S}} p(x)_i \\approx f_i.\\]Then $\\ell_{\\text{aux}} = \\sum_{i=1}^E f_i \\cdot \\bar{p}_i$.Switch TransformerSwitch Transformer scales the model size even more, up to 1.6 trillion parameters, by replacing FFN layer with a sparse MoE layer in which each input is routed to only one expert network. Authors refer to such routing strategy as a Switch layer. Similar to how it is done in GShard each expert has its capacity $C$, which depends on batch size $B$ and number of experts $E$ by formula\\[C = \\frac{B}{E} \\cdot c,\\]where $c$ is a capacity factor. A capacity factor greater than 1.0 creates additional buffer to accommodate for when tokens are not perfectly balanced across experts.Each token $x$ is routed to the expert with the highest router probability $p(x)$, but each expert has a fixed batch size $C$. Smaller capacity factor can lead to experts overflow, while larger factor increases computation and communication costs.Let’s dive into implementation details and describe step-by-step how to combine Switch layer with Data Parallel strategy. Switch Transformer is allocated on $G$ devices, which will also correspond to the number of experts $E$. For each token per device gating function locally computes assignments to the experts.# Probabilities for each token of what expert it should be sent to.# gating_probs shape: [G, S, E]gating_probs = jax.nn.softmax(x @ gate_params)# Get the top−1 expert for each token. # expert_gate is the probability from the gating to top-1 expert# expert_index is what expert each token is going to be routed to# expert_gate shape: [G, S]# expert_index shape: [G, S]expert_gate, expert_index = jax.lax.top_k(gating_probs, 1)expert_gate, expert_index = expert_gate.squeeze(), expert_index.squeeze() The output is a dispatch mask, a 4-D binary tensor of shape $[G, S, E, C]$, which is partitioned across the first dimension and determines expert assignment. On each device $g$ for each token $s$ 2-D slice of dispatch mask contains at most one non-zero element.# expert_mask shape: [G, S, E]expert_mask = jax.nn.one_hot(expert_index, num_classes=gating_probs.shape[2]) # Experts have a fixed capacity C, ensure we do not exceed it. # Construct the batch indices, to each expert, with position in expert# make sure that not more that C examples can be routed to each expert.position_in_expert = jnp.cumsum(expert_mask, axis=1) * expert_mask # Keep only tokens that fit within expert capacity.expert_mask_trunc = expert_mask * jnp.less(position_in_expert, expert_capacity)expert_mask_flat = jnp.sum(expert_mask_trunc, axis=2)# Mask out the experts that have overflowed the expert capacity.expert_gate *= expert_mask_flat# combine_tensor used for combining expert outputs and scaling with gating probability.# combine_tensor shape: [G, S, E, C]expert_capacity_int = int(jnp.ceil(expert_capacity))combine_tensor = (expert_gate[..., None, None] * expert_mask[..., None] * jax.nn.one_hot(position_in_expert, num_classes=expert_capacity_int))combine_tensor = combine_tensor[..., 1:] # cut 0-dimension which is always 0sdispatch_mask = combine_tensor.astype(bool) This mask is then used to do a gather via Einstein summation with the partitioned input tensor $x$ of size $[G, S, d]$, resulting in the final tensor of shape $[E, G, C, d]$, which is sharded across second dimension. Because each device has its own expert, we do an all-to-all communication of size $[E, C, d]$ to now shard the $E$ dimension instead of the $G$-dimension.# Matmul with large boolean tensor to assign tokens to the correct expert.# device layout: [G, 1, 1], −&amp;gt; [1, G, 1, 1]# expert inputs shape: [E, G, C, d]expert_inputs = jnp.einsum(&quot;GSEC,GSd-&amp;gt;EGCd&quot;, dispatch_mask, x)# All−to−All communication. Cores split across G and now we want to split# across E. This sends tokens, routed locally, to the correct expert now# split across different cores.# device layout: [1, G, 1, 1] −&amp;gt; [G, 1, 1, 1]sharding = PositionalSharding(jax.devices())expert_inputs = jax.device_put(expert_inputs, sharding.reshape(G, 1, 1, 1)) Next, we run experts computation with re-sharded inputs and perform all-to-all communication once again to shard expert outputs back along $G$ dimension.# Standard FFN computation, where each expert has# its own unique set of parameters.# Total unique parameters created: E * (d * h * 2).# expert_outputs shape: [E, G, C, d]expert_outputs = ffn(expert_inputs, ffn_params) # All−to−All communication. Cores are currently split across the experts# dimension, which needs to be switched back to being split across num cores.# device layout: [G, 1, 1, 1] −&amp;gt; [1, G, 1, 1]expert_outputs = jax.device_put(expert_outputs, sharding.reshape(1, G, 1, 1)) And finally, to get $y$, we average expert outputs based on gating probabilities:# Convert back to input shape and multiply outputs of experts by the gating probability# expert_outputs shape: [E, G, C, d]# expert_outputs_combined shape: [G, S, d]# device layout: [1, G, 1, 1] −&amp;gt; [G, 1, 1]expert_outputs_combined = jnp.einsum(&quot;EGCd,GSEC-&amp;gt;GSd&quot;, expert_outputs, combine_tensor)A schematic representation of top-1 expert parallel dispatching with data parallel and batch size per device $S=5$. Number of experts $E$ is equal to the number of devices $G$ and capacity factor $c$ is set to 2 (therefore expert capacity $C = \\frac{S}{E} \\cdot c = 2.5$). Any white cell represents zero element. Vectors associated with the first token placed on GPU-1 (initially) are shaded as an example so that the flow of elements in the image can be easily followed. Note that the embedding of this token was dispatched to the last expert by Gating and therefore put to the last device accordingly.The auxiliary loss $\\ell_{\\text{aux}}$ is similar to GShard, except that $f_i$ is derived through dense gating function and aggregated over whole batch:\\[f_i = \\frac{1}{B} \\sum_{x \\in \\mathcal{B}} \\mathbb{1}_{ \\lbrace \\operatorname{argmax} \\mathcal{p}(x) = i \\rbrace }.\\]The same goes for $\\bar{p}_i$.def load_balance_loss(gating_probs, expert_mask): &#39;&#39;&#39; Calculate load−balancing loss to ensure diverse expert routing. &#39;&#39;&#39; # gating probs is the probability assigned for each expert per token # gating probs shape: [G, S, E] # expert index contains the expert with the highest gating # probability in one−hot format # expert mask shape: [G, S, E] # For each core, get the fraction of tokens routed to each expert # density_1 shape: [G, E] density_1 = jnp.mean(expert_mask, axis=1) # For each core, get fraction of probability mass assigned to each expert # from the router across all tokens. # density_1_proxy shape: [G, E] density_1_proxy = jnp.mean(gating_probs, axis=1) # density_1 for a single device: vector of length E that sums to 1. # density_1_proxy for a single device: vector of length E that sums to 1. # Want both vectors to have uniform allocation (1/E) across all E elements. # The two vectors will be pushed towards uniform allocation when the dot product # is minimized. loss = jnp.mean(density_1_proxy * density_1) * (density_1.shape[-1] ** 2) return lossFull code for Switch Layer can be found here.Mixtral of ExpertsRecently an open-source language model called Mixtral 8x7B was introduced in Mixture of Experts paper and is claimed to outperform Llama-2 70B and GPT-3.5 on many benchmarks. As the name suggests, inference requires running only through 7B parameters, which is possible thanks to 8 distinct experts for each layer. For Mixtral 8x7B authors use deterministic sparse gating function, routing to top 2 experts:\\[\\mathcal{G}(x) = \\operatorname{softmax}(\\operatorname{topk} (x\\mathbf{W}_{\\text{G}}, 2)).\\]Another key point is that the SwiGLU layer is chosen as the expert function. It was introduced by Noam Shazeer in GLU Variants Improve Transformers and has two main differences from standard FFN layer. The first one is Swish (also called SiLU) activation function instead of ReLU:\\[\\operatorname{Swish}(x) = x \\sigma (x) = \\frac{x}{1+e^{-x}}.\\]The second one is the gating mechanism, Gated Linear Unit (GLU), introduced by Dauphin et al., 2016:\\[\\operatorname{SwiGLU}(x) = \\big(\\operatorname{Swish}(x \\mathbf{W}_1) \\otimes x \\mathbf{V} \\big)\\mathbf{W}_2,\\]where $\\otimes$ is the element-wise product between matrices.class SwiGLUParams(NamedTuple): w1: jnp.ndarray w2: jnp.ndarray v: jnp.ndarray@jitdef swiglu(x: jnp.ndarray, params: SwiGLUParams): y = x @ params.v z = jax.nn.swish(x @ params.w1) return (z * y) @ params.w2Routing analysis: each token is colored with the first expert choice in Mixtral 8x7B. Authors notice that the selection of experts appears to be more aligned with the syntax rather than the domain, especially at the initial and final layers.Strategies worth considering but beyond the scope of this postZero Redundancy Optimizer (ZeRO) - it also performs sharding of the tensors somewhat similar to TP, except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesn’t need to be modified. It also supports various offloading techniques to compensate for limited GPU memory.Fully sharded data parallel (FSDP) - is a type of data parallel training, but unlike traditional DP strategy, which maintains a per-GPU copy of a model’s parameters, gradients and optimizer states, it shards all of these states across data parallel workers and can optionally offload the sharded model parameters to CPUs.ConclusionTraining ever-larger neural networks requires creative parallelization techniques to distribute computation and memory efficiently across accelerators. In this post, we explored some of the predominant strategies used today like data, tensor, pipeline, and mixture-of-experts parallelism.While simple data parallelism can work for smaller models, combining it with model parallel approaches becomes essential to scale up to the massive architectures used in LLMs. Each strategy makes tradeoffs between computation efficiency, communication overheads, and implementation complexity.Hybrid schemes are often needed in practice, tailored to optimize parallelism for a specific model architecture. As models continue growing in size, new innovations in efficient distributed training will be crucial to unlock further breakthroughs in AI. The insights from this post can guide decisions on parallelization strategies when training large neural networks.Sources: Parallelization with Jax Parallel evaluation in Jax Distributed arrays and automatic parallelization in Jax Data/model parallel strategies: Megatron-LM (Shoeybi et al. 2020) Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B (Smith et al. 2022) GSPMD: General and Scalable Parallelization for ML Computation Graphs (Xu et al. 2021) GPipe: Easy Scaling with Micro-Batch PipelineParallelism (Huang et al. 2019) How to Parallelize Deep Learning on GPUs. Part 1: Data Parallelism Part 2: Model Parallelism Mixture of Experts: Sparse MoE Layer (Shazeer et al. 2017) GShard (Lepikhin et al. 2020) Switch Transformers (Fedus et al. 2021) Supplementary code on GitHub" }, { "title": "Building Aligned Intelligence System. Part II. Improving Large Language Models", "url": "/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/", "categories": "Generative AI, Large Language Models", "tags": "prompt engineering, chain-of-thought, tree of thoughts, peft, prompt-tuning, prefix-tuning, lora, ia3, talm, toolformer", "date": "2023-07-23 19:00:00 +0300", "snippet": " In this post we will look at different techniques for steering LLMs behaviour to get desired outcomes, starting with some basic general principles such as writing a good prompt and ending with fine-tuning and augmenting models with external knowledge. Methods discussed in this post are mainly aimed at improving LLMs reliability and ensuring the consistency and factual accuracy of their outputs.Prompt designSmart prompt design essentially produces efficient context that can lead to desired completion. Such approach is important, because it does not require to change model weights and with a single model checkpoint one can perform many tasks. The general advice for writing a smart prompt is to start simple and iteratively adding more elements and context as you aim for better results.Although some of the techniques are specific to certain types of problems, many of them are built upon general principles that can be applied to a wide range of tasks. Give specific instructions. If you want it to say “I don’t know” when it doesn’t know the answer, tell it ‘Say “I don’t know” if you do not know the answer.’ Split complex tasks into simpler subtasks. Supply high quality and diverse examples. Generate multiple possible answer, ask for justifications and then pick the best one. Prompt the model to explain before answering. Ask it to write down the series of steps explaining its reasoning.Chain-of-ThoughtIn previous part we saw such techniques as zero-shot, when user gives a direct instruction, and few-shot, when instruction is followed by examples in the prompt. But this simple methods could be still unreliable on tasks that require reasoning abilities. As an example, if you simply ask text-davinci-002 (GPT-3.5 model trained with SFT) the arithmetic problem, in most cases the answer will be wrong.Standard few-shot example:Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each canhas 3 tennis balls. How many tennis balls does he have now?A: The answer is 11.Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?A:The answer is 8.A technique known as chain-of-thought (CoT) introduced in Wei et al. (2022) is to prompt the model to explain step-by-step how it arrives at an answer.CoT few-shot example:Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each canhas 3 tennis balls. How many tennis balls does he have now?A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?A:The juggler can juggle 16 balls. Half of the balls are golf balls. So there are 16 / 2 = 8 golf balls. Half of the golf balls are blue. So there are 8 / 2 = 4blue golf balls. The answer is 4.Tests with PaLM 540B on GSM8K benchmark have shown that chain-of-thought outperforms few-shot prompting by a large margin: 57% solving rate on math problems, compared to near 18% with standard prompting. In addition to math problems, CoT also lifted performance on questions related to sports understanding, coin flip tracking, and last letter concatenation.CoT enables complex reasoning capabilities, but it comes with a price: the increase in both latency and cost due to the increased number of input and output tokens. Also, most prompt examples are task-specific and require extra effort to generate. A recent technique, called zero-shot CoT (Kojima et al. 2022), simplifies CoT by essentially adding “Let’s think step by step” to the original prompt without providing any examples:Zero-shot CoT example:Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?A: Let’s think step by step.There are 16 balls in total. Half of the balls are golf balls. That means thatthere are 8 golf balls. Half of the golf balls are blue. That means that there are 4 blue golf balls.On GSM8K benchmark the “Let’s think step by step” trick raised solving rate up to 41% with InstructGPT. Similar magnitudes of improvements have been acheived with PaLM 540B as well. At the same time, while this trick works on math problems, it’s not effective in general. The authors found that it was most helpful for multi-step arithmetic problems, symbolic reasoning problems, strategy problems, and other reasoning problems. It didn’t help with simple math problems or common sense questions, and presumably wouldn’t help with many other non-reasoning tasks either.Also, if you apply this technique to your own tasks, don’t be afraid to experiment with customizing the instruction. “Let’s think step by step” is rather generic prompt, so you may find better performance with instructions that hew to a stricter format customized to your use case.Self-consistencyThe idea of self-consistency proposed by Wang et al. (2022) is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer. There are generally different thought processes for the same problem (e.g. different ways to prove the same theorem), and the output decision can be more faithful by exploring a richer set of thoughts. The output response can be picked either by majority vote, or by language model itself.The self-consistency method contains three steps: (1) prompt a language model using chain-of-thought (CoT) prompting; (2) replace the “greedy decode” in CoT prompting by sampling from the language model’s decoder to generate a diverse set of reasoning paths; and (3) marginalize out the reasoning paths and aggregate by choosing the most consistent answer in the final answer set.Self-consistency technique helps to boost the performance of CoT prompting on tasks involving arithmetic and commonsense reasoning. In particular, on arithmetic tasks self-consistency method applied to PaLM 540B and GPT-3 gave an increase in accuracy up to 17.9% compared to CoT-prompting, surpassing many SoTA solutions. On GSM8K benchmark code-davinci-002 (GPT 3.5 model optimized for code-completion tasks) with self-consistency reached 78%.Although this technique is simple to implement, it can be costly. Remember, that generating a set of N answers will increase your costs N times. Another limitation is that the “most frequent” heuristic only applies when the output space is limited (e.g. multi-choice QA).Tree of ThoughtsSimple prompting techniques can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To overcome these challenges Yao et el. (2023) proposed Tree of Thoughts (ToT), a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts1. ToT allows LLMs to perform deliberate decision making by searching over multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.To explain how ToT works first let’s formalize previous techniques. We’ll use $\\pi_\\theta$ to denote a language model, parameterized by $\\theta$. Sampling response $y$ from LLM, by giving it prompt $x$ then can be formulated as\\[y \\sim \\pi_\\theta(y \\mid x).\\]CoT introduces sequence of thoughts $z_{1 \\dots n} = (z_1, \\dots, z_n)$ to bridge $x$ and $y$. To solve problems with CoT, each thought\\[z_i \\sim \\pi_\\theta(z_i \\mid x, z_1, \\dots z_{i-1})\\]is sampled sequentially and then the output $y \\sim \\pi_\\theta(y \\mid x, z_{1 \\dots n})$. In practice $z$ is sampled as a continuous language sequence, and the decomposition of thoughts (e.g. is each $z_i$ a phrase, a sentence, or a paragraph) is left ambiguous.Self-consistency with CoT is an ensemble approach that samples $k$ i.i.d. chains of thought\\[[z_{1 \\dots n}^{(j)}, y^{(j)}] \\sim \\pi_\\theta(z_{1 \\dots n}, y \\mid x), \\quad j = 1, \\dots k,\\]then returns the most frequent output or the one with the largest score.ToT frames any problem as a search over a tree, where each node is a state $s=[x, z_{1 \\dots i}]$ representing a partial solution with the input and the sequence of generated thoughts.Schematic illustrating various approaches to problem solving with LLMs. A thought is a coherent language sequence that serves as an intermediate step toward problem solving.First, given the state $s$ a thought generator $G(\\pi_\\theta, s, k)$ generates $k$ next states. It can be done either of two ways: Sample $k$ i.i.d. samples:\\[z_{i+1}^j \\sim \\pi_\\theta(z \\mid s), \\quad j=1, \\dots k.\\]This works better when the thought space is rich (e.g. each thought is a paragraph), and i.i.d. samples lead to diversity. Propose multiple thoughts at once, by giving model a propose prompt $w$, e.g. “What are the next steps?”:\\[[z_{i+1}^1, \\cdots z_{i+1}^k] \\sim \\pi_\\theta(z \\mid s, w).\\]This works better when the thought space is more constrained (e.g. each thought is just a word or a line), so proposing different thoughts in the same context avoids duplication.Then, given the set of states $S$, each state in this set is evaluated with a state evaluator $V(\\pi_\\theta, s, S)$. Authors of ToT propose to use LLM to reason about states. Evaluation can be done either by giving a model a value prompt such as “Evaluate if…” and asking to output a scalar, or by voting out of given set $S$ by giving a model a vote prompt, e.g. “Which state to explore?”Finally, search algorithm over a tree is applied. Authors of ToT explored two classic search algorithms on graphs in their work: depth-first search and breadth-first search. Also, pruning is iteratively applied to trade exploration for exploitation, and these algorithms are more like beam searches.A step of deliberate search in a randomly picked Creative Writing task. Given the input, the LM samples 5 different plans, then votes 5 times to decide which plan is best. The majority choice is used to consequently write the output passage with the same sample-vote procedure.ToT can substantially outperform simple sampling methods, but it requires more resources and effort to implement. Although, the modular flexibility of ToT allows users to customize such performance-cost tradeoffs.Parameter Efficient Fine-TuningSmart prompting is an important tool, but it doesn’t help when the model has not learned how to solve the problems that will be given to it. A huge performance gains over using the pretrained LLMs out-of-the-box can be achieved via fine-tuning on downstream tasks. However, training the entire model, which has billions of parameters, is computationally expensive and time-consuming, not to mention impossible on most consumer hardware.This is where Parameter-Efficient Fine-tuning (PEFT) comes in handy. PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs.Prompt-tuningPrompt-tuning (Lester et al. 2021) is a simple mechanism, which allows to condition frozen LLM to perform specific downstream task. The idea is to prepend different trainable tensors $\\mathbf{P_\\theta}$ (so called soft prompts) to input embeddings per each task. Unlike the discrete text prompts, soft prompts do not tie to any embeddings associated with the real words and thus they are more expressive for steering the context.import jax.numpy as jnpdef prompt_tuned_model(token_ids): x = embedding(token_ids) x = jnp.concatenate([soft_prompt, x]) return model(x)Prompt-tuning only requires storing a small task-specific prompt for each task, and enables mixed-task inference using the original pretrained model. Experiments have shown that for large models prompt-tuning produces competitive results as model fine-tuning and its efficiency grows with model size. Also with learned task-specific parameters, prompt-tuning achieves better resilience on domain shift problems. In addition, authors showed that prompt ensembling of multiple prompts beats or matches singular prompts.Overall soft prompts are incredibly parameter-efficient at the cost of inference overhead (given the quadratic complexity of transformer) and more applicable to larger models (&amp;gt; 10B).Prefix-tuningIn prefix-tuning (Li &amp;amp; Liang 2021) instead of adding a soft prompt to the model input, trainable embeddings are prepended to the hidden states of all transformer layers. In practice, directly updating $\\mathbf{P_\\theta}$ leads to unstable optimization and poor performance. To reduce the difficulty associated with high dimensionality training, the matrix $\\mathbf{P_\\theta}$ is reparameterized by a smaller matrix $\\mathbf{P_\\theta’}$ composed with a large linear layer $\\mathbf{W}$:\\[\\mathbf{P_\\theta} = \\mathbf{P_\\theta&#39; W}.\\]def prefix_tuned_model(token_ids): x = embedding(token_ids) p = soft_prompt @ W for block in transformer_blocks: x = block(jnp.concatenate([p, x])) return xPrompt-tuning (left) vs prefix-tuning. Note that after training, only $\\mathbf{P_\\theta}$ is needed for inference, and tensor $\\mathbf{W}$ can be discarded.LoRALow-Rank Adaptation (LoRA) (Hu et. al 2021) freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. The core idea is to modify linear transformation of input vector $x$\\[h = x\\mathbf{W},\\]parameterized with a pretrained weight matrix $\\mathbf{W}$, with an additional parameter update $\\Delta \\mathbf{W}$, which can be decomposed into a product of two low-rank matrices:\\[h \\leftarrow h + x \\Delta\\mathbf{W} = x(\\mathbf{W} + \\Delta \\mathbf{W})=x\\mathbf{W}+x\\mathbf{W_d W_u}.\\]Here $\\mathbf{W}_d \\in \\mathbb{R}^{d \\times r}$ is down-projection, $\\mathbf{W}_u \\in \\mathbb{R}^{r \\times k}$ is up-projection and $r \\ll \\min(d, k)$ is a bottleneck dimension.Low rank adaptation. Frozen layers are marked with ❄.def linear_with_lora(x): h = x @ W # regular linear h += x @ W_d @ W_u # low-rank update return hIn the paper experiments LoRA performs competitively even with a very small $r$, such as 1 or 2. Also applying LoRA to both $\\mathbf{W}^Q$ and $\\mathbf{W}^V$ gives the best performance overall, while adapting only $\\mathbf{W}^Q$ or $\\mathbf{W}^K$ results in significantly lower performance, even with larger value of $r$.In general, LoRA possesses several key advantages: Low storage requirements. A pre-trained model can be shared and used to build many small LoRA modules for different tasks. We can efficiently switch between tasks by replacing the matrices $\\mathbf{W_u}$ and $\\mathbf{W_d}$, while keeping base model frozen. This reduces the storage requirement and task-switching overhead significantly. Training efficiency. LoRA makes training more efficient and lowers the hardware barrier to entry, since it is not needed to calculate the gradients or maintain the optimizer states for frozen parameters. Instead, only the injected, much smaller low-rank matrices are optimized. Inference speed. Simple linear design allows to deploy model with merged trainable matrices and frozen weights: $\\mathbf{W} \\leftarrow \\mathbf{W} + \\Delta \\mathbf{W},$ thus introducing no inference latency by construction. Orthogonality. The combination of LoRA and prefix-tuning significantly outperforms both methods applied separately on WikiSQL benchmark, which indicates that LoRA is somewhat orthogonal to prefix-tuning.Interestingly, studying the relationship between $\\Delta \\mathbf{W}$ and $\\mathbf{W}$ authors concluded that the low-rank adaptation matrix potentially amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pre-training model. Such statement suggests that LoRA can be applied to RLHF fine-tuning stage, which according to OpenAI is required to “unlock” model capabilities it has already learned.AdapterHoulsby et al. (2019) proposed to modify transformer block with additional FFN layers, called (series) adapters. The adapter module is added twice to each transformer layer: after the projection following multi-head attention and after the two feed-forward layers. But like in LoRA, the adapter consists of a bottleneck which has smaller hidden dimension than the input and therefore contains fewer parameters relative to the attention and feed-forward layers in the original model.Adapter transformation of vector $\\mathbf{h}$ can be described as\\[h \\leftarrow h + f(h\\mathbf{W_d})\\mathbf{W_u},\\]where $f(\\cdot)$ is a nonlinear activation function, e.g. ReLU.Architecture of the adapter module and its integration with the transformer block.def transformer_block_with_adapter(x): h = self_attention(x) h = h + ffn(h) # adapter x = layer_norm(x + h) h = ffn(x) # transformer FFN h = h + ffn(h) # adapter h = layer_norm(x + h) return hAdapter tuning is highly parameter-efficient: training with adapters of sizes 0.5-5% of the original model produces strong performance, comparable to full fine-tuning. In addition to that, Lin et al. (2020) and Pfeiffer et al. (2021) proposed a more efficient design with the adapter layer applied only after the FFN “Add &amp;amp; Norm” sub-layer, which achieves similar performance as using two adapters per transformer block.MAM adapterMix-and-match (MAM) adapter was proposed in a paper by He et al. (2022), where adapter placement and combinations with soft prompts were studied. They measured the performance of all prior methods on four different downstream tasks (summarization, translation, entailment/contradiction and classification), where they also included comparisons with parralel adapters.Parallel adapters.They also considered scaling adapter output with tunable parameter $s$. Changing $s$ is roughly the same as changing the learning rate for adapter block if we scale the initialization appropriately. Experiments have shown that scaled parallel adapters outperform series adapters and that placing an adapter in parallel to FFN outperforms adapters parallel to multi-head attention. Finally, they propose MAM adapter, which is a combination of scaled parallel adapter for FFN layer and soft prompt.def transformer_block_with_mam(x): x = jnp.concatenate([soft_prompt, x]) h = self_attention(x) x = layer_norm(x + h) h1 = ffn(x) # transformer FFN h2 = scale * ffn(x) # MAM adapter h = layer_norm(x + h1 + h2) return h(IA)³Liu et al. (2022) proposed another PEFT technique, called (IA)³, which stands for “Infused Adapter by Inhibiting and Amplifying Inner Activations”. (IA)³ introduces new parameters $l_v$ and $l_k$, which rescale key and value in attention mechanism:\\[\\operatorname{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) =\\operatorname{softmax} \\bigg( \\frac{\\mathbf{Q} (l_k \\odot \\mathbf{K}^T)}{\\sqrt{d_k}} \\bigg) \\cdot (l_v \\odot \\mathbf{V}),\\]and $l_{ff}$, which rescales hidden FFN activations:\\[\\operatorname{FFN}(x) = (l_{ff} \\odot f(x\\mathbf{W}_1))\\mathbf{W}_2\\]where $f$ is a nonlinearity activation function. Authors also experimented with T0 model, a variant of famous T5 model by Google, adding auxiliary loss terms to discourage the model from predicting tokens from incorrect target sequences. The called their training pipeline “T-Few” recipe.Diagram of (IA)³ and the loss terms used in the T-Few recipe. Left: (IA)³ introduces the learned vectors $l_k$, $l_v$, and $l_{ff}$ which respectively rescale (via element-wise multiplication, visualized as $\\odot$) the keys and values in attention mechanisms and the inner activations in position-wise feed-forward networks. Right: In addition to a standard cross-entropy loss $L_{LM}$, an unlikelihood loss $L_{UL}$ and length-normalized loss $L_{LN}$ are introduced. Former lowers the probability of incorrect outputs while latter applies a standard softmax cross-entropy loss to length-normalized log-probabilities of all output choices.def scaled_self_attention(x): k, q, v = x @ W_k, x @ W_q, x @ W_v k = l_k * k v = l_v * v return softmax(q @ k.T) @ V def scaled_ffn(x): x = x @ W_1 x = l_ff * f(x) # f is nonlinear activation x = x @ W_2 return xdef transformer_block_with_ia3(x): h = scaled_self_attention(x) x = layer_norm(x + h) h = scaled_ffn(x) h = layer_norm(x + h) return h(IA)³ adds smaller overhead compared to adapter methods as scale vectors $l_v$ and $l_k$ can be merged into $\\mathbf{W}^V$ and $\\mathbf{W}^K$ respectively, thus leaving the only overhead from $l_{ff}$. With minimal number of training parameters it achieves comparable results with LoRA and outperforms prompt- and prefix-tuning methods on multiple benchmarks.Providing external knowledgeLanguage models show remarkable abilities to solve new problems with just a few examples or textual instructions. At the same time, they struggle with basic functionality, such as arithmetic or factual lookup, where they are outperformed by much simpler and smaller models. They are also unable to solve tasks that require access to changing or private data that was unavailable at training time. To be able to do that LLM must be augmented with additional tools that can provide an external information.Internet-augmented language modelsLazaridou et. al (2022) proposed to use few-shot prompting to condition LMs on information returned from a broad and constantly updated knowledge source, for example, Google Search. Such approach does not involve fine-tuning or learning additional parameters, thus making it applicable to any language model.Given a query $q$, clean text is extracted out of multiple URLs returned by search, resulting in a set of documents. Each document is split into $m$ paragraphs $(\\mathcal{P})_m$, which are ranked by TF-IDF cosine similarity with a query. Top $n$ paragraphs are selected and each one is inserted separately into the following $k$-shot prompt as the last Evidence:Evidence: ...Question: ...Answer: ...... (k times)Evidence: ParagraphQuestion: QueryAnswer:This produces $n$ candidate answers $(a)_n$, which can be re-ranked with conditional probabilities $\\pi(a \\mid q, \\mathcal{P})$ and $\\pi(q \\mid \\mathcal{P})$, measured by language model and TF-IDF scores.Schematic representation of Internet-augmented LMTALMTool Augmented Language Model (TALM) Parisi et al. 2022 is a LLM augmented with text-to-text API calls. It learns two subtasks at the same time: calling a tool and generating an answer based on tool results.Tool Augmented LMs.TALM is guided to generate a tool call and tool input text conditioned on the task input text and invokes a tool’s API by generating a delimiter, such as |result. Whenever this delimiter is detected, the tool API is called and its result appended to the text sequence. TALM then continues to generate the final task output, following |output token:Input text |tool-call tool input text |result tool output text|output Output textA weather task example:How hot will it get in NYC today? |weather lookup region=NYC|result precipitation chance: 10, high temp: 20°C, low-temp: 12°C|output Today’s high will be 20°CTo train TALM authors propose to iteratively fine-tune model on a dataset of tool use examples. Each round model interacts with a tool, then expands the dataset based on whether a newly added tool can improve the generated outputs. Such technique helps to boost the model performance on knowledge and reasoning tasks drastically.ToolformerToolformer (Schick et al. 2023) approach is similar to TALM in that they both aimed for LLMs to teach themselves how to use external tools via simple APIs. Toolformer is trained as follows: Sample API calls. First, we annotate a dataset with API call usage examples. It can be done by prompting a pre-trained LM via few-shot learning. An exemplary prompt to generate API calls:Your task is to add calls to a Question Answering API to a piece of text.The questions should help you get information required to complete the text. Youcan call the API by writing &quot;[QA(question)]&quot; where &quot;question&quot; is the question youwant to ask. Here are some examples of API calls:Input: Joe Biden was born in Scranton, Pennsylvania.Output: Joe Biden was born in [QA(&quot;Where was Joe Biden born?&quot;)] Scranton, [QA(&quot;Inwhich state is Scranton?&quot;)] Pennsylvania.Input: Coca-Cola, or Coke, is a carbonated soft drink manufactured bythe Coca-Cola Company.Output: Coca-Cola, or [QA(&quot;What other name is Coca-Cola known by?&quot;)] Coke, isa carbonated soft drink manufactured by [QA(&quot;Who manufactures Coca-Cola?&quot;)]the Coca-Cola Company.Input: xOutput: Execute API calls to obtain the corresponding results. The response for each API call $c_i$ needs to be a single text sequence $r_i$. Filter annotations based on whether API calls help model predict future tokens. Let $i$ be the position of the API call $c_i$ in the sequence $(x_1, \\dots x_n)$ and let $r_i$ be the response from the API. Let also\\[L_i(\\mathbf{z}) = \\sum_{j=i}^n w_{j-i} \\log \\pi(x_j \\mid z, x_{1:j-1})\\]be a weighted cross-entropy loss with condition $\\mathbf{z}$, given as a prefix. Then to decide which API calls are actually helpful, we compare the difference of losses $L_i^- - L_i^+$ to some threshold, where\\[\\begin{aligned}L_i^+ &amp;amp;= L_i(c_i \\rightarrow r_i),\\\\L_i^- &amp;amp;= \\min(L_i(\\varepsilon), L_i(c_i \\rightarrow \\varepsilon))\\end{aligned}\\]and $\\varepsilon$ is an empty sequence. Only API calls with $L_i^- - L_i^+$ larger than some threshold are kept. Fine-tune LM on this annotated dataset.Key steps in Toolformer approach, illustrated for a question answering tool: Given an input text $x$, we first sample a position $i$ and corresponding API call candidates $c_i^1, c_i^2, \\dots, c_i^k$. We then execute these API calls and filter out all calls which do not reduce the loss $L_i$ over the next tokens. All remaining API calls are interleavedwith the original text, resulting in a new text $x^\\ast$.At inference time, decoding runs until the model produces “$\\rightarrow$” token, indicating that it is expecting response from an API call next. At this point, the decoding process is interrupted, the appropriate API is called to get a response, and the decoding process continues after inserting the response.Toolformer considerably improves zero-shot performance of language model, e.g. augmented GPT-J (6.3B) even outperformed a much larger GPT-3 model on a range of different downstream tasks.ConclusionThe list of techniques in this post is far from complete, but it provides direction for those who are looking for a way to make their language model more useful. Assistants based on Large Language Models are relatively new and, while extremely powerful, still face many limitations that can be worked around in a variety of ways. It is only a matter of time before language models cease to be used for entertainment purposes and enter our daily lives as a complete and useful tool. A similar idea but with use of reinforcement learning instead of tree search was proposed by Long (2023). &amp;#8617; " }, { "title": "Building Aligned Intelligence System. Part I: Creating GPT Assistant", "url": "/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/", "categories": "Generative AI, Large Language Models", "tags": "transformer, llm, gpt, chatgpt, rlhf", "date": "2023-07-03 06:00:00 +0300", "snippet": " In recent years, the field of natural language processing has witnessed a remarkable breakthrough with the advent of Large Language Models (LLMs). These models have demonstrated unprecedented performance in a wide range of language tasks, from text generation to question answering. However, the mathematical formulation of these models and the techniques used to fine-tune them for specific tasks can be complex and daunting for many researchers and practitioners. In this post, we will delve into the mathematical underpinnings of LLMs, with a focus on ChatGPT and parameter efficient fine-tuning. We will explore the key concepts and techniques involved in these models, and provide a comprehensive guide to help you understand and apply them in your own research and development. So, whether you’re a seasoned NLP expert or just starting out, join us on this journey to unlock the secrets of LLMs and take your NLP skills to the next level. ‘Engaging preamble’ generated by ChatGPT.There will be two blogposts. The first post will be helpful to those who want to become acquainted with the RLHF pipeline as it focuses on InstructGPT/ChatGPT development. The second one is for those who are interested in applying large language models to specific tasks.Here is the partial list of the sources I used for this and the following parts: Papers: Scalable agent alignment via reward modeling:a research direction (Leike et al., 2018) Language Models are Few-Shot Learners (Brown et al., 2020) Fine-Tuning Language Models from Human Preferences (Ziegler et al., 2020) Training language models to follow instructionswith human feedback (Ouyang et al., 2022) Learning to summarize from human feedback (Stiennon et al., 2022) Training a Helpful and Harmless Assistant withReinforcement Learning from Human Feedback (Bai et al., 2022) Deep Reinforcement Learningfrom Human Preferences (Christiano et al., 2023) Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning (Lialin et al., 2023) Chain-of-Thought Prompting Elicits Reasoningin Large Language Models (Wei et al., 2023) Posts: RLHF: Reinforcement Learning from Human Feedback (Chip Huyen, 2023) Illustrating Reinforcement Learning from Human Feedback (RLHF) (Lambert et al., 2022) Prompt Engineering (Lilian Weng, 2023) OpenAI GPT Models (Lei Mao, 2023) Techniques to improve reliability (OpenAI, 2023) GPT assistant building pipeline can be divided into 4 stages, each of which will be described here, starting from an overview of transformer architecture.A diagram from Andrej Karpathy talk illustrating the four steps of GPT assistant training pipeline: (1) base model pretraining, (2) supervised fine-tuning (SFT), (3) reward model (RM) training, and (4) reinforcement learning via policy optimization on this reward model and human feedback (RLHF).Pretraining Base modelLanguage modellingIn 2018 OpenAI presented Generative pre-trained transformer (GPT), a type of deep learning model, that is designed to generate natural language text. Their work, which they humbly called Improving Language Understanding by Generative Pre-Training, along with famous Attention is all you need paper, gave birth to the entire family of large language models and changed an entire AI industry in just 5 years.The model was trained on a large corpus of text data with a completion task. During such training model is fed by a partial sequence of words, called prompt or context vector of tokens, and then asked to generate a response to complete that prompt. The model learns to predict the probability of the next token $x_{k+1}$ in a sequence given by previous tokens $x = (x_1, \\dots x_k)$ sampled from dataset $\\mathcal{D}$. Strictly speaking, we search for parameters of neural network $\\phi$, such that they minimize the cross-entropy loss\\[\\mathcal{L}(\\phi) = -\\mathbb{E}_{x \\sim \\mathcal{D}}[\\log \\pi_\\phi(x_{k+1} \\mid x_1, \\dots, x_{k})],\\]where $\\pi_\\phi$ is a language model output.Though GPT-1 was pre-trained as an autoregressive language model, transformer’s fine-tuning and inference didn’t have to be autoregressive and language model could be further fine-tuned for specialized natural language processing tasks.Input transformations for fine-tuning GPT on different tasks.The major conclusion from this paper was that it is no longer necessary to develop specific neural network architectures for specific natural language processing tasks. Transfer learning from GPT language model pre-trained with large corpus of text data was already sufficient.Transformer architectureThe process of text generation with GPT is the following. First, embedding layer takes sequence of tokens $x$ and outputs\\[h_0 = x \\mathbf{W_e} + \\mathbf{W_p},\\]where $\\mathbf{W_e}$ and $\\mathbf{W_p}$ are token and position embedding matrices respectively. Then these embedding vectors are processed by so-called transformer, consisting of multiple transformer blocks (more about it later):\\[h_n = \\operatorname{Transformer-block}(h_{n-1}) \\quad n = 1, \\dots, N.\\]And finally we get the token output distribution by taking $h_N$ and run it through unembedding layer and softmax function:\\[\\pi_\\phi(\\cdot \\mid x) = \\operatorname{softmax}(h_N \\mathbf{W_e}^T).\\]Simplified view of GPT architecture. Recently other techniques to encode token positions have appeared, such as Rotary Position Embeddings (RoPE) and Attention with Linear Biases (ALiBi). They are out of the scope of this postThe key innovation of GPT is its use of a transformer architecture, which allows the model to process long sequences of text efficiently. This makes GPT particularly well-suited for tasks that require generating long, coherent pieces of text, such as writing articles or answering questions. The core of the transformer is a scaled dot product attention operation, which takes as input a set of queries $\\mathbf{Q}$, keys $\\mathbf{K}$ and values $\\mathbf{V}$ and outputs\\[\\operatorname{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\operatorname{softmax} \\Big( \\frac{\\mathbf{QK}^T}{\\sqrt{d}} \\Big) \\cdot \\mathbf{V},\\]where $d$ is a hidden dimensionality for queries and keys. The goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute.Transformer architecture can be one of two types: encoder or decoder. The only difference between the two is whether mask is applied to attention layer. This modification in decoder architecture is crucial for next-token-prediction task, because it prevents positions from attending to subsequent positions attention layer modified by mask. Combined with the fact that the output embeddings are offset by one position, masking ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$. It can be implemented inside of scaled dot-product attention by masking out (setting to $-\\infty$) all values in the $\\mathbf{QK}^T$ matrix which correspond to illegal connections.The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. The attention mechanism can be extended to multiple heads, i.e. multiple different query-key-value triplets on the same features. Specifically, given $\\mathbf{Q}$, $\\mathbf{K}$, and $\\mathbf{V}$ matrices, we transform those into $k$ sub-queries, sub-keys, and sub-values, respectively, which we run through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix $\\mathbf{W}^O$. Mathematically, we can express this operation as:\\[\\operatorname{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})=[\\operatorname{head}_1; \\dots; \\operatorname{head}_k] \\cdot \\mathbf{W}^O,\\]where\\[\\operatorname{head}_i = \\operatorname{Attention}(\\mathbf{QW}_i^Q, \\mathbf{KW}_i^W, \\mathbf{VW}_i^V), \\quad i = 1, \\dots, k.\\]We’ll refer to this as multi-head attention layer with the learnable parameters $\\mathbf{W}^Q_{1 \\dots k}, \\mathbf{W}^K_{1 \\dots k}, \\mathbf{W}^V_{1 \\dots k}$ and $\\mathbf{W}^O$ (also called multi-head self-attention for $\\mathbf{Q} = \\mathbf{K} = \\mathbf{V}$). Such mechanism allows the model to jointly attend to information from different representation subspaces at different positions. The output of multi-head attention is added to the original input using a residual connection, and we apply a consecutive layer normalization on the sum.Transformer is basically a stack of $N$ identical blocks with multi-head attention. In addition to attention sub-layers, each block contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a nonlinear function in between, e.g. ReLU1:\\[\\operatorname{FFN}(x) = \\max(0, x\\mathbf{W}_1)\\mathbf{W}_2\\]After feed forward block residual connection with normalization layer is added again.Generative pre-trained transformer decoder architecture. In some recent implementations one can find layer normalization being applied to input stream of the submodules (right before multi-head attention and feed forward layers, not after).There are many guides on the internet for implementing a transformer. To get more detailed explanations one can follow this tutorial on PyTorch or this one on Jax.It turned out that the transformer architecture is highly scalable, meaning that it is able to handle large amounts of data and perform well on tasks of varying complexity. It has been shown to achieve SoTA performance on a variety of NLP benchmarks, demonstrating its ability to scale to handle complex tasks and large datasets. GPT-1 was just a starting point and later OpenAI presented much bigger models, such as GPT-2 and GPT-3. Here is a table for comparison: Model Train data size Transformer blocks Context size Total parameters GPT-1 20M tokens 12 512 tokens 117M GPT-2 9B tokens 48 1024 tokens 1.5B GPT-3 300B tokens 96 2048 tokens 175B There is no official information yet on how large GPT-4 is, but there’ve been some rumors that it consists of multiple expert models with 220B parameters.Language modelling is by far the most resource-intensive phase in InstructGPT training. According to OpenAI the rest of their pipeline used less than 2% of the compute and data relative to model pretraining. One way of thinking about this process is that at the end of this phase base LLM already has all the required capabilities, but they are difficult to elicit and the rest of the process is aimed to “unlock” them.Supervised fine-tuning (SFT) for dialogueSuppose we have a pretrained large language model. But this model is far from being a useful assistant. It doesn’t answer questions, it only wants to complete internet documents. Chip Huyen provided a good example in her blogpost: if you give the pretrained model a question, say, How to make pizza, any of the following could be valid completion. Adding more context to the question: for a family of six Adding follow-up questions: ? What ingredients do I need? How much time would it take? Actually giving the answerThe third option is preferred if you’re looking for an answer. In order to achieve it, base model can be tricked into performing tasks with prompt engineering. For example, we can give model an instruction and expect it to follow, e.g.:Classify the text into neutral, negative or positive. Text: I think the vacation is okay.Sentiment:Output:NeutralThis is an example of zero-shot prompting. One can also give a few demonstration examples of how the dialogue should look like and expect the model to continue in the same vein. This is called few-shot prompting.Few-shot prompt engineering example from Andrej Karpathy talkStandard few-shot prompting works well for many tasks but is still not a perfect technique. A more efficient way is to train the model to answer questions directly. This is what SFT does: its goal is to optimize the pretrained model to generate the responses that users are looking for. At this stage labelers provide demonstrations of the desired behaviour on the input prompt distribution. Then a pretrained model is fine-tuned on this data learning to appropriately respond to prompts of different use cases (e.g. question answering, summarization, translation).Supervised fine-tuning stage of InstructGPT pipeline.Basically, SFT model is an initial language model for RLHF. For InstructGPT training OpenAI fine-tuned three different versions of GPT-3 (1.3B, 6B and 175B) on labeler-written demonstration prompts. OpenAI called supervised finetuning behavior cloning: we demonstrate how the model should behave, and the model clones this behavior. Anthropic, for example, used a different technique: they trained their SFT model by distilling an original language model on context clues for their “helpful, honest, and harmless” criteria.Reward Model (RM) trainingThe problem with SFT is that model learns what kind of responses are plausible for a given context, but it receives no information on how good or bad a response is. At the same time while it is easy for humans to understand which sentences are better than others, it is difficult to formulate and automate reasons for their choice.This is where human feedback (HF) comes into play. At this stage datasets of comparisons between model outputs is collected: annotators indicate which output of SFT model they prefer for a given input. At first glance, it may seem that labelers must apply a scalar score directly to each output in order to create data for reward model, but this is difficult to do in practice. The differing values of humans cause these scores to be uncalibrated and noisy. Instead, rankings are used to compare the outputs of multiple models and create a much better regularized dataset.To collect ranking data, human labelers are presented with $K_x$ generated samples conditioned on the same prompt $x$ (in InstructGPT paper, $K_x$ is anywhere between $4$ and $9$). This produces\\[\\binom{K_x}{2} = \\frac{K_x(K_x-1)}{2}\\]comparisons for each prompt. Labelers express preferences for one answer, denoted as $y_w \\succ y_l \\mid x$ with $y_w$ being a preferred answer over $y_l$.Then a preference model is trained to predict the human-preferred output. Starting from the SFT model with the final unembedding layer removed, a reward model $r_\\theta(x, y)$ is trained to take in a prompt $x$ and response $y$, and output a scalar reward $r$. OpenAI used 6B RM with to evaluate 175B language model as they found out that while larger RM had the potential to achieve lower validation loss, their training was more unstable and using a 175B RM and value function greatly increase the compute requirements of reinforcement learning. Deepmind, on a contrary, trained Chinchilla models with 70B parameters for both reward and language model. An intuition would be that reward models need to have similar capacity to understand the text given to them as a model would need in order to generate said text.Reward modelling stage of InstructGPT pipeline. Boxes A-D are samples from the SFT model that get ranked by labelersThere are a number of approaches used to model preferences, the Bradley-Terry model being a popular choice:\\[\\mathbb{P}(y_i \\succ y_j \\mid x) = \\frac{\\exp(r_\\theta(x, y_i))}{\\exp(r_\\theta(x, y_i))+\\exp(r_\\theta(x, y_j))}.\\]Framing the problem as a balanced binary classification we come up to the negative log-likelihood loss:\\[\\mathcal{L}(\\theta) = -\\mathbb{E}_{(x, y_w, y_l) \\sim \\mathcal{D}}\\bigg[\\frac{1}{\\binom{K_x}{2}}\\log \\sigma (r_\\theta(x, y_w) - r_\\theta(x, y_l))\\bigg].\\]This way model learns to maximize the difference between rewards for chosen and rewards for rejected answers. At the end of training, the reward model is normalized so that\\[\\mathbb{E}_{(x, y) \\sim \\mathcal{D}}[r_\\theta(x, y)]=0\\]for all $x$.The success of reward modeling relies heavily on the quality of the reward model. If the reward model only captures most aspects of the objective but not all of it, this can lead the agent to find undesirable degenerate solutions. In other words, the agent’s behavior depends on the reward model in a way that is potentially very fragile.Reinforcement Learning with Human Feedback (RLHF)At final stage output of the reward model used as a scalar reward to optimize a policy. Following Stiennon et al. (2020), authors of InstructGPT fine-tuned the SFT model on their environment using on-policy algorithm, called Proximal Policy Optimization (PPO), Schulman et al. (2017). The environment is a bandit environment which presents a random customer prompt to language model and expects a response to the prompt with a sequence of probability distributions over tokens $\\pi(y \\mid x)$. The action space of this policy is all the tokens corresponding to the vocabulary of the language model and the observation space is the distribution of possible input token sequences.Given the prompt and response, environment produces a reward determined by RM and ends the episode.Policy optimization stage of InstructGPT pipeline.A per-token KL penalty is added to mitigate overoptimization of the reward model. The intuition is that there are many possible responses for any given prompt, the vast majority of them the RM has never seen before. For many of those unknown $(x, y)$ pairs, the RM might give an extremely high or low score by mistake. Without this constraint, we might bias toward those responses with extremely high scores, even though they might not be good responses. The following modified reward function in RL training is maximized:\\[\\begin{aligned}\\mathcal{J}_{\\operatorname{PPO}}(\\phi) &amp;amp;=\\mathbb{E}_{(x,y)\\sim \\mathcal{D}_{\\pi_\\phi^{\\operatorname{RL}}}}\\big[r_\\theta(x,y)\\big]-\\beta D_{\\operatorname{KL}}(\\pi_\\phi^{\\operatorname{RL}} \\mid \\mid \\pi^{\\operatorname{SFT}})\\\\&amp;amp;= \\mathbb{E}_{(x,y)\\sim \\mathcal{D}_{\\pi_\\phi^{\\operatorname{RL}}}}\\Big[r_\\theta(x,y)-\\beta \\log \\frac{\\pi_\\phi^{\\operatorname{RL}}(y \\mid x)}{\\pi^{\\operatorname{SFT}}(y \\mid x)}\\Big], \\end{aligned}\\]where $\\pi_\\phi^{\\operatorname{RL}}$ is the learned RL policy, $\\pi^{\\operatorname{SFT}}$ is the supervised trained model. The KL-divergence coefficient, $\\beta$ controls the strength of the KL penalty. InstructGPT authors also experimented with mixing the pretraining gradients into the PPO gradients, in order to fix the performance regressions on public NLP datasets. They called these models “PPO-ptx”:\\[\\mathcal{J}_{\\operatorname{PPO-ptx}}(\\phi) = \\mathcal{J}_{\\operatorname{PPO}}(\\phi) + \\gamma \\mathbb{E}_{x \\sim \\mathcal{D}_{\\operatorname{pretrain}}}[\\log\\pi_\\phi^{\\operatorname{RL}}(x)],\\]where $D_{\\operatorname{pretrain}}$ is the pretraining distribution. The pretraining loss coefficient $\\gamma$ controls strength of pretraining gradients.At this stage one has to be careful to avoid reward hacking - an effect that lets the agent get more reward than intended by exploiting loopholes in the process determining the reward. An agent can exploit some misspecification in the reward function, e.g. when the reward function incorrectly provides high reward to some undesired behavior. One potential source for such exploit is the reward model’s vulnerability to adversarial inputs. The agent might figure out how to specifically craft these adversarially perturbed inputs in order to trick the reward model into providing higher reward than the user intends.Finally, RM stage and RL stage can be iterated continuously: collect more comparison data on the current best policy, then use it to train a new RM and then a new policy. In InstructGPT pipeline, most of comparison data comes from supervised policies, with some coming from PPO policies.Details of RL algorithmConsider response generation as a sequence of input states and actions. Let’s denote input state at timestep $t$ as\\[s_t=(x, y_1, \\dots y_{t-1}),\\]where $y_s$ is a response token, generated at timestep $s$ by actor $\\pi_\\phi$. During rollout at each timestep $t$ RL model is penalized by\\[R_{t+1} = -\\beta \\log \\frac{\\pi_\\phi^{\\operatorname{RL}}(y_t \\mid s_t)}{\\pi^{\\operatorname{SFT}}(y_t \\mid s_t)}\\]until it reaches terminal state and receives final reward\\[R_T = r_\\theta(x, y)-\\beta \\log \\frac{\\pi_\\phi^{\\operatorname{RL}}(y_{T-1} \\mid s_t)}{\\pi^{\\operatorname{SFT}}(y_{T-1} \\mid s_t)}.\\]Backup diagram illustrating one step of sequence generation in terms of reinforcement learningIn general we are interested in maximizing return, which is a total sum of rewards going forward:\\[G_t = R_{t+1} + R_{t+2} + \\dots + R_T.\\]It is also common to use the discounted return:\\[G_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-t} R_T = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1},\\]where the discounting factor $\\gamma \\in [0, 1]$ trades off later rewards to earlier ones.Besides actor model, we must have a critic $V_\\theta(s_t)$, which estimates $G_t$ and is trained with mean-squared loss:\\[\\mathcal{L}_V(\\theta) = \\mathbb{E}_t[(V_\\theta(s_t) - G_t)^2]\\]While actor is initialized from SFT model, critic can also be initialized from RM model. With critic network estimating the return under current policy actor network learns to maximize the probability of getting a positive advantage, the relative value of selected response token:\\[\\hat{A}_t = G_t - V_\\theta(s_t).\\]The equality above2 can be used for return calculation, but such estimator would have low bias but high variance. Usually returns and advantages are estimated with a technique called bootstrapping, e.g. with generalized version of advantage estimation (GAE), popularized by Schulman et. al (2018):\\[\\begin{aligned}\\hat{A}_t = \\delta_t + (\\gamma \\lambda) \\delta_{t+1} + \\dots = \\sum_{k=0}^{T-t-1} (\\gamma \\lambda)^k \\delta_{t+k}, \\\\\\text{where } \\delta_t = R_{t+1} + \\gamma V_\\theta(s_{t+1}) - V_\\theta(s_t)\\end{aligned}\\]and $\\lambda \\in [0,1]$ is a decay factor, penalizing high variance estimates. I recommend this nice post to grasp the idea of GAE.To train actor network vanilla policy gradient loss can be used:\\[\\mathcal{L}^{\\operatorname{PG}}(\\phi) = -\\mathbb{E}_t[\\log \\pi_\\phi(y_t \\mid s_t) \\hat{A}_t].\\]This way, when $\\hat{A}_t$ is positive, meaning that the action agent took resulted in a better than average return, we increase the probability of selecting it again in the future. On the other hand, if an advantage is negative, we reduce the likelihood of selected action.However, consider the case of optimizing target policy $\\pi_\\phi$, when the behaviour policy $\\pi_{\\phi_{\\text{old}}}$ is used for collecting trajectories with $\\phi_{\\text{old}}$ policy parameters before the update. In an original PPO paper it is stated, that while it is appealing to perform multiple steps of optimization on this loss using the same trajectory, doing so is not well-justified, and empirically it often leads to destructively large policy updates. In other words, we have to impose the constraint which won’t allow our new policy to move too far away from an old one. Let $\\rho_t(\\phi)$ denote the probability ratio between target and behaviour policies:\\[\\rho_t(\\phi) = \\frac{\\pi_\\phi(y_t \\mid x)}{\\pi_{\\phi_{\\operatorname{old}}}(y_t \\mid x)},\\]so $\\rho_t(\\phi_{\\operatorname{old}}) = 1$. Then we can minimize the surrogate objective function (the superscript CPI refers to conservative policy iteration)\\[\\mathcal{L}^{\\text{CPI}}(\\phi) = -\\mathbb{E}_t[\\rho_t(\\phi) \\hat{A}_t].\\]Indeed,\\[\\begin{aligned}\\nabla_\\phi\\mathcal{L}^{\\operatorname{PG}}(\\phi) \\big \\vert_{\\phi_{\\text{old}}} &amp;amp;= -\\mathbb{E}_t\\big[\\nabla_\\phi\\log \\pi_\\phi(y_t \\mid x)\\big \\vert_{\\phi_{\\text{old}}} \\hat{A}_t\\big] \\\\&amp;amp;=-\\mathbb{E}_t\\bigg[\\frac{\\nabla_\\phi \\pi_\\phi(y_t \\mid s_t) \\big \\vert_{\\phi_{\\text{old}}}}{\\pi_{\\phi_{\\text{old}}}(y_t \\mid s_t)} \\hat{A}_t\\bigg] \\\\ &amp;amp;= -\\mathbb{E}_t\\big[\\nabla_\\phi \\rho_t(\\phi) \\big \\vert_{\\phi_{\\text{old}}} \\hat{A}_t\\big] \\\\ &amp;amp;= \\nabla_\\phi\\mathcal{L}^{\\text{CPI}}(\\phi).\\end{aligned}\\]Now, we would like to insert the aforementioned constraint into this loss function. The main objective which authors of PPO propose is the following.\\[\\mathcal{L}^{\\text{CLIP}}(\\phi) = -\\mathbb{E}_t\\Big[\\min \\big(\\rho_t(\\phi) \\hat{A}_t, \\text{clip}(\\rho_t(\\phi), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\big)\\Big],\\]where $\\epsilon$ is a clip ratio hyperparameter. The first term inside $\\min$ function, $\\rho_t(\\phi) \\hat{A}_t$ is a normal policy gradient objective. And the second one is its clipped version, which doesn’t allow us to destroy our current policy based on a single estimate, because the value of $\\hat{A}_t$ is noisy (as it is based on an output of our network).The total objective is a combination of clipped loss and error term on the value estimation\\[\\mathcal{L}(\\phi, \\theta) = \\mathcal{L}^{\\text{CLIP}}(\\phi) + c\\mathcal{L}_V(\\theta).\\]import jaximport jax.numpy as jnp# actor - policy neural network# params - learnable parameters# states[B] - batch of input states# token_ids[B] - generated response token indices# advantages[B] - batch of estimated advantages# logp_old[B] - log-probabilities of generated tokens according to behaviour policy# eps - clip ratio, usually around 0.2@jax.jitdef actor_loss(actor, params, states, token_ids, advantages, logp_old, eps=0.2): logp_dist = actor.apply(params, states) logp = jnp.stack([lp[y] for lp, y in zip(logp_dist, token_ids)]) ratio = jnp.exp(logp - logp_old) clip_adv = jnp.clip(ratio, 1 - eps, 1 + eps) * advantages return -jnp.min(ratio * advantages, clip_adv) # critic - value prediction neural network# returns[B] - batch of discounted returns@jax.jitdef critic_loss(critic, params, states, returns): values = critic.apply(params, states) return (values - returns) ** 2Note on KL approximationsJohn Schulman, author of PPO algorithm, proposes different estimators of KL-divergence $D_{\\operatorname{KL}}(\\pi’ \\mid \\mid \\pi)$ in his blogpost. Let $\\kappa=\\frac{\\pi(x)}{\\pi’(x)}$, then for $\\pi \\approx \\pi’$ we get empirically:   Estimation Bias Variance $k_1$ $-\\log \\kappa$ $0$ High $k_2$ $\\frac{1}{2}(\\log \\kappa)^2$ Low Low $k_3$ $(\\kappa-1)-\\log \\kappa$ $0$ Low The main advantage of $k_2$ and $k_3$ estimators is the zero probability of getting negative values. Although, with true KL-divergence between $\\pi$ and $\\pi’$ getting larger we can observe that bias for $k_2$ and variance for $k_3$ are increasing as well.GPT chatbot limitationsIs RLHF just putting smileys on a shoggoth?AI alignment may be difficult and ambiguous to assess. While being powerful tools, GPT assistants can still output harmful or factually inaccurate text without any uncertainty. OpenAI admits that ChatGPT sometimes gives convincing-sounding answers that are incorrect or even complete nonsense. Fixing this issue is a long-term challenge, as: During RLHF training model operates in an inherently human problem domain with no source of truth. Supervised training misleads the model because the ideal answer depends on what the model knows, rather than what the human demonstrator knowsThe effect of an AI making up facts is called hallucination. Avoiding this effect through training the model to be more cautious can cause it to decline questions that it can answer correctly. In such situation we would say that we’ve been over-optimizing for harmlessness, while under-optimizing helpfulness. Putting restrictions on the system to avoid systemic biases as well as unethical opinions that might exist within the training dataset is a reason why it’s currently hard to have nuanced conversations with ChatGPT about controversial, or sensitive, subjects.The problem involves the definition of harmlessness – if simply refusing to answer a question is the ‘least harmful’ behavior, then this is probably both very easy to learn, and hard to improve on. That said, a more interesting ‘least harmful’ behavior would involve the model (helpfully) explaining why the request was harmful, and perhaps even trying to convince the human not to pursue such requests. Anthropic informally refer to such a model as a ‘hostage negotiator’.Another problem is model robustness. ChatGPT is sensitive to tweaks to the input phrasing or attempting the same prompt multiple times. For example, given one phrasing of a question, the model can claim to not know the answer, but given a slight rephrase, can answer correctly. Moreover, GPT models are stochastic – there’s no guarantee that LLM will give you the same output for the same input every time.That being said, we’re still in the early days of GPT applications. We do not have much experience applying RL techniques to large generative models and many things about LLMs, including RLHF, will evolve. There are a large variety of tweaks and tricks that require experimentation to identify, and that can majorly improve the stability and performance of training. And many methods could be tried to further decrease the models’ propensity to generate toxic, biased, or otherwise harmful outputs.OpenAI states that one of the biggest open questions is how to design an alignment process that is transparent, that meaningfully represents the people impacted by the technology, and that synthesizes peoples’ values in a way that achieves broad consensus amongst many groups. And I personally believe that finding an answer to this rather general question will be one of the major goals of the future works on AI. Authors of original GPT-1 paper were using Gaussian Error Linear Unit (GELU) activation in Feed-Forward layers\\[\\operatorname{GELU}(x) = x \\Phi(x).\\] &amp;#8617; The hat sign means that $\\hat{A}_t$ is an estimator of the true advantage, which is:\\[A_t = \\mathbb{E}_t[G_t \\mid s_t, y_t] - \\mathbb{E}_t[G_t \\mid s_t].\\] &amp;#8617; " }, { "title": "Power of Diffusion Models", "url": "/posts/power-of-diffusion-models/", "categories": "Generative AI, Diffusion Models", "tags": "clip, ddim, score-based model, guided diffusion, dall·e 2, imagen, stable diffusion, latent diffusion model, jax", "date": "2022-09-25 06:00:00 +0300", "snippet": " In 2022, insanely beautiful and original images created with generative neural networks are taking the internet by storm. This post focuses on the theory behind diffusion models that underpin the core ideas of the latest generative AI. Brace yourself, this post is math-heavy and there are a lot of formulas ahead.In 2022 ‘Théâtre D’opéra Spatial’, an artwork by Jason M. Allen with help of Midjourney took 1st place in the digital art competition at Colorado State Fair. This event sparked a backslash from artists, claiming that creative jobs are now not safe from machines and in danger of becoming obsolete. Here I chose this picture emerging from noise as a symbol of an upcoming age of art, created by artificial intelligence.Before we start, I want to mention all the sources which were helpful to write this post: Papers: Denoising Diffusion Probabilistic Models Improved Denoising Diffusion Probabilistic Models Denoising Diffusion Implicit Models Diffusion Models Beat GANs on Image Synthesis Generative Modeling by Estimating Gradients of theData Distribution Classifier-free diffusion guidance GLIDE: Towards Photorealistic Image Generation and Editing withText-Guided Diffusion Models Hierarchical Text-Conditional Image Generation with CLIP Latents Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding High-Resolution Image Synthesis with Latent Diffusion Models Posts: What are Diffusion Models? Generative Modeling by Estimating Gradients of the Data Distribution Denoising Diffusion-based Generative Modeling: Foundations and Applications The recent rise of diffusion-based models Denoising diffusion probabilistic models (DDPM)To define a diffusion probabilistic model (usually called a “diffusion model” for brevity), we first define a Markov chain, which starts from initial datapoint $\\mathbf{x}_0$, then gradually adds noise to the data, creating sequence $\\mathbf{x}_0, \\mathbf{x}_1, \\dots, \\mathbf{x}_T$, until signal is destroyed.Forward diffusion process. Given a data point sampled from a real data distribution $\\mathbf{x}_0 \\sim q(x_0)$, we produce noisy latents $\\mathbf{x}_1 \\rightarrow \\cdots \\rightarrow \\mathbf{x}_T$ by adding small amount of Gaussian noise at each timestep $t$. The latent $\\mathbf{x}_t$ gradually loses its recognizable features as the step $t$ becomes larger and eventually with $T \\rightarrow \\infty$, $\\mathbf{x}_T$ is nearly an isotropic Gaussian distribution.The step sizes are controlled by a variance schedule $\\beta_t \\in (0, 1)$:\\[\\mathbf{x}_t = \\sqrt{1-\\beta_t} \\mathbf{x}_{t-1} + \\sqrt{\\beta_t} \\epsilon_{t-1}, \\quad \\epsilon_{t-1} \\sim \\mathcal{N}(0, \\mathbf{I})\\]Conditional distribution for the forward process is\\[q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\sqrt{1-\\beta_t} \\mathbf{x}_t, \\beta_t \\mathbf{I}), \\quad q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0) = \\prod_{t=1}^T q(\\mathbf{x}_t \\vert \\mathbf{x}_{t-1}).\\]Recall that for two Gaussian random variables $\\epsilon_1 \\sim \\mathcal{N}(0, \\sigma^2_1\\mathbf{I})$ and $\\epsilon_2 \\sim \\mathcal{N}(0, \\sigma^2_2 \\mathbf{I})$ we have\\[\\epsilon_1 + \\epsilon_2 \\sim \\mathcal{N}(0, (\\sigma_1^2 + \\sigma_2^2)\\mathbf{I}).\\]Therefore for each latent $\\mathbf{x}_t$ at arbitrary step $t$ we can sample it in a closed form.Using the notation $\\alpha_t := 1 - \\beta_t$ and $\\overline\\alpha_t := \\prod_{s=1}^t \\alpha_s$ we get\\[\\begin{aligned}\\mathbf{x}_t &amp;amp; = {\\sqrt{\\alpha_t} \\mathbf{x}_{t-1}} + { \\sqrt{1-\\alpha_t} \\epsilon_{t-1}} \\\\ &amp;amp; = {\\sqrt{\\alpha_t \\alpha_{t-1}} \\mathbf{x}_{t-2} + \\sqrt{\\alpha_t (1-\\alpha_{t-1})} \\epsilon_{t-2}} + { \\sqrt{1-\\alpha_t} \\epsilon_{t-1}} \\\\ &amp;amp; = {\\sqrt{\\alpha_t \\alpha_{t-1}} \\mathbf{x}_{t-2}} + \\sqrt{1-\\alpha_t \\alpha_{t-1}} \\bar\\epsilon_{t-2} \\qquad \\color{Salmon}{\\leftarrow \\bar\\epsilon_{t-2} \\sim \\mathcal{N}(0, \\mathbf{I})} \\\\ &amp;amp; = \\cdots \\\\ &amp;amp;= \\sqrt{\\overline\\alpha_t} \\mathbf{x}_0 + \\sqrt{1-\\overline\\alpha_t} \\epsilon\\end{aligned}\\]and\\[q(\\mathbf{x}_t \\vert \\mathbf{x}_{0}) \\sim \\mathcal{N}\\big(\\sqrt{\\overline\\alpha_t}\\mathbf{x}_{0}, \\sqrt{1-\\overline\\alpha_t} \\mathbf{I}\\big).\\]If we were able to go in the opposite direction and sample from reverse process distribution $q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)$, we could recreate samples from a true distribution $q(\\mathbf{x}_0)$ with only a Gaussian noise input $\\mathbf{x}_T$. In general reverse process distribution is intractable, since its calculation would require marginalization over the entire data distribution.The core idea of diffusion algorithm is to train a model $p_\\theta$ to approximate $q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)$ in order to run the reverse diffusion process:\\[p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}) = \\mathcal{N}(\\mu_\\theta(\\mathbf{x}_t, t), \\Sigma_\\theta(\\mathbf{x}_t, t)),\\]where $\\mu_\\theta$ and $\\Sigma_\\theta$ are trainable networks. Although, for simplicity we can decide for\\[\\Sigma_\\theta(\\mathbf{x}_t, t) = \\sigma_t^2 \\mathbf{I}.\\]Forward and reverse diffusion processes. Going backwards, we start from isotropic Gaussian noise $p(\\mathbf{x}_T) \\sim \\mathcal{N}(0, \\mathbf{I})$ and gradually sample from $p_\\theta(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t)$ for $t=T, \\dots, 1$ until we get a data point from approximated distribution.Note that reverse conditional probability is tractable when conditioned on $\\mathbf{x}_0$:\\[q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}({\\color{#5286A5}{\\tilde \\mu(\\mathbf{x}_t, \\mathbf{x}_0)}}, {\\color{#C19454}{\\tilde \\beta_t \\mathbf{I}}}).\\]Efficient training is therefore possible by minimizing Kullback-Leibler divergence between $p_\\theta$ and $q$, or formally, evidence lower bound loss\\[\\begin{aligned}L_{\\operatorname{ELBO}} &amp;amp;= \\mathbb{E}_q\\bigg[\\log\\frac{q(\\mathbf{x}_{1:T} \\vert \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{0:T})} \\bigg]\\\\ &amp;amp;= \\mathbb{E}_q\\bigg[\\log\\frac{\\prod_{t=1}^T q(\\mathbf{x}_t|\\mathbf{x}_{t-1}) }{p_\\theta(\\mathbf{x}_T) \\prod_{t=1}^T p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t)} \\bigg]\\\\ &amp;amp;= \\mathbb{E}_q\\bigg[\\sum_{t=1}^T \\log \\frac{ q(\\mathbf{x}_t|\\mathbf{x}_{t-1})} {p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t)} -\\log p_\\theta(\\mathbf{x}_T)\\bigg]\\\\ &amp;amp;= \\mathbb{E}_q\\bigg[\\log \\frac{q(\\mathbf{x}_1|\\mathbf{x}_{0})}{p_\\theta(\\mathbf{x}_{0}|\\mathbf{x}_1)} + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_0) q(\\mathbf{x}_t|\\mathbf{x}_0)}{q(\\mathbf{x}_{t-1}|\\mathbf{x}_0)p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t)} -\\log p_\\theta(\\mathbf{x}_T)\\bigg]\\\\ &amp;amp;= \\mathbb{E}_q\\bigg[\\log \\frac{q(\\mathbf{x}_1|\\mathbf{x}_{0})}{p_\\theta(\\mathbf{x}_{0}|\\mathbf{x}_1)} + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t)} + \\log \\frac{q(\\mathbf{x}_T|\\mathbf{x}_0)}{q(\\mathbf{x}_1|\\mathbf{x}_0)}-\\log p_\\theta(\\mathbf{x}_T)\\bigg]\\\\ &amp;amp;= \\mathbb{E}_q\\bigg[-\\log p_\\theta(\\mathbf{x}_0|\\mathbf{x}_1) + \\sum_{t=2}^T \\log \\frac{q(\\mathbf{x}_{t-1}|\\mathbf{x}_{t}, \\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t)}+ \\log \\frac{q(\\mathbf{x}_T|\\mathbf{x}_0)}{p_\\theta(\\mathbf{x}_T)}\\bigg].\\end{aligned}\\]Labeling each term:\\[\\begin{aligned}L_0 &amp;amp;= \\mathbb{E}_q[-\\log p_\\theta(\\mathbf{x}_0|\\mathbf{x}_1)], &amp;amp; \\\\L_{t} &amp;amp;= D_{\\operatorname{KL}}\\big(q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_{t}, \\mathbf{x}_0) \\big|\\big| p_\\theta(\\mathbf{x}_{t-1}|\\mathbf{x}_t)\\big), &amp;amp;t = 1, \\dots T-1, \\\\L_T &amp;amp;= D_{\\operatorname{KL}}\\big(q(\\mathbf{x}_T \\vert \\mathbf{x}_0) \\big|\\big| p_\\theta(\\mathbf{x}_T)\\big)\\big],\\end{aligned}\\]we get total objective\\[L_{\\operatorname{ELBO}}= \\sum_{t=0}^{T} L_t.\\]Last term $L_T$ can be ignored, as $q$ doesn’t depend on $\\theta$ and $p_\\theta(\\mathbf{x}_T)$ is isotropic Gaussian. All KL divergences in equation above are comparisons between Gaussians, so they can be calculated with closed form expressions instead of high variance Monte Carlo estimates. One can estimate $\\color{#5286A5}{\\tilde\\mu(\\mathbf{x}_t, \\mathbf{x}_0)}$ directly with\\[L_t = \\mathbb{E}_q \\Big[ \\frac{1}{2\\sigma_t^2} \\|{\\color{#5286A5}{\\tilde\\mu(\\mathbf{x}_t, \\mathbf{x}_0)}} - \\mu_\\theta(\\mathbf{x}_t, t) \\|^2 \\Big] + C,\\]where $C$ is some constant independent of $\\theta$. However Ho et al. propose a different way - train neural network $\\epsilon_\\theta(\\mathbf{x}_t, t)$ to predict the noise.We can start from reformulation of $q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)$. First, note that\\[\\begin{aligned}\\log q(\\mathbf{x}_t|\\mathbf{x}_{t-1}, \\mathbf{x}_0) &amp;amp;\\propto - {\\frac{(\\mathbf{x}_t - \\sqrt{\\alpha_t} \\mathbf{x}_{t-1})^2}{\\beta_t}} = - {\\frac{\\mathbf{x}_t^2 - 2 \\sqrt{\\alpha_t} \\mathbf{x}_t{\\color{#5286A5}{\\mathbf{x}_{t-1}}} + {\\alpha_t} {\\color{#C19454}{\\mathbf{x}_{t-1}^2}}}{\\beta_t}},\\\\\\log q(\\mathbf{x}_{t-1}|\\mathbf{x}_0) &amp;amp;\\propto -{\\frac{(\\mathbf{x}_{t-1} - \\sqrt{\\bar\\alpha_{t-1}} \\mathbf{x}_{0})^2}{1-\\bar\\alpha_{t-1}}} = - {\\frac{ {\\color{#C19454} {\\mathbf{x}_{t-1}^2} } - 2\\sqrt{\\bar\\alpha_{t-1}}{\\color{#5286A5}{\\mathbf{x}_{t-1}}} \\mathbf{x}_{0} + \\bar\\alpha_{t-1}\\mathbf{x}_{0}^2}{1-\\bar\\alpha_{t-1}}}.\\end{aligned}\\]Then, using Bayesian rule we have:\\[\\begin{aligned}\\log q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) &amp;amp; = \\log q(\\mathbf{x}_t|\\mathbf{x}_{t-1}, \\mathbf{x}_0) + \\log q(\\mathbf{x}_{t-1}|\\mathbf{x}_0) - \\log q(\\mathbf{x}_{t}|\\mathbf{x}_0)\\\\ &amp;amp; \\propto {-\\color{#C19454}{(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}}) \\mathbf{x}_{t-1}^2}} + {\\color{#5286A5}{(\\frac{2\\sqrt{\\alpha_t}}{\\beta_t}\\mathbf{x}_t + \\frac{2\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t-1}}\\mathbf{x}_0 )\\mathbf{x}_{t-1}}} + f(\\mathbf{x}_t, \\mathbf{x}_0),\\end{aligned}\\]where $f$ is some function independent of $\\mathbf{x}_{t-1}$.Now following the standard Gaussian density function, the mean and variance can be parameterized as follows (recall that $\\alpha_t +\\beta_t=1$):\\[{\\color{#C19454}{\\tilde \\beta_t}} = \\Big(\\frac{\\alpha_t}{\\beta_t} + \\frac{1}{1-\\bar{\\alpha}_{t-1}}\\Big)^{-1} = \\Big(\\frac{\\alpha_t-\\bar{\\alpha}_{t}+\\beta_t}{\\beta_t (1-\\bar{\\alpha}_{t-1})}\\Big)^{-1} = \\beta_t \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_{t}}\\]and\\[\\begin{aligned}{\\color{#5286A5}{\\tilde\\mu(\\mathbf{x}_t, \\mathbf{x}_0)}} &amp;amp;= \\Big( \\frac{\\sqrt{\\alpha_t}}{\\beta_t}\\mathbf{x}_t + \\frac{\\sqrt{\\bar{\\alpha}_{t-1}}}{1-\\bar{\\alpha}_{t-1}}\\mathbf{x}_0 \\Big) \\cdot \\color{#C19454}{\\tilde \\beta_t} \\\\ &amp;amp;= \\frac{\\sqrt{\\alpha_t}(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t} \\mathbf{x}_t + \\frac{\\sqrt{\\bar\\alpha_{t-1}}}{1-\\bar\\alpha_t}\\mathbf{x}_0.\\end{aligned}\\]Using representation $\\mathbf{x}_0 = \\frac{1}{\\sqrt{\\bar\\alpha_t}}(\\mathbf{x}_t - \\sqrt{1-\\bar\\alpha_t}\\epsilon)$ we get\\[\\begin{aligned} L_t &amp;amp;= \\mathbb{E}_q \\Big[ \\frac{1}{2\\sigma_t^2} \\|{\\color{#5286A5}{\\tilde\\mu(\\mathbf{x}_t, \\mathbf{x}_0)}} - \\mu_\\theta(\\mathbf{x}_t, t) \\|^2 \\Big]\\\\ &amp;amp;= \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\Big[ \\frac{1}{2\\sigma_t^2} \\Big \\|{\\color{#5286A5}{\\frac{1}{\\sqrt{\\bar\\alpha_t}}\\Big(\\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon\\Big)}} - \\frac{1}{\\sqrt{\\bar\\alpha_t}}\\Big(\\mathbf{x}_t - \\frac{\\beta_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(\\mathbf{x}_t, t)\\Big) \\Big \\|^2 \\Big]\\\\ &amp;amp;= \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\Big[ \\frac{\\beta_t^2}{2\\sigma_t^2 \\bar\\alpha_t (1-\\bar\\alpha_t)} \\Big \\|{\\color{#5286A5}{\\epsilon}} - \\epsilon_\\theta(\\mathbf{x}_t, t) \\Big \\|^2 \\Big]\\end{aligned}\\]Empirically, Ho et al. found that training the diffusion model works better with a simplified objective that ignores the weighting term:\\[L_t^{\\text{simple}} = \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\big[ \\|\\epsilon - \\epsilon_\\theta(\\mathbf{x}_t, t) \\|^2 \\big] = \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\big[ \\|\\epsilon - \\epsilon_\\theta(\\sqrt{\\bar\\alpha_t}\\mathbf{x}_0+\\sqrt{1-\\bar\\alpha_t} \\epsilon, t) \\|^2 \\big]\\]Diffusion models often use U-Net architectures with ResNet blocks and self-attention layers to represent $\\epsilon_\\theta(\\mathbf{x}_t, t)$. Time features (usually sinusoidal positional embeddings or random Fourier features) are fed to the residual blocks using either simple spatial addition or using adaptive group normalization layers. Image source.To summarize, our training process: Sample $\\mathbf{x}_0 \\sim q(\\mathbf{x}_0)$ Choose randomly a certain step in diffusion process: $t \\sim \\mathcal{U}(\\lbrace 1,2, \\dots T \\rbrace)$ Apply noising: $\\mathbf{x}_t = \\sqrt{\\bar\\alpha_t}\\mathbf{x}_0+\\sqrt{1-\\bar\\alpha_t} \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})$ Take a gradient step on\\(\\nabla_\\theta \\| \\epsilon - \\epsilon_\\theta(\\mathbf{x}_t, t) \\|^2\\) Repeat until convergeimport jax.numpy as jnpfrom jax import grad, jit, vmap, random# hyperparametersimg_shape = (256, 256)T = 1000key = random.PRNGKey(42)# linear schedulebetas = jnp.linspace(1e-4, 0.02, T)alphas = 1 - betasalpha_bars = jnp.cumprod(alphas)# initial model weightsdummy = random.uniform(key, shape=img_shape)params = model.init(key, dummy)@jitdef loss(params, eps, x_t, t): return jnp.sum((eps - model.apply(params, x_t, t)) ** 2)@jitdef apply_noising(img, a, noise): return jnp.sqrt(a) * img + jnp.sqrt(1 - a) * noise def train_step(x_0): # choose random steps t = random.randint(key, shape=(x_0.shape[0],), minval=0, maxval=T) # add noise eps = random.normal(key, shape=x_0.shape) x_t = jit(vmap(apply_noising))(x_0, alpha_bars[t], eps) # calculate gradients grads = jit(grad(loss))(params, eps, x_t) # update parameters with gradients and your favourite optimizer ...Inference process consists of the following steps: Sample $\\mathbf{x}_T \\sim \\mathcal{N}(0, \\mathbf{I})$ For $t = T, \\dots, 1$\\[\\mathbf{x}_{t-1} = \\mu_\\theta(\\mathbf{x}_t, t) + \\sigma_t \\epsilon,\\]where $\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})$ and\\[\\mu_\\theta(\\mathbf{x}_t, t) = \\frac{1}{\\sqrt{\\bar\\alpha_t}}\\Big(\\mathbf{x}_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(\\mathbf{x}_t, t) \\Big).\\] Return $\\mathbf{x}_0$def sample_step(params, x_t, t): eps = model.apply(params, x_t, t) mu_t = x_t - eps * (1 - alphas[t]) / jnp.sqrt(1 - alpha_bars[t]) mu_t /= jnp.sqrt(alpha_bars[t]) return mu_t + sigma_t * random.normal(key, shape=x_0.shape)def sample(): x_t = random.normal(key, shape=img_shape) for t in reversed(range(T)): x_t = sample_step(params, x_t, t) return x_tDenoising diffusion implicit models (DDIM)A critical drawback of these models is that they require many iterations to produce a high quality sample. Reverse diffusion process could have thousands of steps and iterating over all the steps is required to produce a single sample, which is much slower compared to GANs, which only needs one pass through a network. For example, it takes around 20 hours to sample 50k images of size 32 × 32 from a DDPM, but less than a minute to do so from a GAN on a Nvidia 2080 Ti GPU. This becomes more problematic for larger images as sampling 50k images of size 256 × 256 could take nearly 1000 hours on the same GPU.One simple acceleration method is to reduce diffusion time steps in training. Another one is strided sampling schedule: take sampling update every $[T/S]$ steps to reduce process from $T$ down to $S$ steps. However, both of them lead to immediate worse performance.In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process, meaning that each event $t$ depends only on the state attained in the previous event $t-1$. Song et al. generalized DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective.We can redefine joint distribution $q(\\mathbf{x}_{1 : T} \\vert \\mathbf{x}_0)$ in a way such that forward process is non-Markovian, while marginals stay the same. Let\\[\\begin{aligned}\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0 &amp;amp;= \\sqrt{\\bar\\alpha_{t-1}} \\mathbf{x}_0 + \\sqrt{1 - \\bar\\alpha_{t-1}} \\epsilon_{t-1} &amp;amp; \\color{Salmon}{\\epsilon_{t-1} \\sim \\mathcal{N}(0, \\mathbf{I})} \\\\ &amp;amp; = \\sqrt{\\bar\\alpha_{t-1}} \\mathbf{x}_0 + \\sqrt{1 - \\bar\\alpha_{t-1} - \\sigma_t^2} \\epsilon_{t} + \\sigma_t \\epsilon &amp;amp; \\color{Salmon}{\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})}\\\\ &amp;amp; = \\sqrt{\\bar\\alpha_{t-1}} \\mathbf{x}_0 + \\sqrt{1 - \\bar\\alpha_{t-1} - \\sigma_t^2} \\frac{\\mathbf{x}_t - \\sqrt{\\bar\\alpha_t}\\mathbf{x}_0}{\\sqrt{1-\\bar\\alpha_t}} + \\sigma_t \\epsilon\\end{aligned}\\]with some deviation process $\\sigma_t$. Then we have\\[q_\\sigma(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}(\\sqrt{\\bar\\alpha_{t-1}} \\mathbf{x}_0 + \\sqrt{1 - \\bar\\alpha_{t-1} - \\sigma_t^2} \\frac{\\mathbf{x}_t - \\sqrt{\\bar\\alpha_t}\\mathbf{x}_0}{\\sqrt{1-\\bar\\alpha_t}}, \\sigma^2_t \\mathbf{I})\\]and\\[q_\\sigma(\\mathbf{x}_{t} \\vert \\mathbf{x}_0) = \\mathcal{N}(\\sqrt{\\bar\\alpha_{t}} \\mathbf{x}_0, \\sqrt{1 - \\bar\\alpha_{t}} \\mathbf{I}).\\]Recall that for Markovian diffusion process we have distribution\\[q(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0) = \\mathcal{N}({\\color{#5286A5}{\\tilde \\mu(\\mathbf{x}_t, \\mathbf{x}_0)}}, {\\color{#C19454}{\\tilde \\beta_t \\mathbf{I}}})\\]with\\[{\\color{#C19454}{\\tilde \\beta_t}} = \\frac{1-\\bar\\alpha_{t-1}}{1-\\bar\\alpha_t} \\beta_t.\\]Hence, we can parameterize distribution $q_\\sigma(\\mathbf{x}_{t-1} \\vert \\mathbf{x}_t, \\mathbf{x}_0)$ with hyperparamer $\\eta &amp;gt; 0$, such that\\[\\sigma_t^2 = \\eta {\\color{#C19454}{\\tilde \\beta_t}}.\\]Different choices of $\\eta$ result in different generative processes, all while using the same model $\\epsilon_\\theta$, so re-training the model is unnecessary. The special case of $\\eta = 1$ corresponds to DDPM. Setting $\\eta = 0$ makes the sampling process deterministic. Such model is named the denoising diffusion implicit model (DDIM). In general, one can generate samples in autoregressive way by formula\\[\\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}(\\mathbf{x}_t - \\sqrt{1-\\bar\\alpha_t}\\epsilon_\\theta(\\mathbf{x}_t, t)) + \\sqrt{1-\\bar\\alpha_{t-1} - \\sigma_t^2} \\epsilon_\\theta(\\mathbf{x}_t, t) + \\sigma_t \\epsilon.\\]We can accelerate inference process by only sampling a subset of $S$ diffusion steps $\\lbrace \\tau_1, \\dots, \\tau_S \\rbrace$.Accelerated generation with DDIM and $\\tau = [1, 3]$.def sample_step(params, x_t, t): eps = model.apply(params, x_t, t) x_0_scaled = (x_t - eps * jnp.sqrt(1 - alpha_bars[t])) / jnp.sqrt(alphas[t]) mu_t = x_0_scaled + jnp.sqrt(1 - alpha_bars[t-1] - sigma_t ** 2) return mu_t + sigma_t * random.normal(key, shape=x_0.shape) def sample(taus): x_t = random.normal(key, shape=img_shape) for t in reversed(taus): x_t = sample_step(params, x_t, t) return x_tScore based generative modellingDiffusion model is an example of discrete Markov chain. We can extend it to continuous stochastic process. Let’s define Wiener process (Brownian motion) $\\mathbf{w}_t$ - a random process, such that it starts with $0$, its samples are continuous paths and all of its increments are independent and normally distributed, i.e.\\[\\frac{\\mathbf{w}(t) - \\mathbf{w}(s)}{\\sqrt{t - s}} \\sim \\mathcal{N}(0, \\mathbf{I}), \\quad t &amp;gt; s.\\]Let also\\[\\mathbf{x}\\big(\\frac{t}{T}\\big) := \\mathbf{x}_t \\ \\text{ and } \\ \\beta\\big(\\frac{t}{T}\\big) := \\beta_t \\cdot T,\\]then\\[\\mathbf{x}\\big(\\frac{t + 1}{T}\\big) = \\sqrt{1-\\frac{\\beta(t/T)}{T}} \\mathbf{x}(t/T) + \\sqrt{\\beta(t/T)} \\Big( \\mathbf{w}\\big(\\frac{t+1}{T}\\big)-\\mathbf{w}\\big(\\frac{t}{T}\\big) \\Big).\\]Rewriting equation above with $t:=\\frac{t}{T}$ and $\\Delta t := \\frac{1}{T}$, we get\\[\\begin{aligned}\\mathbf{x}(t+\\Delta t) &amp;amp;= \\sqrt{1-\\beta(t)\\Delta t} \\mathbf{x}(t) + \\sqrt{\\beta(t)} (\\mathbf{w}(t + \\Delta t)-\\mathbf{w}(t)) \\\\&amp;amp; \\approx \\Big(1 - \\frac{\\beta(t) \\Delta t}{2} \\Big) \\mathbf{x}(t) + \\sqrt{\\beta(t)}(\\mathbf{w}(t + \\Delta t)-\\mathbf{w}(t)). &amp;amp; \\color{Salmon}{\\leftarrow \\text{Taylor expansion}}\\end{aligned}\\]With $\\Delta t \\rightarrow 0$ this converges to stochastic differential equation (SDE):\\[d\\mathbf{x} = -\\frac{1}{2}\\beta(t)\\mathbf{x}dt + \\sqrt{\\beta(t)} d\\mathbf{w}.\\]The equation of type\\[d\\mathbf{x} = f(\\mathbf{x}, t)dt + g(t)d\\mathbf{w}\\]has a unique strong solution as long as the coefficients are globally Lipschitz in both state and time (Øksendal (2003)). We hereafter denote by $q_t(\\mathbf{x})$ probability density of $\\mathbf{x}(t)$.By starting from samples of $\\mathbf{x}_T \\sim q_T(\\mathbf{x})$ and reversing the process, we can obtain samples $\\mathbf{x}_0 \\sim q_0(\\mathbf{x})$. It was proved by Anderson (1982) that the reverse of a diffusion process is also a diffusion process, running backwards in time and given by the reverse-time SDE:\\[\\begin{aligned}d\\mathbf{x} = [f(\\mathbf{x}, t) - g(t)^2 &amp;amp;\\underbrace{\\nabla_{\\mathbf{x}} \\log q_t(\\mathbf{x})}]dt + g(t) d\\bar{\\mathbf{w}}, &amp;amp;\\\\&amp;amp;\\color{Salmon}{\\text{Score Function}} \\\\\\end{aligned}\\]where $\\bar{\\mathbf{w}}$ is a standard Wiener process when time flows backwards from $T$ to $0$. In our case with\\[f(\\mathbf{x},t) = -\\frac{1}{2}\\beta(t)\\mathbf{x}(t) \\ \\text{ and } \\ g(t) = \\sqrt{\\beta(t)}\\]we have reverse diffusion process\\[d\\mathbf{x} = \\big[-\\frac{1}{2}\\beta(t)\\mathbf{x} - \\beta(t) \\nabla_{\\mathbf{x}} \\log q_t(\\mathbf{x})\\big] dt + \\sqrt{\\beta(t)}d\\bar{\\mathbf{w}}.\\]Once the score of each marginal distribution is known for all $t$, we can map data to a noise (prior) distribution with a forward SDE, and reverse this SDE for to sample from $q_0$.Forward and reverse generative diffusion SDEs.In order to estimate $\\nabla_{\\mathbf{x}} \\log q_t(\\mathbf{x})$ we can train a time-dependent score-based model $\\mathbf{s}_\\theta(\\mathbf{x}, t)$, such that\\[\\mathbf{s}_\\theta(\\mathbf{x}, t) \\approx \\nabla_{\\mathbf{x}} \\log q_t(\\mathbf{x}).\\]The marginal diffused density $q_t(\\mathbf{x}(t))$ is not tractable, however,\\[q_t(\\mathbf{x}(t) \\vert \\mathbf{x}(0)) \\sim \\mathcal{N}(\\sqrt{\\bar{\\alpha}(t)} \\mathbf{x}(0), (1 - \\bar{\\alpha}(t)) \\mathbf{I})\\]with $\\bar{\\alpha}(t) = e^{\\int_0^t \\beta(s) ds}$. Therefore we can minimize\\[\\mathcal{L} = \\mathbb{E}_{t \\sim \\mathcal{U}(0, t)} \\mathbb{E}_{\\mathbf{x}(0) \\sim q_0(\\mathbf{x})} \\mathbb{E}_{\\mathbf{x}(t) \\sim q_t(\\mathbf{x}(t) \\vert \\mathbf{x}(0))}[ \\| \\mathbf{s}_\\theta(\\mathbf{x}(t), t) - \\nabla_{\\mathbf{x}(t)} \\log q_t(\\mathbf{x}(t) \\vert \\mathbf{x}(0)) \\|^2 ].\\]Connection to diffusion modelGiven a Gaussian distribution\\[\\mathbf{x}(t) = \\sqrt{\\bar{\\alpha}(t)} \\mathbf{x}(0) + \\sqrt{1 - \\bar{\\alpha}(t)} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I}),\\]we can write the derivative of the logarithm of its density function as\\[\\begin{aligned}\\nabla_{\\mathbf{x}(t)} \\log q_t(\\mathbf{x}(t) \\vert \\mathbf{x}(0)) &amp;amp;= -\\nabla_{\\mathbf{x}(t)} \\frac{(\\mathbf{x}(t) - \\sqrt{\\bar{\\alpha}(t)} \\mathbf{x}(0))^2}{2 (1 - \\bar{\\alpha}(t))} \\\\&amp;amp;= -\\frac{\\mathbf{x}(t) - \\sqrt{\\bar{\\alpha}(t)} \\mathbf{x}(0)}{1 - \\bar{\\alpha}(t)} \\\\&amp;amp;= \\frac{\\epsilon}{\\sqrt{1 - \\bar{\\alpha}(t)}}.\\end{aligned}\\]Also,\\[\\mathbf{s}_\\theta(\\mathbf{x}, t) = -\\frac{\\epsilon_\\theta(\\mathbf{x}, t)}{\\sqrt{1 - \\bar{\\alpha}(t)}}.\\]Guided diffusionOnce the model $\\epsilon_\\theta(\\mathbf{x}_t, t)$ is trained, we can use it to run the isotropic Gaussian distribution $\\mathbf{x}_T$ back to $\\mathbf{x}_0$ and generate limitless image variations. But how can we guide the class-conditional model to generate specific images by feeding additional information about class $y$ during the training process?Classifier guidanceIf we have a differentiable discriminative model $f_\\phi(y \\vert \\mathbf{x}_t)$, trained to classify noisy images $\\mathbf{x}_t$, we can use its gradients to guide the diffusion sampling process toward the conditioning information $y$ by altering the noise prediction.We can write the score function for the joint distribution $q(\\mathbf{x}, y)$ as following,\\[\\begin{aligned}\\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t, y) &amp;amp;= \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t) + \\nabla_{\\mathbf{x}_t} \\log q(y \\vert \\mathbf{x}_t) \\\\&amp;amp; \\approx -\\frac{\\epsilon_\\theta(\\mathbf{x}_t, t)}{\\sqrt{1 - \\bar{\\alpha}_t}} + \\nabla_{\\mathbf{x}_t} \\log f_\\phi (y \\vert \\mathbf{x}_t)\\\\ &amp;amp;= -\\frac{1}{\\sqrt{1 - \\bar{\\alpha}_t}}\\big(\\epsilon_\\theta(\\mathbf{x}_t, t) - \\sqrt{1 - \\bar{\\alpha}_t}\\nabla_{\\mathbf{x}_t} \\log f_\\phi (y \\vert \\mathbf{x}_t)\\big).\\end{aligned}\\]At each step of denoising, the classifier checks whether the image is denoised in the right direction and contributes its own gradient of loss function into the overall loss of diffusion model. To control the strength of the classifier guidance, we can add a weight $\\omega$, called the guidance scale, and here is our new classifier-guided model:\\[\\tilde{\\epsilon}_\\theta(\\mathbf{x}_t, t) = \\epsilon_\\theta(\\mathbf{x}_t, t) - \\omega \\sqrt{1 - \\bar{\\alpha}_t} \\nabla_{\\mathbf{x}_t} \\log f_\\phi (y \\vert \\mathbf{x}_t).\\]We can then use the exact same sampling procedure, but with the modified noise predictions $\\tilde{\\epsilon}_\\theta$. This results in approximate sampling from distribution:\\[\\tilde{q}(\\mathbf{x}_t \\vert y) \\propto q(\\mathbf{x}_t) \\cdot q(y \\vert \\mathbf{x}_t)^\\omega.\\]Basically, we are raising the conditional part of the distribution to a power, which corresponds to tuning the inverse temperature of that distribution. With large $\\omega$ we focus onto distribution modes and produce higher fidelity (but less diverse) samples.Guidance on a toy 2D example of three classes, in which the conditional distribution for each class is an isotropic Gaussian, each mixture component representing data conditioned on a class. The leftmost plot is the non-guided marginal density. Left to right are densities of mixtures of normalized guided conditionals with increasing guidance strength. Image sourceA downside of classifier guidance is that it requires an additional classifier model and thus complicates the training pipeline. One can’t plug in a standard pre-trained classifier, because this model has to be trained on noisy data $\\mathbf{x}_t$. And even having a classifier, which is robust to noise, classifier guidance is inherently limited in its effectiveness. Most of the information in the input $\\mathbf{x}_t$ is not relevant to predicting $y$, and as a result, taking the gradient of the classifier w.r.t. its input can yield arbitrary (and even adversarial) directions in input space.Classifier-free guidanceHo &amp;amp; Salimans proposed an alternative method, a classifier-free guidance, which doesn’t require training a separate classifier. Instead, one trains a conditional diffusion model, parameterized by $\\epsilon_\\theta(\\mathbf{x}_t, t \\vert y)$ with conditioning dropout: 10-20% of the time, the conditioning information $y$ is removed. In practice, it is replaced with a special input value $y=\\emptyset$ representing the absence of conditioning information. This way model knows how to generate images unconditionally as well, i.e.\\[\\epsilon_\\theta(\\mathbf{x}_t, t) = \\epsilon_\\theta(\\mathbf{x}_t, t \\vert \\emptyset).\\]How could we use it for sampling? By Bayes rule we have\\[\\begin{aligned}q(y \\vert \\mathbf{x}_t) &amp;amp;= \\frac{q(\\mathbf{x}_t \\vert y) q(y)}{q(\\mathbf{x}_t)} \\\\\\Longrightarrow \\nabla_{\\mathbf{x}_t} \\log q(y \\vert \\mathbf{x}_t) &amp;amp;= \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t \\vert y) - \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t) \\\\&amp;amp; \\approx -\\frac{\\epsilon_\\theta(\\mathbf{x}_t, t \\vert y) - \\epsilon_\\theta(\\mathbf{x}_t, t)}{\\sqrt{1 -\\bar{\\alpha}_t}}. \\end{aligned}\\]Substituting this into the formula for classifier guidance, we get\\[\\begin{aligned}\\tilde{\\epsilon}_\\theta(\\mathbf{x}_t, t \\vert y) &amp;amp;= \\epsilon_\\theta(\\mathbf{x}_t, t) - \\omega \\sqrt{1 - \\bar{\\alpha}_t} \\nabla_{\\mathbf{x}_t} \\log q(y \\vert \\mathbf{x}_t)\\\\ &amp;amp;= (1-\\omega) \\epsilon_\\theta(\\mathbf{x}_t, t) + \\omega\\epsilon_\\theta(\\mathbf{x}_t, t \\vert y).\\end{aligned}\\]The classifier-free guided model is a linear interpolation between models with and without labels: for $\\omega=0$ we get unconditional model, and for $\\omega=1$ we get the standard conditional model. However, as experiments have shown in Dhariwal &amp;amp; Nichol paper, guidance works even better with $\\omega &amp;gt; 1$.Note on notation: authors of original paper applied classifier guidance to already conditional diffusion model $\\epsilon(\\mathbf{x}_t, t \\vert y)$:\\[\\begin{aligned}\\tilde{\\epsilon}_\\theta(\\mathbf{x}_t, t \\vert y) &amp;amp;= \\epsilon_\\theta(\\mathbf{x}_t, t \\vert y) - \\omega \\sqrt{1 - \\bar{\\alpha}_t} \\nabla_{\\mathbf{x}_t} \\log q(y \\vert \\mathbf{x}_t)\\\\ &amp;amp;= (\\omega + 1) \\epsilon_\\theta(\\mathbf{x}_t, t \\vert y) - \\omega\\epsilon_\\theta(\\mathbf{x}_t, t).\\end{aligned}\\]This is the same as applying guidance to unconditional model with $\\omega + 1$ scale, because\\[\\tilde{q}(\\mathbf{x}_t \\vert y) \\propto q(\\mathbf{x}_t \\vert y) \\cdot q(y \\vert \\mathbf{x}_t)^\\omega \\propto q(\\mathbf{x}_t) \\cdot q(y \\vert \\mathbf{x}_t)^{\\omega+1}.\\]CLIP guidanceWith CLIP guidance the classifier is replaced with a CLIP model (abbreviation for Contrastive Language-Image Pre-training). CLIP was originally a separate auxiliary model to rank the results from generative model, called DALL·E. DALL·E was the first public system capable of creating images based on a textual description from OpenAI, however it was not a diffusion model and is therefore out of the scope for this post. DALL·E’s name is a portmanteau of the names of animated robot Pixar character WALL-E and the Spanish surrealist artist Salvador Dalí.The idea behind CLIP is fairly simple: Take two encoders, one for a text snippet and another one for an image Collect a sufficiently large dataset of image-text pairs (e.g. 400 million scraped from the Internet in the paper) Train the model in a contrastive fashion: it must produce high similarity score for an image and a text from the same pair and a low similarity score for mismatched image and text.CLIP approach: jointly train an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes. The classes can be adjustable without retraining a model.# image_encoder - ResNet or Vision Transformer# text_encoder - CBOW or Text Transformer# I[n, h, w, c] - minibatch of aligned images# T[n, l] - minibatch of aligned texts# W_i[d_i, d_e] - learned proj of image to embed# W_t[d_t, d_e] - learned proj of text to embed# tau - learned temperature parameter# extract feature representations of each modalityI_f = image_encoder(I) #[n, d_i]T_f = text_encoder(T) #[n, d_t]# joint multimodal embedding [n, d_e]I_e = l2_normalize(jnp.dot(I_f, W_i), axis=1)T_e = l2_normalize(jnp.dot(T_f, W_t), axis=1)# scaled pairwise cosine similarities [n, n]logits = jnp.dot(I_e, T_e.T) * jnp.exp(tau)# symmetric loss functionlabels = jnp.arange(n)loss_i = cross_entropy_loss(logits, labels, axis=0)loss_t = cross_entropy_loss(logits, labels, axis=1)loss = (loss_i + loss_t) / 2Let $f(\\mathbf{x})$ and $g(y)$ be image and text encoders respectively. Then CLIP loss for $(i, j)$ pair is\\[\\begin{aligned}\\mathcal{L}_{\\operatorname{CLIP}}(i, j) &amp;amp;= \\frac{1}{2} \\bigg(-\\log \\frac{\\exp(f(\\mathbf{x}_i) \\cdot g(y_j) / \\tau)}{\\sum_k \\exp(f(\\mathbf{x}_i) \\cdot g(y_k) / \\tau)}-\\log \\frac{\\exp(f(\\mathbf{x}_i) \\cdot g(y_j) / \\tau)}{\\sum_k \\exp(f(\\mathbf{x}_k) \\cdot g(y_j) / \\tau)} \\bigg).\\end{aligned}\\]Ideally, we get\\[f(\\mathbf{x}) \\cdot g(y) \\approx \\frac{q(\\mathbf{x}, y)}{q(\\mathbf{x}) q(y)} = \\frac{q(y \\vert \\mathbf{x})}{ q(y)},\\]which can be used to steer generative models instead of pretrained classifier:\\[\\begin{aligned}\\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t, y) &amp;amp;= \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t) + \\nabla_{\\mathbf{x}_t} \\log q(y \\vert \\mathbf{x}_t) \\\\&amp;amp;= \\nabla_{\\mathbf{x}_t} \\log q(\\mathbf{x}_t) + \\nabla_{\\mathbf{x}_t} (\\log q(y \\vert \\mathbf{x}_t) -\\log q(y)) \\\\&amp;amp; \\approx -\\frac{\\epsilon_\\theta(\\mathbf{x}_t, t)}{\\sqrt{1 - \\bar{\\alpha}_t}} + \\nabla_{\\mathbf{x}_t} \\log (f(\\mathbf{x}_t) \\cdot g(y)).\\end{aligned}\\]Similar to classifier guidance, CLIP must be trained on noised images $\\mathbf{x}_t$ to obtain the correct gradient in the reverse process.GLIDEGLIDE, which stands for Guided Language to Image Diffusion for Generation and Editing, is a model by OpenAI that has beaten DALL·E, arguably presented the most novel and interesting ideas and yet received comparatively little publicity.Motivated by the ability of guided diffusion models to generate photorealistic samples and the ability of text-to-image models to handle free-form prompts, authors of the paper applied guided diffusion to the problem of text-conditional image synthesis. They also compared two techniques for guiding diffusion models towards text prompts: CLIP guidance and classifier-free guidance. Using human and automated evaluations, they found that classifier-free guidance yields higher-quality images.An architecture of GLIDE consists of three major blocks: U-Net based text-conditional diffusion model at 64 × 64 resolution (2.3B parameters) Transformer based text encoder to condition on natural language descriptions (1.2B parameters) Another text-conditional upsampling diffusion model to increase the resolution to 256 × 256 (1.5B parameters)The U-Net model as usual stacks residual layers with downsampling convolutions, followed by a stack of residual layers with upsampling convolutions, with skip connections connecting the layers with the same spatial size. It also consists of attention layers which are crucial for simultaneous text processing.The text encoder was built from 24 residual blocks of width 2048. The input is a sequence of $K$ tokens. The output of the transformer is used in two ways: the final token embedding token is used in place of a class embedding $y$, the final layer of token embeddings is added to every attention layer of the model.Model has fewer parameters than DALL·E (5B vs 12B) and was trained on the same dataset, however was favored over it by human evaluators and has beaten it by FID score. However, as the authors mentioned, unoptimized GLIDE takes 15 seconds to sample one image on a single A100 GPU. This is much slower than sampling for related GAN methods, which produce images in a single forward pass and are thus more favorable for use in real-time applications.DALL·E 2 (unCLIP)In April 2022, OpenAI released a new model, called DALL·E 2 (or unCLIP in the paper), which is a clever combination of CLIP and GLIDE. The CLIP model is trained separately on a data of image-text pairs $(\\mathbf{x}, y)$. Let\\[\\mathbf{z}_i = \\operatorname{CLIP}(\\mathbf{x}) \\quad \\text{and} \\quad \\mathbf{z}_t = \\operatorname{CLIP}(y)\\]be CLIP image and text embeddings respectively. In the paper Vision Transformer was used as image encoder and Transformer with a causal attention mask as text encoder. Then CLIP is frozen and the unCLIP learns two models in parallel: a special prior model $q(\\mathbf{z}_i \\vert y)$ outputs CLIP image embedding given the text $y$, a diffusion decoder (modified GLIDE) $q(\\mathbf{x} \\vert \\mathbf{z}_i, [y])$ generates the image $\\mathbf{x}$ given CLIP image embedding $\\mathbf{z}_i$ and optionally the original text $y$.Stacking these two components yields a generative model of images $\\mathbf{x}$ given captions $y$:\\[q(\\mathbf{x} \\vert y) = q(\\mathbf{x}, \\mathbf{z}_i \\vert y) = q(\\mathbf{x} \\vert \\mathbf{z}_i, y) \\cdot q(\\mathbf{z}_i \\vert y).\\]unCLIP architecture. Below the dotted line the text-to-image process is depicted. Given a text $y$ the CLIP text encoder generates a text embedding $\\mathbf{z}_t$. Then a diffusion or autoregressive prior model processes this CLIP text embedding to construct an image embedding $\\mathbf{z}_i$. Then a diffusion decoder generates images conditioned on CLIP image embeddings (and text). The decoder essentially inverts image embeddings back into imagesThe authors tested two model classes for the prior: Autoregressive prior quantizes image embedding to a sequence of discrete codes and predict them autoregressively. Diffusion prior models the continuous image embedding by diffusion models conditioned on $y$.Diffusion prior outperforms the autoregressive prior for comparable model size and reduced training compute. The diffusion prior also performs better than the autoregressive prior in pairwise comparisons against GLIDE.As opposed to the way of training proposed by Ho et al., predicting the unnoised image embedding directly instead of predicting the noise was a better fit. Meaning, that instead of\\[L_t = \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\big[ \\|\\epsilon - \\epsilon_\\theta(\\mathbf{x}_t, t \\vert y) \\|^2 \\big]\\]the unCLIP diffusion prior loss is\\[L_t = \\mathbb{E}_{\\mathbf{x}_0, \\epsilon} \\big[ \\| \\mathbf{z}_i - f_\\theta(\\mathbf{z}_i^{(t)}, t \\vert y) \\|^2 \\big],\\]where $f_\\theta$ stands for the prior model and $\\mathbf{z}_i^{(t)}$ is the noised image embedding.The bipartite latent representation enables several text-guided image manipulation tasks. For example, one can fix CLIP image embeddings $\\mathbf{z}_i$ and run decoder with different decoder latents $\\mathbf{x}_T$.Variations of an input image by encoding with CLIP and then decoding with a diffusion model. The variations preserve both semantic information like presence of a clock in the painting and the overlapping strokes in the logo, as well as stylistic elements like the surrealism in the painting and the color gradients in the logo, while varying the non-essential details.Or one can change $\\mathbf{z}_i$ towards the difference of the text CLIP embeddings $\\mathbf{z}_t$ of two prompts.Text diffs applied to images by interpolating between their CLIP image embeddings and a normalised difference of the CLIP text embeddings produced from the two descriptions. Decoder latent $\\mathbf{x}_T$ is kept as a constant.ImagenTwo months after the publication of DALL·E 2 Google Brain team presented Imagen. It uses a pre-trained T5-XXL language model instead of CLIP to encode text for image generation. The idea is that this model has vastly more context regarding language processing than a model trained only on the image captions, and so is able to produce more valuable embeddings without the need to additionally fine-tune it. Authors of the paper noted, that scaling text encoder is extremely efficient and more important than scaling diffusion model size.Next, the resolution is increased via super-resolution diffusion models. There is another key observation from authors: noise conditioning augmentation weakens information from low-resolution models, thus it is beneficial to use text conditioning as extra information input.Visualization of Imagen. Imagen uses a frozen text encoder to encode the input text into text embeddings. A conditional diffusion model maps the text embedding into a 64 × 64 image. Imagen further utilizes text-conditional super-resolution diffusion models to upsample the image, first 64 × 64 → 256 × 256, and then 256 × 256 → 1024 × 1024.Noise conditioning augmentationThe solution can be viewed as a sequence of diffusion models, which was called cascaded diffusion models in Ho et al. (2021). Noise conditioning augmentation between these models is crucial to the final image quality, which is to apply strong data augmentation to the low-resolution image $\\mathbf{z}$ of each super-resolution model $p_\\theta(\\mathbf{x} \\vert \\mathbf{z})$. In simple terms, it is equivalent to applying various data augmentation techniques, such as a Gaussian noise/blur, to a low-resolution image before it is fed into the super-resolution models.def train_step_base(z_0): # diffusion forward process s = random.randint(key, shape=(z.shape[0],), minval=0, maxval=T) eps = random.normal(key, shape=z.shape) z_s = jit(vmap(apply_noising))(z, alpha_bars[s], eps) # optimize loss(z_0, model(z_s, s)) ... def train_step_sr(z_0, x_0): # add gaussian conditioning augmentation to the low-resolution image s = random.randint(key, shape=(z_0.shape[0],), minval=0, maxval=T) eps_z = random.normal(key, shape=z_0.shape) z_0 = jit(vmap(apply_noising))(z_0, alpha_bars[s], eps_z) # diffusion forward process t = random.randint(key, shape=(x_0.shape[0],), minval=0, maxval=T) eps_x = random.normal(key, shape=x_0.shape) x_t = jit(vmap(apply_noising))(x_0, alpha_bars[t], eps_x) # optimize loss(x_0, model(x_t, z_0, t, s)) ...In addition, there are also two similar forms of conditioning augmentation that require small modification to the training process: Truncated conditioning augmentation stops the diffusion process early at step $t &amp;gt; 0$ for low resolution. Non-truncated conditioning augmentation runs the full low resolution reverse process until step $0$ but then corrupt it by $\\mathbf{z}_t’ \\sim q(\\mathbf{z}_t \\vert \\mathbf{z}_0)$ and then feeds the corrupted $\\mathbf{z}_t’$ into the super-resolution model.def sample_step(params, x_t, t, condition=None): eps = model.apply(params, x_t, t, condition) mu_t = x_t - eps * (1 - alphas[t]) / jnp.sqrt(1 - alpha_bars[t]) mu_t /= jnp.sqrt(alpha_bars[t]) return mu_t + sigma_t * random.normal(key, shape=x_0.shape)def sample_base(): z_t = random.normal(key, shape=img_shape) for t in reversed(range(s, T)): z_t = sample_step(params, z_t, t) if not_using_truncated: for t in reversed(range(s)): z_t = sample_step(params, z_t, t) eps_z = random.normal(key, shape=z_t.shape) z_t = apply_noising(z_t, alpha_bars[s], eps_z) return z_tdef sample_sr(): # sample augmented low-resolution image z_0 = sample_base() # sample high-resolution image x_t = random.normal(key, shape=img_shape) for t in reversed(range(T)): x_t = sample_step(params, x_t, t, z_t) return x_tDynamic thresholdingAnother major key feature of Imagen is a so-called dynamic thresholding. Authors of the model found out that larger classifier-free guidance scale $\\omega$ leads to better text alignment, but worse image fidelity producing highly saturated and unnatural images. They hypothesised that large $\\omega$ increases train-test mismatch and generated images are saturated due to the very large gradient updates during sampling.At each sampling step $t$, the prediction $\\mathbf{x}_t$ must be within the same bounds as training data, i.e. within $[−1, 1]$, but authors of Imagen found empirically that high guidance weights cause predictions to exceed these bounds. To counter this problem, they proposed to adjust the pixel values of samples at each sampling step to be within this range. Basically, two approaches could be applied: Static thresholding: clip $\\mathbf{x}_t$ to $[-1, 1]$. Dynamic thresholding: at each sampling step $t$, compute $s$ as a certain $p$-percentile absolute pixel value; if $s &amp;gt; 1$ clip the prediction to $[-s, s]$ and divide by $s$.def sample(): x_t = random.normal(key, shape=img_shape) for t in reversed(range(T)): x_t = sample_step(params, x_t, t) if using_static: x_t = jnp.clip(x_t, -1.0, 1.0) else: s = jnp.percentile(jnp.abs(x_t), p, axis=tuple(range(1, x_t.ndim))) s = jnp.max(s, 1.0) x_t = jnp.clip(x_t, -s, s) / s return x_tLatent-space diffusion model / Stable diffusionRombach &amp;amp; Blattmann, et al. 2022 presented latent diffusion models (LDM), which operate in the latent space of pretrained variational autoencoders instead of pixel space, making training cost lower and inference speed faster.The diffusion and denoising processes happen on a 2D latent vector $\\mathbf{z}$, which is an image $\\mathbf{x}$, compressed by encoder. Then an decoder reconstructs the images from the latent vector. The paper explored two types of regularization in autoencoder training to avoid arbitrarily high-variance in the latent spaces: KL-regularization: a small KL penalty towards a standard normal distribution over the learned latent, similar to VAE. VQ-regularization: Uses a vector quantization layer within the decoder, like VQVAE but the quantization layer is absorbed by the decoder.The denoising model is a time-conditioned U-Net. Authors also introduced cross-attention layers into the model architecture to handle flexible conditioning information. Each type of conditioning information is paired with a domain-specific encoder $\\tau_\\theta$ to project the conditioning input $y$ to an intermediate representation, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing\\[\\operatorname{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\operatorname{softmax} \\Big( \\frac{\\mathbf{QK}^T}{\\sqrt{d}} \\Big) \\cdot \\mathbf{V},\\]where\\[\\mathbf{Q} = \\mathbf{W}_Q^{(i)} \\cdot \\varphi_i(\\mathbf{z}_t), \\ \\mathbf{K} =\\mathbf{W}_K^{(i)} \\cdot \\tau_\\theta(y), \\ \\mathbf{V} =\\mathbf{W}_V^{(i)} \\cdot \\tau_\\theta(y).\\]Here $\\varphi_i$ denotes a (flattened) intermediate representation of the UNet implementing $\\epsilon_\\theta$.The architecture of latent diffusion model.In 2022 in collaboration with Stability AI and Runway a text-to-image LDM called Stable Diffusion was released. Stable Diffusion was trained on 256x256 (then finetuned on 512x512) images from a subset of the LAION-Aesthetics V2 dataset, using 256 Nvidia A100 GPUs on Amazon Web Service for a total of 150,000 GPU-hours, at a cost of $600,000.Similar to Google’s Imagen, this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM.Stable diffusion samples.Unlike previous models, Stable Diffusion makes its source code available, along with pre-trained weights.ConclusionDiffusion models have shown amazing capabilities as generative models. They are both analytically tractable and flexible: they can be analytically evaluated and cheaply fit arbitrary structures in data. Besides text-conditioned image generation there a lot of interesting characteristics of the models which were not covered in this post, such as image in-/outpainting, style transfer and image editing.On the other hand, there is a shortcoming embedded into the diffusion process structure: the sampling relies on a long Markov chain of diffusion steps and it is still slower than GAN. Latent diffusion models have shown that performance gains can be achieved by learning semantically meaningful latent space with complex compressing encoders." }, { "title": "Applying Graph Neural Networks to Kaggle Competition", "url": "/posts/applying-graph-neural-networks-to-kaggle-competition/", "categories": "Graph Neural Networks", "tags": "kaggle, reinforcement learning", "date": "2022-07-24 19:00:00 +0300", "snippet": "Few months ago, Kaggle launched featured simulation competition Kore-2022. In this kind of competitions participants bots are competing against each other in an game environment, supported by Kaggle. Often there are 2 or 4 players in a game, at the end each winner/loser moves up/down according to skill rating system. Team reaching top-rating wins.Kore rulesThat’s how entry to Kore competition looks like: In this turn-based simulation game you control a small armada of spaceships. As you mine the rare mineral “kore” from the depths of space, you teleport it back to your homeworld. But it turns out you aren’t the only civilization with this goal. In each game two players will compete to collect the most kore from the board. Whoever has the largest kore cache by the end of 400 turns—or eliminates all of their opponents from the board before that—will be the winner!Fig. 1. Gameplay visualization by Tong Hui KangGame setup looks extremely complex at first glance (at least compared to the other Kaggle simulation competitions like Hungry Geese, which was basically a variation of Tetris Snake game).Here I’ll try to list the main rules: The game board is 21x21 tiles large and wraps around on both the north/south and east/west borders You start the game with 1 shipyard and 500 kore. Each turn you can either spawn new ships (each costs 10 kore), or launch fleet with a flight plan, or do nothing. Flight plan is a sequence of letters and integers: ‘N’, ‘E’, ‘S’ or ‘W’ defines the direction of the fleet. The integer following the letter determines how many steps (+1) fleet will take in that direction. For example, ‘NE2SW’ forms a circle trajectory, where fleet makes one step up, three steps right, one step down and then it moves left until it reaches starting point. The maximum length of the flight plan depends on fleet size by formula:\\[\\max \\text{length} = \\lfloor 2 \\log(\\text{fleet size}) \\rfloor + 1\\] E.g. the minimum number of ships for a fleet that can complete a circle like ‘NE2SW’ is 8. If plan ends with ‘C’, at the end tile of the flight fleet builds a shipyard, consuming 50 of its ships. Each fleet does damage to all enemy fleets orthogonally adjacent to it equal to its current ship count. Destroyed fleets drop 50% of their kore onto their currently occupied position with 50% being awarded to their attackers. If the attacking fleet does not survive, the kore is dropped back onto the tile instead. Fleets that move onto the same square collide. Allied fleets are absorbed into the largest fleet. The largest fleet survives and all other fleets will be destroyed. The remaining fleet loses ships equal to the ship count of the second largest fleet. The largest fleet steals all of the kore from other fleets it collides with.In a nutshell: you collect kore, spawn ships and try to destroy all of enemy objects before he destroys yours. The main difficulty for player is to construct an optimal path planning solution for fleet flights. Players must calculate many steps ahead as the effects of their actions in most cases do not appear immediately.Graph constructionNow how one can approach this task? Prior to this competition, Kaggle launched its trial version, Kore Beta, with the only difference being that it was 4-players game. First places were taken by rule-based agents, based mostly on simple heuristics.Another way is to think of this task as text generation. One can train a network which takes board image as input and returns flight plan as text output for each shipyard. A solution with this kind of approach took one of the top places eventually.One can also represent board as graph, where each tile is a node and edges connect adjacent tiles. Then each turn, when a fleet is sent from shipyard, searching for optimal flight plan is equivalent to searching for optimal path on a graph.Fig. 2. This is how periodic 21x21 grid looks like: a doughnut.One of the main advantages of an algorithm on a graph is rotation equivariance. Board is symmetric, therefore algorithm behaviour must not change if we swap players initial positions. With standard convolutional networks we can try to overcome this issue by using augmentations, but there is no need for them if we use graph neural networks.We also don’t want to miss the information which is not included in this kind of board representation: future positions of fleets. We know all of them by fact, because players plans are not hidden from each other. Therefore, it seems useful to add board representations at the next step, at the step after next and so on until we reach desirable amount of steps. Also, after we make our moves, these future graphs may change, so it doesn’t always seem reasonable to look far away from current step. In my implementation I was looking 12 steps ahead.Fig. 3. Graph zoomed in. Each node corresponds to a tile on the board, each edge represents that either nodes are spatial neighbors, or they are the same tiles but at adjacent timestamps. Graph is oriented, nodes from future graphs are pointing at nodes at previous steps. Thus, all the information flows to current board representation which will be used for actions generation later.Node input features $\\mathbf{v}_i$ contain kore amount on the tile, timestamp, total amount of kore collected by player and its opponent. Also, if any fleet is located on the tile - its cargo, collection rate and number of ships. If any shipyard is on the tile - its number of ships, maximum plan length for this number and maximum number of ships, which can be spawned.The edge input features $\\mathbf{e}_{ji}$ are $(1, 0, 0)^T$, $(0, 1, 0)^T$ and $(0, 0, 1)^T$ for temporal, lateral and longitudinal edges respectively. Basically, they are one-hot representations of these edge classes. Why treat spatial edges differently? When you create a flight plan, its length increases when fleet makes a turn: while for both “N1S” and “NEWS” fleet moves 2 tiles from shipyard and returns back, the first one is smaller and requires fewer ships.Graph Encoder ArchitectureIf we had to work with standard representations of board, we would most likely use convolutional layers. There exists an analogue of convolutions in a graph world, called, big surprise, graph convolutions. Similarly to ResNet architecture we can build a ResGCN encoder:Fig. 4. Bird’s eye view of graph encoder architecture. The summation of ResGCN input and output graphs is performed both over node and over edge features.It is also extremely important to make our graph neural network anisotropic, which means that each neighbor should have a different effect on the node depending on the weight of the edge between them. The idea is that the neural network transforms the nodes in such a way that the encoded features of the nodes lying on the agent’s path are more similar to each other than to nodes that are not.Fig. 5. ResGCN architecture. First, node features are encoded through several independent ResGCN blocks. Each block is a mapping $\\mathbb{R}^d \\rightarrow \\mathbb{R}^{d/n}$, where $n$ is a number of blocks. Outputs are concatenated together, constructing vector of the same size as the input. Then to get new edge features we pass outputs through feed-forward layers: $\\operatorname{Query}, \\operatorname{Key} \\in \\mathbb{R}^{d \\times 3}$, followed by element-wise multiplication and $\\operatorname{tanh}$ activation.ResGCN block consists of sequential graph convolutional layers:\\[\\operatorname{GCN}(v_i, e) = \\Theta^T \\sum_{j\\in\\mathcal{N}(i) \\cup \\lbrace i \\rbrace} \\frac{e_{ji}}{\\sqrt{\\hat{d}_i \\hat{d}_j}} v_j\\]with \\(\\hat{d}_i = 1 + \\sum_{j \\in \\mathcal{N}(i)}e_{ji}\\) and $\\mathcal{N}(i)$ - set of all neighbors for node $v_i$.Fig. 6. ResGCN block schema. GraphNorm layer normalizes node features over each graph in a batch.Imitation learningNow, we can train our network to imitate actions of best agents on a leaderboard. Each turn for each node with player shipyard on it, we have to decide for two things: Should we spawn, launch or do nothing? What is a number of ships to be spawned/launched?One can train a network with cross-entropy loss for the action and mean-squared loss for the ships number. However, due to the fact that flight plan length depends on discretized $2 \\log$ of fleet size, we can end up with errors like predicting number of ships to be $20$ with true number $21$ and thus having maximum plan length of $6.99$ and not being able to build a path with desired length of $7$. To avoid this we must split our policy into multiple classes, each representing maximum flight plan length: Do nothing Spawn Launch 1 ship (maximum length 1) Launch 2 ships (maximum length 2) Launch 3 or 4 ships (maximum length 3) … Launch 666, 667, …, 1096 ships (maximum length 14)Here in total we have 16 classes, but this amount can be reduced or increased, depending on what engineer thinks is a reasonable number. To choose a fleet size we can set our target to be a ratio of ships to total amount of ships in a shipyard (or to maximum spawn number in case of ‘spawn’ action).Fig. 7. Main policy heads. Encoded embedding is taken from node of current board graph representation, where shipyard is located on. This embedding is passed through linear layer to predict an expert action. In parallel, it is concatenated with one-hot representation of expert action and then goes through another feed-forward layers to predict expert spawn/launch ships ratio. On inference, $\\arg\\max$ of agent predictions can be used instead of expert action.Finally, we face path generation task. The idea is that for each node, starting from shipyard, we take all of its neighbors and itself, and predict next node in a flight plan. If it chooses node itself, we convert ships to a new shipyard (we can mask such action if fleet size is less than 50 or if fleet is already on a shipyard). To make the prediction dependent not only on the current node, but also on the entire sequence of previously selected nodes, we can use recurrent layers. It is also important to consider amount of space we have left for path generation.Fig. 8. Path policy head. We start at shipyard node. At each step all neighbor candidates, including node itself, are concatenated with one-hot representation of a ‘space left’ for plan generation. Then they are passed through recurrent layer. Received neighbor embeddings are passed through linear layer $\\operatorname{Query}$ and multiplied with node features passed through $\\operatorname{Key}$ layer. Resulting vector represents probabilities of each neighbor to be the next node. Then we move along path defined by an expert agent and repeat the same procedure.Now having probabilities for each move, we can generate path by greedy or beam search, cutting path when it reaches shipyard or when there is no more space left. There is also a case, when path stuck in a loop like ‘NW999..’, and it can take forever until we won’t have enough space. For such thing it is useful to make a stop by some maximum-moves threshold.Bonus: reinforcement learningIn my experiments, imitation learning was good enough for an agent to start making good moves, however, there was constant instability on inference. At some point an agent could make a ridiculously stupid move (with small probability, but nevertheless), effecting balance of the game dramatically. This move could’ve never been made by an expert agent, therefore we end up with board state which neural network didn’t see in training data. So the chance of making wrong moves increases at each step and finally imitation agent loses.To tackle this issue I’ve tried off-policy reinforcement learning. The architecture of policy neural network was the same, critic neural network had similar but smaller encoder. Reward was +1 for winning, -1 for losing. I used TD($\\lambda$) target $v_s$ for value prediction and policy loss with clipped importance sampling:\\[\\mathcal{L}_{value} = (v_s - V_\\omega(s))^2, \\quad \\mathcal{L}_{policy} = -\\log \\pi_\\theta(a|s) \\min \\big(1, \\frac{\\pi_\\theta(a|s)}{\\beta(a|s)} \\big) A_\\omega(a, s),\\]where $A_\\omega(a, s)$ is an advantage, obtained by UPGO method.RL helped to stabilize inference, but sadly it didn’t go far beyond that. New agent was able to beat rule-based agents, which were on top of Kore Beta competition ~80% of the time.Results and conclusionsUnfortunately, best of my agents were able to reach only up to 30-ish places, but still, it was a fun ride. What could’ve been done better? Here is my list: Better feature engineering. The input state which I was using contained a lot of data, however not all of the important information could have been distilled by neural network. Looking ahead for 12 steps made input graph enormously huge: $21 \\times 21 \\times 12 \\times n$, where $n$ is a feature dimension for every node. And still there were a lot of opponent flight plans with bigger time-length than 12 steps. Larger neural network. It is related to previous issue: construction of huge input state at every turn was taking a lot of time on inference. In order to fit Kaggle time requirements I had to reduce network size. I’m convinced that bigger dimensions could lead to better results. Reinforcement learning experiments. It is well known that it takes a lot of time to make RL work. Training is slow and unstable most of the time. It is extremely important to closely monitor training procedure and analyze the results.Hopefully I’ll take this experience into account in the next Kaggle simulation competition." }, { "title": "Visual Guide to Statistics. Part IV: Foundations of Testing", "url": "/posts/visual-guide-to-statistics-part-iv-foundations-of-testing/", "categories": "Statistics, Visual Guide", "tags": "statistics, hypothesis, significance level, power of a test, neyman-pearson test, ump-test, confidence interval, one-sided gauss test, one-sided t-test, two-sample t-test, likelihood-ratio test, wilks theorem, bartlett test, chi-square independence test", "date": "2022-05-01 06:00:00 +0300", "snippet": " This is the fourth and the last part of a ‘Visual Guide to Statistics’ cycle. All the previous parts and other topics related to statistics could be found here. In this post we will test hypotheses about the unknown parameter $\\vartheta$. As before, we have a statistical experiment with sample space $\\mathcal{X}$ and family of probability measures $\\mathcal{P} = \\lbrace P_\\vartheta \\mid \\vartheta \\in \\Theta \\rbrace$.Introductory exampleLet’s discuss a simplified clinical study, in which we want to decide whether a newly invented drug $B$ is better than a well-known drug $A$ or not. Suppose that you know from previous years that $A$ has a chance of healing about $p_a$. The new drug $B$ was tested on $n$ persons and $m$ became healthy. Do we choose $A$ or $B$? In terms of mathematics we test\\[H\\colon p_b \\leq p_a \\quad \\text{vs} \\quad K\\colon p_b &amp;gt; p_a,\\]where $p_b$ is the unknown chance of healing with $B$.Let $\\Theta = \\Theta_H \\cup \\Theta_K$ be a partition of $\\Theta$. $\\Theta_H$ is called (null) hypothesis, $\\Theta_K$ is called the alternative. A randomized test is a measurable map $\\varphi: \\mathcal{X} \\rightarrow [0, 1]$. Here $\\varphi(x)$ is the probability of a decision for $\\Theta_K$ when $x=X(\\omega)$ is observed. For a test $\\varphi$ we call $\\mathcal{K}= \\lbrace x \\mid \\varphi(x)=1 \\rbrace$ the critical region and $\\mathcal{R}= \\lbrace x \\mid \\varphi(x) \\in (0,1) \\rbrace$ - the region of randomization. A test $\\varphi$ is called non-randomized if $\\mathcal{R} = \\emptyset$.In our example we know that the statistic $\\overline{X}_n$ is the UMVU estimator for $p$. A reasonable decision rule is to decide for $K$ if $\\overline{X}_n$ is “large”. For example,\\[\\varphi(x) = \\left \\lbrace \\begin{array}{cl} 1, &amp;amp; \\overline{X}_n &amp;gt; c, \\\\ 0, &amp;amp; \\overline{X}_n \\leq c \\end{array} \\right.\\]with some constant $c$ is a reasonable test. But how “large” must $c$ be?When deciding for $H$ or $K$ using $\\varphi$, two errors can occur: Error of the 1st kind: decide for $K$ when $H$ is true. Error of the 2nd kind: decide for $H$ when $K$ is true.Both errors occur with certain probabilities. In our example the probability of a decision for $K$ is\\[P(\\varphi(X)=1)=P(\\overline{X}_n &amp;gt; c).\\]In practice, we can use approximation by normal distribution\\[\\begin{aligned} P(\\overline{X}_n &amp;gt; c) &amp;amp; = P\\bigg(\\frac{\\sqrt{n}(\\overline{X}_n - p_b)}{\\sqrt{p_b(1-p_b)}} &amp;gt; \\frac{\\sqrt{n}(c - p_b)}{\\sqrt{p_b(1-p_b)}}\\bigg) \\\\ \\color{Salmon}{\\text{Central Limit Theorem} \\rightarrow} &amp;amp; \\approx P\\bigg(\\mathcal{N}(0,1) &amp;gt; \\frac{\\sqrt{n}(c - p_b)}{\\sqrt{p_b(1-p_b)}}\\bigg) \\\\&amp;amp; = \\Phi\\bigg(\\frac{\\sqrt{n}(p_b - c)}{\\sqrt{p_b(1-p_b)}}\\bigg), \\end{aligned}\\]where $\\Phi$ is the distribution function of $\\mathcal{N}(0, 1)$. The probability of error of the 1st kind is bounded from above:\\[\\begin{aligned}P(\\text{reject } H \\mid H \\text{ is true}) &amp;amp;= P(\\overline{X}_n &amp;gt; c \\mid p_b \\leq p_a) \\\\ &amp;amp;\\leq P(\\overline{X}_n &amp;gt; c \\mid p_b = p_a) \\\\ &amp;amp; =\\Phi\\bigg(\\frac{\\sqrt{n}(p_a - c)}{\\sqrt{p_a(1-p_a)}}\\bigg).\\end{aligned}\\]By symmetry,\\[P(\\text{accept } H \\mid K \\text{ is true}) \\leq 1 - \\Phi\\bigg(\\frac{\\sqrt{n}(p_a - c)}{\\sqrt{p_a(1-p_a)}}\\bigg).\\]Fig. 1. Visualization of basic test experiment. Parameters $p_a$ and $c$ are draggable.Power of a testIdeally we want to minimize both errors simulaneously and pick the optimal test. The problem is that criterias $\\varphi_0(x) \\equiv 0$ and $\\varphi_1(x) \\equiv 1$ are optimal if one needs to minimize one of the errors, but they don’t minimize both errors at the same time. In practice, the upper bound $\\alpha$ is taken for the probability of error of the 1st kind and probability of error of the 2nd kind is minimized. Typically, $0.01 \\leq \\alpha \\leq 0.1$ (the set belonging to the more severe consequences is the alternative).Now suppose $\\varphi$ is a test for $H \\colon \\vartheta \\in \\Theta_H$ vs $K \\colon \\vartheta \\in \\Theta_K$. Let’s define function\\[\\beta_\\varphi(\\vartheta) = 1 - \\mathbb{E}_\\vartheta[\\varphi(X)].\\]Note that for non-randomized test $\\varphi$ we have\\[\\beta_\\varphi(\\vartheta) = P_\\vartheta(\\varphi(X) = 0),\\]which is the probability to decide for $H$. In particular, $\\vartheta \\in \\Theta_H$: $1 - \\beta_\\varphi(\\vartheta)$ is the probability of an error of the 1st kind, $\\vartheta \\in \\Theta_K$: $\\beta_\\varphi(\\vartheta)$ is the probability of an error of the 2nd kind.The function $1 - \\beta_\\varphi(\\vartheta)$ for $\\vartheta \\in \\Theta_K$, which is the probability of correctly rejecting hypothesis $H$, when alterntative $K$ is true, is called power of a test $\\varphi$. The same intuition holds for randomized tests. Test $\\varphi$ is called a test with significance level $\\alpha \\in [0, 1]$ if\\[1 - \\beta_\\varphi(\\vartheta) \\leq \\alpha \\quad \\forall \\vartheta \\in \\Theta_H.\\]A test with significance level $\\alpha$ has a probability of an error of the 1st kind, which is bounded by $\\alpha$. We will denote set of all tests with significance level $\\alpha$ as $\\Phi_\\alpha$. Test $\\varphi$ is also called unbiased with significance level $\\alpha$ if $\\varphi \\in \\Phi_\\alpha$ and\\[1-\\beta_\\varphi(\\vartheta) \\geq \\alpha \\quad \\forall \\vartheta \\in \\Theta_K.\\]For an unbiased test with significance level $\\alpha$ the probability of deciding for $K$ for every $\\vartheta \\in \\Theta_K$ is not smaller than for $\\vartheta \\in \\Theta_H$. The set of all unbiased tests with level $\\alpha$ we will call $\\Phi_{\\alpha \\alpha}$.Test $\\tilde{\\varphi} \\in \\Phi_\\alpha$ is called uniformly most powerful (UMP) test with significance level $\\alpha$ if\\[\\beta_{\\tilde{\\varphi}}(\\vartheta) = \\inf_{\\varphi \\in \\Phi_\\alpha} \\beta_\\varphi(\\vartheta) \\quad \\forall \\vartheta \\in \\Theta_K.\\]Test $\\tilde{\\varphi} \\in \\Phi_{\\alpha\\alpha}$ is called uniformly most powerful unbiased (UMPU) test with significance level $\\alpha$ if\\[\\beta_{\\tilde{\\varphi}}(\\vartheta) = \\inf_{\\varphi \\in \\Phi_{\\alpha\\alpha}} \\beta_\\varphi(\\vartheta) \\quad \\forall \\vartheta \\in \\Theta_K.\\]Neyman-Pearson lemmaLet’s start with simple hypothesis:\\[H\\colon \\vartheta \\in \\lbrace \\vartheta_0 \\rbrace \\ \\ \\text{vs} \\ \\ K\\colon \\vartheta \\in \\lbrace \\vartheta_1 \\rbrace , \\quad \\vartheta_0 \\neq \\vartheta_1.\\]Corresponding densities: $p_i = \\frac{dP_{\\vartheta_i}}{dx}$. UMP-test with level $\\alpha$ maximizes\\[1-\\beta_\\varphi(\\vartheta_1) = \\mathbb{E}_{\\vartheta_1}[\\varphi(X)] = \\int_{\\mathcal{X}} \\varphi(x)p_1(x)dx\\]under the constraint\\[1-\\beta_\\varphi(\\vartheta_0) = \\mathbb{E}_{\\vartheta_0}[\\varphi(X)] = \\int_{\\mathcal{X}} \\varphi(x)p_0(x)dx \\leq \\alpha.\\]In the situation of simple hypotheses a test $\\varphi$ is called a Neyman-Pearson test (NP test) if $c\\in[0, \\infty)$ exists such that\\[\\varphi(x): \\left \\lbrace \\begin{array}{cl} 1, &amp;amp; p_1(x) &amp;gt; cp_0(x), \\\\ 0, &amp;amp; p_1(x) &amp;lt; cp_0(x). \\end{array} \\right.\\]Let $\\tilde{\\varphi}$ be an NP-test with constant $\\tilde{c}$ and let $\\varphi$ be some other test with\\[\\beta_\\varphi(\\vartheta_0) \\geq \\beta_{\\tilde{\\varphi}}(\\vartheta_0).\\]Then we have\\[\\begin{aligned} \\beta_\\varphi(\\vartheta_1) - \\beta_{\\tilde{\\varphi}}(\\vartheta_1) &amp;amp;= (1 - \\beta_{\\tilde{\\varphi}}(\\vartheta_1) ) - (1 - \\beta_\\varphi(\\vartheta_1) ) \\\\&amp;amp;=\\int (\\tilde{\\varphi} - \\varphi) p_1 dx \\\\&amp;amp;= \\int (\\tilde{\\varphi} - \\varphi)(p_1 - \\tilde{c}p_0)dx + \\int \\tilde{c} p_0 (\\tilde{\\varphi} - \\varphi) dx.\\end{aligned}\\]For the first integral note that\\[\\begin{aligned}\\tilde{\\varphi} - \\varphi &amp;gt; 0 \\Longrightarrow \\tilde{\\varphi} &amp;gt; 0 \\Longrightarrow p_1 \\geq \\tilde{c}p_0, \\\\\\tilde{\\varphi} - \\varphi &amp;lt; 0 \\Longrightarrow \\tilde{\\varphi} &amp;lt; 1 \\Longrightarrow p_1 \\leq \\tilde{c}p_0.\\end{aligned}\\]Hence, $(\\tilde{\\varphi} - \\varphi)(p_1 - \\tilde{c}p_0) \\geq 0$ always. The second integral is\\[\\tilde{c}(\\beta_{\\tilde{\\varphi}}(\\vartheta_0) - \\beta_\\varphi(\\vartheta_0)) \\geq 0.\\]Therefore we have\\[\\beta_\\varphi(\\vartheta_1) \\geq \\beta_{\\tilde{\\varphi}}(\\vartheta_1)\\]and NP-test $\\tilde{\\varphi}$ is an UMP test with level $\\alpha = \\mathbb{E}_{\\vartheta_0}[\\tilde{\\varphi}(X)]$. This statement is called NP lemma.There are also other parts of this lemma which I will state here without proof: For any $\\alpha \\in [0, 1]$ there is an NP-test $\\varphi$ with $\\mathbb{E}_{\\vartheta_0}[\\varphi(X)] = \\alpha$. If $\\varphi’$ is UMP with level $\\alpha$, then $\\varphi’$ is (a.s.) an NP-test. Also\\[\\mathbb{E}_{\\vartheta_0}[\\varphi&#39;(X)] &amp;lt; \\alpha \\Longrightarrow \\mathbb{E}_{\\vartheta_1}[\\varphi&#39;(X)]=1.\\]An NP-test $\\tilde{\\varphi}$ for $H \\colon \\vartheta = \\vartheta_0$ vs $K \\colon \\vartheta = \\vartheta_1$ is uniquely defined outside of\\[S_= =\\lbrace x\\ |\\ p_1(x) = \\tilde{c}p_0(x) \\rbrace.\\]On $S_=$ set the test can be chosen such that $\\beta_{\\tilde{\\varphi}}(\\vartheta_0) = \\alpha$.Is must also be noted that every NP-test $\\tilde{\\varphi}$ with $\\beta_{\\tilde{\\varphi}}(\\vartheta_0) \\in (0, 1)$ is unbiased. In particular\\[\\alpha := 1 - \\beta_{\\tilde{\\varphi}}(\\vartheta_0) &amp;lt; 1 - \\beta_{\\tilde{\\varphi}}(\\vartheta_1).\\]ProofTake test $\\varphi \\equiv \\alpha$. It has significance level $\\alpha$ and since $\\tilde{\\varphi}$ is UMP, we have $$1-\\beta_\\varphi(\\vartheta_1) \\leq 1-\\beta_{\\tilde{\\varphi}}(\\vartheta_1).$$If $\\alpha = 1-\\beta_{\\tilde{\\varphi}}(\\vartheta_1) &amp;lt; 1$, then $\\varphi \\equiv \\alpha$ is UMP. Since every UMP test is an NP test, we know that $p_1(x) = \\tilde{c}p_0(x)$ for almost all $x$. Therefore, $\\tilde{c}=1$ and $p_1 = p_0$ a.s. and also $P_{\\vartheta_0} = P_{\\vartheta_1}$, which is contradictory.Confidence intervalLet $X_1, \\dots X_n$ i.i.d. $\\sim \\mathcal{N}(\\mu,\\sigma^2)$ with $\\sigma^2$ known. We test\\[H \\colon \\mu = \\mu_0 \\quad \\text{vs} \\quad K \\colon \\mu = \\mu_1\\]with $\\mu_0 &amp;lt; \\mu_1$. For the density of $X_1, \\dots X_n$ it holds\\[p_j(x) = (2 \\pi \\sigma^2)^{-n/2} \\exp \\Big( -\\frac{1}{2\\sigma^2} \\Big( \\sum_{i=1}^{n} X_i^2 - 2 \\mu_j \\sum_{i=1}^{n}X_i + n\\mu_j^2 \\Big)\\Big), \\quad j = 0, 1.\\]As the inequality for the likelihood ratio which we need for the construction of the NP test, we get\\[\\frac{p_1(x)}{p_0(x)} = \\exp \\Big( \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} x_i(\\mu_1 - \\mu_0) \\Big) \\cdot f(\\sigma^2, \\mu_1, \\mu_0) &amp;gt; \\tilde{c},\\]where the known constant $f(\\sigma^2, \\mu_1, \\mu_0)$ is positive. This inequality is equivalent to\\[\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^{n}X_i &amp;gt; c,\\]for some appropriate $c$ (because of $\\mu_1 &amp;gt; \\mu_0$). Therefore it is equally well possible to determine $c$ such that\\[P_{\\mu_0}(\\overline{X}_n &amp;gt; c) = \\alpha\\]or equivalently\\[\\begin{aligned} P_{\\mu_0}\\Big( &amp;amp;\\underbrace{\\frac{\\sqrt{n}(\\overline{X}_n - \\mu_0)}{\\sigma}} &amp;gt; \\frac{\\sqrt{n}(c-\\mu_0)}{\\sigma}\\Big) = 1 - \\Phi\\Big(\\frac{\\sqrt{n}(c - \\mu_0)}{\\sigma}\\Big) = \\alpha. \\\\ &amp;amp;\\quad \\color{Salmon}{\\sim \\mathcal{N}(0, 1)} \\end{aligned}\\]If we call $u_p$ the p-quantile of $\\mathcal{N}(0, 1)$, which is the value such that $\\Phi(u_p)=p$, then we get\\[\\frac{\\sqrt{n}(c - \\mu_0)}{\\sigma} = u_{1-\\alpha} \\quad \\Longleftrightarrow \\quad c = \\mu_0 + u_{1-\\alpha}\\frac{\\sigma}{\\sqrt{n}}.\\]The NP-test becomes\\[\\tilde{\\varphi}(X) = 1_{\\lbrace\\overline{X}_n &amp;gt; \\mu_0 + u_{1-\\alpha} \\frac{\\sigma}{\\sqrt{n}} \\rbrace }.\\]Sample H Sample K n: ResetFig. 2. Visualization of simple hypothesis testing with $\\mu_0 = -1$ and $\\mu_1=1$. Significance level $\\alpha$ on the right-hand side is draggable.Simple hypotheses like that are not relevant in practice, but: They explain intuitively how to construct a test. One needs a so called confidence interval $c(X) \\subset \\Theta$ in which the unknown parameter lies with probability $1-\\alpha$. In example above we used that for\\[c(X) = [\\overline{X}_n - u_{1-\\alpha} \\frac{\\sigma}{\\sqrt{n}}, \\infty)\\]we have\\[P_{\\mu_0}(\\mu_0 \\in c(X)) = P_{\\mu_0}(\\overline{X}_n \\leq \\mu_0 + \\frac{\\sigma}{\\sqrt{n}} u_{1-\\alpha}) = 1-\\alpha.\\]Any such $c(X)$ can be used to construct a test, for example,\\[c&#39;(X) =\\Big[\\overline{X}_n -u_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}, \\overline{X}_n + u_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\Big].\\]In addition, simple hypotheses tell you on which side the alternative lies. Formal results like the NP lemma are useful to derive more relevant results.Monotone likelihood ratioLet $\\Theta = \\mathbb{R}$, $\\mathcal{P} = \\lbrace P_\\vartheta \\mid \\vartheta \\in \\Theta \\rbrace$ and $T\\colon \\mathcal{X} \\rightarrow \\mathbb{R}$ be some statistic. Family $\\mathcal{P}$ is called class with monotone (isotonic) likelihood ratio if for every $\\vartheta &amp;lt; \\vartheta_1$ there exists monotonically increasing function $H_{\\vartheta_0, \\vartheta_1} \\colon \\mathbb{R} \\rightarrow [0, \\infty)$, such that\\[\\frac{p_{\\vartheta_1}(x)}{p_{\\vartheta_0}(x)} =H_{\\vartheta_0, \\vartheta_1}(T(x)) \\quad P_{\\vartheta_0} + P_{\\vartheta_1}\\text{-a.s.}\\]In our example above we had\\[\\frac{p_{\\mu_1}(x)}{p_{\\mu_0}(x)} = \\exp \\Big( \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} x_i(\\mu_1 - \\mu_0) \\Big) \\cdot f(\\sigma^2, \\mu_1, \\mu_0),\\]which is monotonically increasing in $\\overline{x}_n$. This can be generalized to one-parametric exponential families.Let $\\mathcal{P} = \\lbrace P_\\vartheta \\mid \\vartheta \\in \\Theta \\rbrace$ be class with monotone likelihood ratio in $T$, $\\vartheta \\in \\Theta$, $\\alpha \\in (0, 1)$ and we test the one-sided hypothesis\\[H\\colon\\vartheta \\leq \\vartheta_0 \\quad \\text{vs} \\quad K\\colon\\vartheta &amp;gt; \\vartheta_0.\\]Let also\\[\\tilde{\\varphi}(x) = 1_{\\lbrace T(x) &amp;gt; c\\rbrace} + \\gamma 1_{\\lbrace T(x) = c\\rbrace},\\]where $c = \\inf \\lbrace t \\mid P_{\\vartheta_0}(T(X) &amp;gt; t) \\leq \\alpha \\rbrace$ and\\[\\gamma = \\left \\lbrace \\begin{array}{cl} \\frac{\\alpha - P_{\\vartheta_0}(T(X) &amp;gt; c) }{ P_{\\vartheta_0}(T(X) = c) }, &amp;amp; \\text{if } P_{\\vartheta_0}(T(X) = c) \\neq 0 \\\\ 0, &amp;amp; \\text{otherwise}. \\end{array} \\right.\\]Then $1-\\beta_{\\tilde{\\varphi}}(\\vartheta_0) = \\alpha$ and $\\tilde{\\varphi}$ is UMP test with significance level $\\alpha$.ProofWe have$$1-\\beta_{\\tilde{\\varphi}}(\\vartheta_0)=P_{\\vartheta_0}(T(X)&amp;gt;c) + \\gamma P_{\\vartheta_0}(T(X) = c) = \\alpha. $$Let $\\vartheta_0 &amp;lt; \\vartheta_1$, then due to monotonicity$$H_{\\vartheta_0, \\vartheta_1}(T(x)) &amp;gt; H_{\\vartheta_0, \\vartheta_1}(c) = s \\quad \\Longrightarrow \\quad T(x) &amp;gt; c $$and$$\\tilde{\\varphi}(x) = \\left \\{ \\begin{array}{cl} 1, &amp;amp; H_{\\vartheta_0, \\vartheta_1}(x) &amp;gt; s, \\\\ 0, &amp;amp; H_{\\vartheta_0, \\vartheta_1}(x) &amp;lt; s. \\end{array} \\right.$$Therefore $\\tilde{\\varphi}$ is NP-test with significance level $\\alpha$ and by NP lemma$$ \\beta_{\\tilde{\\varphi}}(\\vartheta_1) = \\inf \\lbrace \\beta_\\varphi(\\vartheta_1)\\ |\\ \\beta_\\varphi(\\vartheta_0) = 1-\\alpha \\rbrace. $$ As $\\tilde{\\varphi}$ doesn&#39;t depend on $\\vartheta_1$, this relation holds for all $\\vartheta_1 &amp;gt; \\vartheta_0$. Finally, let $\\varphi&#39;(x) = 1 - \\tilde{\\varphi}(x)$. Using the similar reasoning as above one can show that$$\\beta_{\\varphi&#39;}(\\vartheta_2) = \\inf \\lbrace \\beta_\\varphi(\\vartheta_2)\\ |\\ \\beta_\\varphi(\\vartheta_0) = 1 - \\alpha \\rbrace \\quad \\forall \\vartheta_2 &amp;lt; \\vartheta_0. $$For trivial test $\\overline{\\varphi} \\equiv \\alpha$ the following equality takes place: $\\beta_{\\overline{\\varphi}}(\\vartheta_0) = 1-\\alpha$. Hence we conclude that$$1-\\beta_{\\tilde{\\varphi}}(\\vartheta_2) = \\beta_{\\varphi&#39;}(\\vartheta_2) \\geq \\beta_{1-\\overline{\\varphi}}(\\vartheta_2) = 1-\\beta_{\\overline{\\varphi}}(\\vartheta_2) = \\alpha. $$ Hence, $1-\\beta_{\\tilde{\\varphi}}(\\vartheta_2) \\geq \\alpha$, $\\tilde{\\varphi} \\in \\Phi_\\alpha$ and $\\tilde{\\varphi}$ is UMP test. Also for any $\\vartheta &amp;lt; \\vartheta_0$ we have $$\\beta_{\\tilde{\\varphi}}(\\vartheta) = \\sup \\lbrace \\beta_\\varphi(\\vartheta)\\ |\\ 1 - \\beta_\\varphi(\\vartheta_0) = \\alpha \\rbrace,$$because of $\\beta_{\\varphi&#39;} = 1 - \\beta_{\\tilde{\\varphi}}$.Back to our previous example with $X_1, \\dots, X_n$ with known $\\sigma^2$, we know that\\[p_\\mu(x) = (2 \\pi \\sigma^2)^{-\\frac{n}{2}} \\exp \\Big( -\\frac{1}{2\\sigma^2}\\sum_{i=1}^{n}(x_i - \\mu)^2 \\Big)\\]has a monotone likelihood ratio in $T(X) = \\overline{X}_n$. An UMP test with level $\\alpha$ is given by\\[\\tilde{\\varphi}(x) = 1_{\\lbrace\\overline{x}_n &amp;gt; c\\rbrace } + \\gamma 1_{\\lbrace\\overline{x}_n = c\\rbrace}.\\]Since $P_{\\mu_0}(T(X) = c) = 0$, then $\\gamma = 0$ and we choose $c$ such that\\[P_{\\mu_0}(\\overline{X}_n &amp;gt; c) = \\alpha \\Longleftrightarrow c = \\mu_0 + \\frac{\\sigma}{\\sqrt{n}} u_{1-\\alpha}.\\]This UMP test\\[\\tilde{\\varphi}(x) = 1_{\\lbrace \\overline{X}_n &amp;gt; \\mu_0 + \\frac{\\sigma}{\\sqrt{n}}u_{1-\\alpha} \\rbrace }\\]is called the one-sided Gauss test.There is a heuristic how to get to the one-sided Gauss test: since $\\overline{X}_n$ is UMVU for $\\mu$, a reasonable strategy is to decide for $K$ if $\\overline{X}_n$ is “large enough”, so the test shoud be of the form\\[\\varphi(x) = 1_{\\lbrace \\overline{X}_n &amp;gt; c \\rbrace }.\\]Choosing $c$ happens by controlling the error of the 1st kind. For all $\\mu \\leq \\mu_0$ we have\\[\\begin{aligned}\\beta_\\varphi(\\mu) &amp;amp;= P_\\mu(\\overline{X}_n &amp;gt; c) \\\\ &amp;amp;= P_\\mu \\Big( \\frac{\\sqrt{n}(\\overline{X}_n - \\mu) }{\\sigma} &amp;gt; \\frac{\\sqrt{n}(c-\\mu)}{\\sigma}\\Big) \\\\ &amp;amp;= 1 - \\Phi\\Big(\\frac{\\sqrt{n}(c-\\mu)}{\\sigma}\\Big) \\\\&amp;amp;\\leq 1 - \\Phi\\Big(\\frac{\\sqrt{n}(c-\\mu_0)}{\\sigma}\\Big).\\end{aligned}\\]So we need to secure that\\[1- \\Phi\\Big(\\frac{\\sqrt{n}(c-\\mu_0)}{\\sigma}\\Big) \\leq \\alpha \\Longleftrightarrow c \\geq \\mu_0 + \\frac{\\sigma}{\\sqrt{n}} u_{1-\\alpha}.\\]We take $c = \\mu_0 + \\frac{\\sigma}{\\sqrt{n}} u_{1-\\alpha}$ for an error of the 1st kind to be $\\alpha$.This method doesn’t tell you anything about optimality, but at least provides a test. Most importantly, it can be applied in more general situations like unknown $\\sigma^2$. In this case one can use\\[\\hat{\\sigma}_n^2 = \\frac{1}{n-1}\\sum_{i=1}^{n}(X_i - \\overline{X}_n)^2.\\]As above we obtain\\[\\beta_\\varphi(\\mu) = P_\\mu\\Big( \\frac{\\sqrt{n}(\\overline{X}_n - \\mu) }{\\hat{\\sigma}_n} &amp;gt; \\frac{\\sqrt{n}(c-\\mu)}{\\hat{\\sigma}_n}\\Big) = 1 - F_{t_{n-1}}\\bigg( \\frac{c - \\mu}{\\sqrt{\\hat{\\sigma}_n^2 / n}} \\bigg),\\]where $F_{t_{n-1}}$ denotes the distribution function of $t_{n-1}$. A reasonable choice is\\[c = \\mu_0 + \\frac{\\hat{\\sigma}_n}{\\sqrt{n}}t_{n-1,1-\\alpha},\\]with the corresponding quantile of a $t_{n-1}$ distribution. The test\\[\\phi(x) = 1_{\\lbrace \\overline{x}_n &amp;gt; \\mu_0 + \\frac{\\hat{\\sigma}_n}{\\sqrt{n}}t_{n-1,1-\\alpha} \\rbrace}\\]is called the one-sided t-test.Two-sided testsThere are in general no UMP tests for\\[H\\colon\\vartheta = \\vartheta_0 \\quad \\text{vs} \\quad K\\colon\\vartheta \\neq \\vartheta_0,\\]because these have to be optimal for all\\[H&#39;\\colon\\vartheta = \\vartheta_0 \\quad \\text{vs} \\quad K&#39;\\colon\\vartheta = \\vartheta_1\\]with $\\vartheta_0 \\neq \\vartheta_1$. In case of monotone likelihood-ratio, the optimal test in this case is\\[\\varphi(x) = 1_{\\lbrace T(x) &amp;gt; c \\rbrace} + \\gamma(x) 1_{\\lbrace T(x) = c\\rbrace}\\]for $\\vartheta_1 &amp;gt; \\vartheta_0$ and\\[\\varphi&#39;(x) = 1_{\\lbrace T(x) &amp;lt; c&#39;\\rbrace } + \\gamma&#39;(x) 1_{\\lbrace T(x) = c&#39;\\rbrace}\\]for $\\vartheta_1 &amp;lt; \\vartheta_0$. This is not possible.There is a theorem for one-parametric exponential family with density\\[p_\\vartheta(x) = c(\\vartheta)h(x)\\exp(Q(\\vartheta) T(x))\\]with increasing $Q$: UMPU test for\\[H \\colon \\vartheta \\in [\\vartheta_1, \\vartheta_2] \\quad \\text{vs} \\quad K\\colon\\vartheta \\notin [\\vartheta_1, \\vartheta_2]\\]is\\[\\varphi(x) = \\left \\lbrace \\begin{array}{cl} 1, &amp;amp; \\text{if } T(x) \\notin [c_1, c_2], \\\\ \\gamma_i, &amp;amp; \\text{if } T(x) = c_i, \\\\ 0, &amp;amp; \\text{if } T(x) \\in (c_1, c_2), \\end{array} \\right.\\]where the constants $c_i, \\gamma_i$ determined from\\[\\beta_\\varphi(\\vartheta_1) = \\beta_\\varphi(\\vartheta_2) = 1-\\alpha.\\]Similar results hold for $k$-parametric exponential families.Take an example: let $X$ be exponentially distributed random variable: $X \\sim \\operatorname{Exp}(\\vartheta)$ with density\\[f_\\vartheta(x) = \\vartheta e^{-\\vartheta x} 1_{[0, \\infty)}(x)\\]and we test\\[H\\colon \\vartheta \\in [1, 2] \\quad \\text{vs} \\quad K\\colon\\vartheta \\notin [1, 2].\\]We have $T(x) = x$ and\\[\\varphi(x) = 1_{\\lbrace X \\notin [c_1, c_2] \\rbrace}.\\]It is known that for $X$ distribution function is $F(x) = 1 - e^{-\\vartheta x}$, therefore\\[\\begin{aligned}P_{1}(X \\in [c_1, c_2])&amp;amp;=e^{-c_1}-e^{-c_2} = 1-\\alpha, \\\\P_{2}(X \\in [c_1, c_2])&amp;amp;=e^{-2 c_1}-e^{-2 c_2} = 1-\\alpha.\\end{aligned}\\]Solving this for $c_1$ and $c_2$ we get\\[c_1 = \\ln\\frac{2}{2-\\alpha}, \\quad c_2 = \\ln\\frac{2}{\\alpha}.\\]Asymptotic properties of testsLet $X_1, \\dots , X_m$ i.i.d. $\\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ and $Y_1, \\dots , Y_n$ i.i.d. $\\sim \\mathcal{N}(\\mu_2, \\tau^2)$ are two independent samples. We want to test the hypothesis:\\[H\\colon \\mu_1 \\leq \\mu_2 \\quad \\text{vs} \\quad K\\colon \\mu_1 &amp;gt; \\mu_2.\\]We reject $H$ if $\\overline{Y}_n$ is much smaller than $\\overline{X}_m$. Let $\\sigma^2=\\tau^2$, but variance is unknown. We know from Part I that\\[\\overline{X}_m - \\overline{Y}_n=\\mathcal{N}\\bigg(\\mu_1-\\mu_2, \\sigma^2\\bigg( \\frac{1}{m}+\\frac{1}{n} \\bigg)\\bigg)\\]and pooled variance:\\[\\hat{\\sigma}_{m,n}^2=\\frac{1}{m+n-2}\\Big( \\sum_{i=1}^{m}(X_i-\\overline{X}_m)^2+\\sum_{i=1}^{n}(Y_i-\\overline{Y}_n)^2 \\Big) \\sim \\frac{\\sigma^2}{m+n-2} \\chi_{m+n-2}^2.\\]For $\\mu_1=\\mu_2$ we have\\[T_{m,n}=\\sqrt{\\frac{mn}{m+n}}\\frac{\\overline{X}_m-\\overline{Y}_n}{\\hat{\\sigma}_{m,n}} \\sim t_{m+n-2},\\]therefore test\\[\\varphi_{m,n}(x)=1_{\\lbrace T_{m,n} &amp;gt; t_{m+n-2, 1-\\alpha}\\rbrace }\\]is UMPU with significance level $\\alpha$. This test is called two-sample t-test. Let $\\sigma^2 \\neq \\tau^2$, then\\[\\overline{X}_m - \\overline{Y}_n=\\mathcal{N}\\bigg(\\mu_1-\\mu_2, \\frac{\\sigma^2}{m}+\\frac{\\tau^2}{n} \\bigg).\\]Unbiased estimators for variances are\\[\\hat{s}_{m}^2(X)=\\frac{1}{m-1}\\sum_{i=1}^{m}(X_i-\\overline{X}_m)^2, \\quad \\hat{s}_{n}^2(Y)=\\frac{1}{n-1}\\sum_{i=1}^{n}(Y_i-\\overline{Y}_n)^2.\\]Let also\\[\\hat{s}_{m, n}^2 = \\frac{1}{m}\\hat{s}_{m}^2(X) + \\frac{1}{n}\\hat{s}_{n}^2(Y).\\]The distribution of random variable\\[T_{m,n}^*=\\frac{\\overline{X}_m-\\overline{Y}_n}{\\hat{s}_{m, n}}\\]was unknown until recently (so called Behrens-Fisher problem) and can’t be expressed in terms of elementary functions, but from Central Limit Theorem we know that\\[\\frac{\\overline{X}_m-\\overline{Y}_n - (\\mu_1-\\mu_2)}{\\hat{s}_{m,n}} \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0,1),\\]if $m \\rightarrow \\infty$, $n \\rightarrow \\infty$ and $\\frac{m}{n}\\rightarrow \\lambda \\in (0, \\infty)$. Let\\[\\varphi_{m,n}^*(x)=1_{\\lbrace T_{m,n}^* &amp;gt; u_{1-\\alpha}\\rbrace },\\]then\\[\\begin{aligned} \\beta_{\\varphi_{m,n}^*}(\\mu_1, \\mu_2) &amp;amp; =P_{\\mu_1, \\mu_2}(T_{m,n}^* \\leq u_{1-\\alpha})\\\\&amp;amp;=P_{\\mu_1, \\mu_2}\\Big(\\frac{\\overline{X}_m-\\overline{Y}_n - (\\mu_1-\\mu_2)}{\\hat{s}_{m,n}} \\leq \\frac{- (\\mu_1-\\mu_2)}{\\hat{s}_{m,n}}+ u_{1-\\alpha}\\Big) \\\\ &amp;amp; \\xrightarrow[m \\rightarrow \\infty,\\ n \\rightarrow \\infty,\\ \\frac{m}{n}\\rightarrow \\lambda]{} \\left \\lbrace \\begin{array}{cl} 0, &amp;amp; \\mu_1 &amp;gt; \\mu_2, \\\\ 1-\\alpha, &amp;amp; \\mu_1=\\mu_2, \\\\ 1, &amp;amp; \\mu_1&amp;lt;\\mu_2. \\end{array} \\right. \\end{aligned}\\] We say that sequence $(\\varphi_n)$ has asymptotic significance level $\\alpha$, if\\[\\lim_{n \\rightarrow \\infty} \\inf_{\\vartheta \\in \\Theta_H} \\beta_{\\varphi_n}(\\vartheta) \\geq 1-\\alpha.\\] We say that sequence $(\\varphi_n)$ is consistent, if\\[\\lim_{n \\rightarrow \\infty} \\beta_{\\varphi_n}(\\vartheta) = 0 \\quad \\forall \\vartheta \\in \\Theta_K.\\]In our example $\\varphi_{m,n}^*(x)$ is consistent and has asymptotic significance level $\\alpha$.Likelihood ratioThe common principle of building tests for\\[H\\colon\\vartheta \\in \\Theta_H \\quad \\text{vs} \\quad K\\colon\\vartheta \\in \\Theta_K\\]is likelihood ratio method. Let $f_n(x^{(n)},\\vartheta)$ be density of $P_\\vartheta^n$. Then\\[\\lambda(x^{(n)})=\\frac{\\sup_{\\vartheta \\in \\Theta_H}f_n(x^{(n)},\\vartheta)}{\\sup_{\\vartheta \\in \\Theta}f_n(x^{(n)},\\vartheta)}\\]is likelihood ratio and\\[\\varphi_n(x^{(n)})=1_{\\lbrace \\lambda(x^{(n)})&amp;lt;c \\rbrace }\\]is likelihood ratio test. It is common to choose $c$, such that\\[\\sup_{\\vartheta \\in \\Theta_H} P_\\vartheta(\\lambda(X^{(n)})&amp;lt;c) \\leq \\alpha.\\]Distribution $\\lambda(X^{(n)})$ nevertheless can be estimated only asymptotically.Before we continue further, we will formulate some conditions. Let $\\Theta \\subset \\mathbb{R}^d$ and there exist $\\Delta \\subset \\mathbb{R}^c$ and twice continuously differentiable function $h:\\Delta \\rightarrow \\Theta$, such that $\\Theta_H = h(\\Delta)$ and Jacobian of $h$ is matrix of full rank.For example, let $X_1, \\dots, X_n$ i.i.d. $\\sim \\mathcal{N}(\\mu_1, \\sigma^2)$ and $Y_1, \\dots, Y_n$ i.i.d. $\\sim \\mathcal{N}(\\mu_2, \\sigma^2)$ be two independent samples. Suppose we want to test the equivalency of means:\\[H\\colon \\mu_1 = \\mu_2 \\quad \\text{vs} \\quad K\\colon \\mu_1 \\neq \\mu_2.\\]Then $\\Theta \\in \\mathbb{R}^2 \\times \\mathbb{R}^+$, $\\Delta = \\mathbb{R} \\times \\mathbb{R}^+$ and $h(\\mu, \\sigma^2) = (\\mu, \\mu, \\sigma^2)$. Jacobian matrix is\\[J = \\begin{pmatrix} 1 &amp;amp; 0 \\\\ 1 &amp;amp; 0 \\\\ 0 &amp;amp; 1 \\end{pmatrix},\\]matrix of full rank.Let\\[\\hat{\\eta}_n=\\arg\\max_{\\eta \\in \\Delta}f_n(X^{(n)},h(\\eta)) \\quad \\text{and} \\quad \\hat{\\theta}_n=\\arg\\max_{\\vartheta \\in \\Theta}f_n(X^{(n)},\\vartheta)\\]be maximum-likelihood estimators for families\\[\\mathcal{P}_h = \\lbrace P_{h(\\eta)}\\ |\\ \\eta \\in \\Delta\\rbrace \\quad \\text{and} \\quad \\mathcal{P}_\\vartheta = \\lbrace P_\\vartheta\\ |\\ \\vartheta \\in \\Theta \\rbrace\\]respectively. Also let conditions from theorem of asymptotic efficiency for maximum-likelihood estimators for both families be satisfied. Then\\[T_n=-2\\log \\lambda(X^{(n)})=2(\\log f_n(X^{(n)}, \\hat{\\theta}_n)-\\log f_n(X^{(n)}, h(\\hat{\\eta}_n))) \\xrightarrow[]{\\mathcal{L}} \\chi_{d-c}^2,\\]if $\\vartheta \\in \\Theta_H$.ProofAs before we use notation$$\\ell(x, \\vartheta) = \\log f(x, \\vartheta).$$We start with$$\\begin{aligned} T_n^{(1)} &amp;amp; = 2(\\log f_n(X^{(n)}, \\hat{\\theta}_n)-\\log f_n(X^{(n)}, \\vartheta)) \\\\ &amp;amp; = 2\\sum_{i=1}^{n}\\Big(\\ell(X_i, \\hat{\\theta}_n) - \\ell(X_i, \\vartheta)\\Big) \\\\ &amp;amp; = 2(\\hat{\\theta}_n - \\vartheta)^T \\sum_{i=1}^{n} \\dot{\\ell}(X_i, \\vartheta) +(\\hat{\\theta}_n - \\vartheta)^T \\sum_{i=1}^{n} \\ddot{\\ell}(X_i, \\widetilde{\\vartheta}_n)(\\hat{\\theta}_n - \\vartheta) \\\\ &amp;amp; = 2 (\\hat{\\theta}_n - \\vartheta)^T \\Big( \\sum_{i=1}^{n} \\dot{\\ell}(X_i, \\vartheta) + \\sum_{i=1}^{n} \\ddot{\\ell}(X_i, \\widetilde{\\vartheta}_n)(\\hat{\\theta}_n - \\vartheta) \\Big) - (\\hat{\\theta}_n - \\vartheta)^T\\sum_{i=1}^{n}\\ddot{\\ell}(X_i, \\widetilde{\\vartheta}_n)(\\hat{\\theta}_n - \\vartheta) \\end{aligned}$$ for some $\\widetilde{\\theta}_n \\in [\\hat{\\theta}_n, \\vartheta]$. Using the notations from [Part III](https://astralord.github.io/posts/visual-guide-to-statistics-part-iii-asymptotic-properties-of-estimators/#asymptotic-efficiency-of-maximum-likelihood-estimators) we rewrite the first term of equation above:$$\\begin{aligned} 2n(\\hat{\\theta}_n - \\vartheta)^T&amp;amp; \\underbrace{(\\dot{L}_n(\\vartheta) - \\ddot{L}_n(\\tilde{\\vartheta})(\\hat{\\theta}_n - \\vartheta))}. \\\\ &amp;amp; \\qquad \\qquad\\ \\color{\\Salmon}{ = 0 \\text{ (by Mean Theorem)}} \\end{aligned}$$ Also$$T_n^{(1)} = -\\sqrt{n}(\\hat{\\theta}_n - \\vartheta)^T \\ddot{L}_n(\\widetilde{\\vartheta}_n) \\sqrt{n}(\\hat{\\theta}_n - \\vartheta),$$where$$\\begin{aligned} \\sqrt{n}(\\hat{\\theta}_n - \\vartheta)^T &amp;amp; \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, I^{-1}(f(\\cdot, \\vartheta))), \\\\ \\ddot{L}_n(\\widetilde{\\vartheta}_n)&amp;amp; \\xrightarrow[]{\\mathbb{P}} -I(f(\\cdot, \\vartheta)), \\\\ \\sqrt{n}(\\hat{\\theta}_n - \\vartheta) &amp;amp;\\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, I^{-1}(f(\\cdot, \\vartheta))). \\end{aligned}$$ We know that for $X \\sim \\mathcal{N}_d(0, \\Sigma)$ with $\\Sigma &amp;gt; 0$ we have$$X^T \\Sigma X ~ \\sim \\mathcal{X}_d^2.$$Therefore,$$T_n^{(1)} \\xrightarrow[]{\\mathcal{L}} A \\sim \\mathcal{X}_d^2.$$In the same way,$$ T_n^{(2)} = 2 (\\log f_n(X^{(n)}, h(\\hat{\\eta}_n) ) - \\log f_n(X^{(n)},h(\\eta))) \\xrightarrow[]{\\mathcal{L}} B \\sim \\mathcal{X}_c^2. $$ If $H$ is true, then $\\vartheta = h(\\eta)$ and$$T_n = T_n^{(1)} - T_n^{(2)} \\xrightarrow[]{\\mathcal{L}} A-B \\sim \\mathcal{X}_{d-c}^2,$$which follows from independence of $A-B$ and $B$. This statement is called Wilk’s theorem and it shows that\\[\\varphi_n (X^{(n)}) = \\left \\{ \\begin{array}{cl} 1, &amp;amp; -2\\log\\lambda(X^{(n)}) &amp;gt; \\mathcal{X}_{d-c, 1-\\alpha}^2, \\\\ 0, &amp;amp; \\text{otherwise} \\end{array} \\right.\\]is a test with asymptotic level $\\alpha$. Also, sequence $(\\varphi_n)$ is consistent, because\\[\\begin{aligned} -\\frac{2}{n} \\log (\\lambda(X^{(n)})) &amp;amp; = \\frac{2}{n} \\sum_{i=1}^{n} \\Big( \\ell(X_i, \\hat{\\theta}_n) - \\ell(X_i, h(\\hat{\\eta}_n)) \\Big) \\\\ &amp;amp; \\xrightarrow{\\mathcal{L}} 2 \\mathbb{E}_\\vartheta[\\ell(X,\\vartheta) - \\ell(X, h(\\eta))] \\\\ &amp;amp; = 2 KL(\\vartheta | h(\\eta)) &amp;gt; 0, \\end{aligned}\\]if $\\vartheta \\neq h(\\eta)$. Hence for $\\vartheta \\in \\Theta_K$\\[-2\\log(\\lambda(X^{(n)}))\\xrightarrow{\\mathcal{L}} \\infty.\\]Likelihood-ratio testsTake an example: let $X_{ij} \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)$, $i = 1, \\dots, r$ and $j = 1, \\dots, n_i$, where $n_i \\rightarrow \\infty$ with the same speed. We test equivalence of variances:\\[H\\colon \\sigma_1^2 = \\dots = \\sigma_r^2 \\quad \\text{vs} \\quad K \\colon \\sigma_i^2 \\neq \\sigma_j^2 \\text{ for some } i \\neq j.\\]Here $\\Theta = \\mathbb{R}^r \\times (\\mathbb{R}^+)^r$, $\\Delta = \\mathbb{R}^r \\times \\mathbb{R}^+$ and\\[h((x_1, \\dots, x_r, y)^T) = (x_1, \\dots, x_r, y, \\dots, y)^T.\\]Maximum-likelihood estimator is\\[\\hat{\\theta}_n = (\\overline{X}_{1 \\cdot}, \\dots, \\overline{X}_{r \\cdot}, \\hat{s}_1^2, \\dots, \\hat{s}_r^2)\\]with\\[\\overline{X}_{i \\cdot} = \\frac{1}{n_i} \\sum_{j=1}^{n_i}X_{ij} \\quad \\text{and} \\quad\\hat{s}_i^2 = \\frac{1}{n_i}\\sum_{j=1}^{n_i}(X_{ij} -\\overline{X}_{i \\cdot})^2.\\]Then\\[f_n(X^{(n)}, \\hat{\\vartheta}_n) = \\prod_{i=1}^{r} (2 \\pi e \\hat{s}_i^2)^{-\\frac{n_i}{2}}.\\]Under null hypothesis maximum-likelihood estimator maximizes\\[f_n(X^{(n)}, \\hat{\\eta}_n) = \\prod_{i=1}^{r} (2 \\pi \\sigma^2)^{-\\frac{n_i}{2}} \\exp \\Big( -\\frac{1}{2\\sigma^2} \\sum_{j=1}^{n_i} (X_{ij} - \\overline{X}_{i \\cdot})^2 \\Big ).\\]Setting $n = \\sum_{i=1}^{r}n_i$, we get\\[\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{r} \\sum_{j=1}^{n_i} (X_{ij}-X_{i \\cdot})^2 = \\sum_{i=1}^r \\frac{n_i}{n}\\hat{s}_i^2.\\]Then\\[f_n(X^{(n)}, \\hat{\\eta}_n) = \\prod_{i=1}^{r}(2\\pi e\\hat{\\sigma}^2)^{-\\frac{n_i}{2}} = (2\\pi e \\hat{\\sigma}^2)^{-\\frac{n}{2}}\\]and test statistic becomes\\[T_n = -2\\log \\lambda(X^{(n)}) = n \\log \\hat{\\sigma}^2 - \\sum_{i=1}^{r} n_i \\log \\hat{s}_i^2.\\]The test\\[\\varphi_n(X^{(n)}) = \\left \\{ \\begin{array}{cl} 1, &amp;amp; T_n &amp;gt; \\mathcal{X}_{r-1, 1-\\alpha}^2, \\\\ 0, &amp;amp; \\text{otherwise}. \\end{array} \\right.\\]is called the Bartlett test.Sample Add Delete ResetFig. 3. Visualization of Bartlett test. Up to five normally distributed samples can be added, each with $n_i=30$. Significance level $\\alpha$ is fixed at $0.05$. Choose different variations of $\\sigma_i^2$ to observe how it affects test statistic $T_n$.Take another example: suppose we have two discrete variables $A$ and $B$ (e.g. such as gender, age, education or income), where $A$ can take $r$ values and $B$ can take $s$ values. Further suppose that $n$ individuals are randomly sampled. A contingency table can be created to display the joint sample distribution of $A$ and $B$.   $1$ $\\cdots$ $s$ Sum $1$ $X_{11}$ $\\cdots$ $X_{1s}$ $X_{1\\cdot} = \\sum_{j=1}^n X_{1j}$ $\\vdots$ $\\vdots$ $\\vdots$ $\\vdots$ $\\vdots$ $r$ $X_{r1}$ $\\cdots$ $X_{rs}$ $X_{r\\cdot}$ Sum $X_{\\cdot1}$ $\\cdots$ $X_{\\cdot s}$ $n$ We model vector $X$ with multinomial distribution:\\[(X_1, \\dots X_n)^T \\sim \\mathcal{M}(n, p_{11}, \\dots, p_{rs}),\\]where $\\sum_{ij} p_{ij} = 1$. Joint density is\\[f_n(x^{(n)}, p) = P_p(X_{ij}=x_{ij}) = \\frac{n!}{\\prod_{i,j=1}^{r,s} x_{ij}!} \\prod_{i,j=1}^{r,s} (p_{ij})^{x_{ij}},\\]where $x_{ij} = \\lbrace 0, \\dots, n\\rbrace$ and $\\sum_{i,j=1}^{r,s} x_{ij} = n$. Maximum-likelihood estimator is\\[\\hat{p}_{ij} = \\frac{X_{ij}}{n}\\](in analogy to binomial distribution) and\\[f_n(X^{(n)}, \\hat{p}) = \\frac{n!}{\\prod_{i,j=1}^{r,s} X_{ij}!} \\prod_{i,j=1}^{r,s} \\Big(\\frac{X_{ij}}{n}\\Big)^{X_{ij}}\\]Suppose we want to test independence between $A$ and $B$:\\[H\\colon p_{ij} = p_i q_j \\ \\forall i,j \\quad \\text{vs} \\quad K\\colon p_{ij} \\neq p_i q_j \\text{ for some } i \\neq j,\\]where $p_i = p_{i \\cdot} = \\sum_{j=1}^{s}p_{ij}$ and $q_j = p_{\\cdot, j} = \\sum_{i=1}^{r}p_{ij}$. Here $d = rs-1$, $c = r + s - 2$ and $d-c = (r-1)(s-1)$. If null hypothesis is true, then\\[f_n(X^{(n)}, p, q) = \\frac{n!}{\\prod_{i,j=1}^{r,s} X_{ij}!} \\prod_{i,j=1}^{r,s} (p_i q _j)^{X_{ij}} = \\frac{n!}{\\prod_{i,j=1}^{r,s} X_{ij}!} \\prod_{i}^{r} p_i^{X_{i \\cdot}} \\prod_{j=1}^{s} q_j ^ {X_{\\cdot j}}.\\]Maximum-likelihood estimators are\\[\\hat{p}_i = \\frac{X_{i \\cdot}}{n} \\quad \\text{and} \\quad \\hat{q}_j = \\frac{X_{\\cdot j}}{n},\\]and likelihood function is\\[f_n(X^{(n)}, \\hat{p}, \\hat{q}) = \\frac{n!}{\\prod_{i,j=1}^{r,s} X_{ij}!} \\prod_{i,j=1}^{r,s} \\Big( \\frac{X_{i \\cdot} X_{\\cdot j}}{n^2} \\Big)^{X_{ij}}.\\]We get\\[T_n = -2 \\log \\lambda(X^{(n)}) = 2 \\sum_{i=1}^r \\sum_{j=1}^s X_{ij} \\log \\Big( \\frac{nX_{ij}}{ X_{i \\cdot} X_{\\cdot j} } \\Big)\\]and\\[\\varphi_n(X^{(n)}) = \\left \\{ \\begin{array}{cl} 1, &amp;amp; T_n &amp;gt; \\mathcal{X}_{(r-1)(s-1), 1-\\alpha}^2, \\\\ 0, &amp;amp; \\text{otherwise}, \\end{array} \\right.\\]which is called chi-square independence test. Using Taylor expansion with Law of Large Number we can get asymptotic equivalent\\[\\tilde{T}_n = \\sum_{i=1}^{r} \\sum_{j=1}^s \\frac{\\Big(X_{ij} -\\frac{X_{i \\cdot} X_{\\cdot j}}{n}\\Big)^2}{\\frac{X_{i \\cdot} X_{\\cdot j}}{n}}.\\]Usually,\\[V_n = \\sqrt{\\frac{\\tilde{T}_n}{n (\\min(r, s) - 1)}}\\]is used as dependency measure between $A$ and $B$, because under both null hypothesis and alternative convergence takes place\\[V_n^2 \\xrightarrow{\\mathbb{P}} \\frac{1}{\\min(r, s) - 1}\\sum_{i=1}^{r} \\sum_{j=1}^s \\frac{(p_{ij} - p_{i \\cdot}p_{\\cdot j} )^2}{p_{i \\cdot}p_{\\cdot j}}\\]Fig. 4. Visualization for chi-square independence test with $r=4$ and $s=5$. Significance level $\\alpha$ is fixed at $0.05$. Click on the cell of contingency table to increase $X_{ij}$ value, and CTRL + click to decrease (or ⌘ + click for Mac OS)." }, { "title": "Visual Guide to Statistics. Part III: Asymptotic Properties of Estimators", "url": "/posts/visual-guide-to-statistics-part-iii-asymptotic-properties-of-estimators/", "categories": "Statistics, Visual Guide", "tags": "statistics, consistent estimator, central limit theorem, slutsky lemma, delta method, asymptotic efficiency, maximum likelihood estimator", "date": "2022-04-12 06:00:00 +0300", "snippet": " A minimal condition for a good estimator is that it is getting closer to estimated parameter with growing size of sample vector. In this post we will focus on asymptotic properties of estimators.Consistency of estimatorsBerore talking about estimators convergence, let’s recall that there exist several different notions of convergence of random variables. Let $(X_n)$ be sequence of real-valued random variables, then we say $X_n$ converges in distribution towards the random variable $X$ if\\[\\lim\\limits_{n \\to \\infty} F_{n}(x) = F(x),\\]for every $x \\in \\mathbb{R}$, at which $F$ is continuous. $F_n(x)$ and $F(x)$ are the cumulative distribution functions for $X_n$ and $X$ respectively. We denote convergence in distribution as $X_n \\xrightarrow[]{\\mathcal{L}} X$. $X_n$ converges in probability to random variable $X$ if\\[\\lim\\limits_{n \\to \\infty} P(|X_n-X|&amp;gt;\\varepsilon)=0 \\quad \\forall \\varepsilon &amp;gt; 0.\\]Convergence in probability implies convergence in distribution. In the opposite direction, convergence in distribution implies convergence in probability when the limiting random variable $X$ is a constant. We denote convergence in probability as $X_n \\xrightarrow[]{\\mathbb{P}} X$. $X_n$ converges almost surely towards $X$ if\\[P(\\omega \\in \\Omega: \\lim\\limits_{n \\to \\infty} X_n(\\omega) = X(\\omega)) = 1.\\]Almost sure convergence implies convergence in probability, and hence implies convergence in distribution. Notation: $X_n \\xrightarrow[]{\\text{a.s.}} X$.The similar logic can be applied to a sequence of $d$-dimensional random variables. Also, recall continuous mapping theorem, which states that for a continuous function $f$ we have\\[\\begin{aligned}&amp;amp;X_n \\xrightarrow[]{\\mathcal{L}} X \\quad \\Rightarrow \\quad f(X_n) \\xrightarrow[]{\\mathcal{L}} f(X), \\\\&amp;amp;X_n \\xrightarrow[]{\\mathbb{P}} X \\quad \\Rightarrow \\quad f(X_n) \\xrightarrow[]{\\mathbb{P}} f(X), \\\\&amp;amp;X_n \\xrightarrow[]{\\text{a.s.}} X \\quad \\Rightarrow \\quad f(X_n) \\xrightarrow[]{\\text{a.s.}} f(X). \\end{aligned}\\]Now let $g_n$ be an estimator of $\\gamma(\\vartheta)$ with values in metric space. Assume that all experiments are defined on a joint probability space $P_\\vartheta$ for all $n$. We say that $g_n$ is (weakly) consistent if\\[g_n \\xrightarrow[]{\\mathbb{P}}\\gamma(\\vartheta) \\quad \\forall \\vartheta \\in \\Theta.\\] $g_n$ is strongly constistent if\\[g_n \\xrightarrow[]{\\text{a.s.}} \\gamma(\\vartheta) \\quad \\forall \\vartheta \\in \\Theta.\\]Recall the method of moments from Part I: $X_1, \\dots, X_n$ i.i.d. $\\sim P_\\vartheta$, $\\vartheta \\in \\Theta \\subset \\mathbb{R}^k$ and $\\gamma: \\Theta \\rightarrow \\Gamma \\subset \\mathbb{R}^l$. Also\\[m_j = \\mathbb{E}_\\vartheta[X_1^j] = \\int x^j P_\\vartheta(dx)\\]for $j = 1, \\dots, k$, and\\[\\gamma(\\vartheta) = f(m_1, \\dots, m_k).\\]Then choose\\[\\hat{\\gamma}(X) = f(\\hat{m}_1, \\dots, \\hat{m}_k),\\]where\\[\\hat{m}_j = \\frac{1}{n} \\sum_{i=1}^{n}X_k^j.\\]By Law of Large Numbers $\\hat{m}_j \\rightarrow m_j$ a.s. Since $f$ is continuous, we obtain\\[\\hat{\\gamma}(X) \\xrightarrow[]{\\text{a.s.}} \\gamma(\\vartheta).\\]Hence, $\\hat{\\gamma}(X)$ is a strongly consistent estimator.Central Limit TheoremLet $(X_n)$ be a sequence of $d$-dimensional random variables. Lévy’s continuity theorem states that\\[X_n \\xrightarrow[]{\\mathcal{L}} X \\quad \\Longleftrightarrow \\quad \\mathbb{E}[\\exp(iu^TX_n)] \\rightarrow \\mathbb{E}[\\exp(iu^TX)] \\quad \\forall u \\in \\mathbb{R}^d.\\]If we write $u=ty$ for $t \\in \\mathbb{R}$, $y \\in \\mathbb{R}^d$, then we can say that $X_n \\xrightarrow[]{\\mathcal{L}} X$ if and only if\\[y^TX_n \\xrightarrow[]{\\mathcal{L}} y^TX \\quad \\forall y \\in \\mathbb{R}^d.\\]This statement is called Cramér–Wold theorem.If $X_1, \\dots, X_n$ are i.i.d. with $\\mathbb{E}[X_j]=\\mu \\in \\mathbb{R}^d$ and $\\operatorname{Cov}(X_j)=\\Sigma \\in \\mathbb{R}^{d \\times d}$ (positive-definite, $\\Sigma &amp;gt; 0$), then for random vector\\[X^{(n)} = \\frac{1}{n}\\sum_{j=1}^n X_j \\in \\mathbb{R}^d\\]we know from one-dimensional Central Limit Theorem (CLT) that\\[\\sqrt{n}(y^TX^{(n)} -y^T\\mu) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, y^T\\Sigma y) \\quad \\forall y \\in \\mathbb{R}^d.\\]Applying Cramér–Wold theorem we get\\[\\sqrt{n}(X^{(n)}-\\mu) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, \\Sigma).\\]This statement is known as Multidimensional Central Limit Theorem.SampleSample 100xResetFig. 1. Visualization of multidimensional CLT for two-dimensional case. On the left-hand side there is random vector of two uniformly distributed random variables: $X_1, X_2 \\sim \\mathcal{U}(-1, 1)$ with mean $\\mu=(0, 0)^T$ and correlation $\\rho$. On the right-hand side is $\\sqrt{n} X^{(n)}$ which for large $n$ has approximately normal distribution with zero mean and the same covariance as $X$.Delta-methodLet $(X_n)$ and $(Y_n)$ be sequences of $d$-dimensional random variables, such that\\[X_n \\xrightarrow[]{\\mathcal{L}} X \\quad \\text{and} \\quad Y_n \\xrightarrow[]{\\mathbb{P}} c\\]for some constant vector $c$. Then we can apply the continuous mapping theorem, recognizing the functions $f(x, y)=x+y$ and $f(x, y)=xy$ are continuous, and conclude that $X_n+Y_n \\xrightarrow[]{\\mathcal{L}} X + c,$ $Y_n^TX_n \\xrightarrow[]{\\mathcal{L}} c^TX.$This statement is called Slutsky’s lemma and it can be extremely useful in estimating approximate distribution of estimators. For example, let $X_1, \\dots X_n$ i.i.d. $\\sim \\operatorname{Bin}(1, p)$. Estimator of $p$ $g_n(X) = \\overline{X}_n$ is unbiased and we know from CLT that\\[\\sqrt{\\overline{X}_n(1-\\overline{X}_n)} \\xrightarrow[]{\\mathbb{P}} \\sqrt{p(1-p)}.\\]By Slutsky’s lemma,\\[\\frac{\\sqrt{n}(\\overline{X}_n-p)}{\\sqrt{\\overline{X}_n(1-\\overline{X}_n)}} \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0,1)\\]and for large $n$ we have\\[P_p(|\\overline{X}_n-p|&amp;lt;\\varepsilon) \\approx 2 \\Phi\\Bigg(\\varepsilon\\sqrt{\\frac{n}{\\overline{X}_n(1-\\overline{X}_n)}}\\Bigg) -1 \\quad \\forall p \\in (0, 1),\\]where $\\Phi$ is cumulative distribution function for $\\mathcal{N}(0,1)$.Slutsky’s lemma also leads to important asymptotic property of estimator $g_n$, called Delta-method. Let $(X_n)$ be sequence of $d$-dimensional random variables, such that\\[\\frac{X_n-\\mu}{c_n} \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, \\Sigma),\\]where $c_n \\rightarrow 0$, $\\mu \\in \\mathbb{R}^d$ и $\\Sigma \\geq 0 \\in \\mathbb{R}^{d \\times d}$. Let also $g:\\mathbb{R}^d \\rightarrow \\mathbb{R}^m$ be continuously differentiable in $\\mu$ with Jacobian matrix $D \\in \\mathbb{R}^{m \\times d}$. Then:\\[\\frac{g(X_n)-g(\\mu)}{c_n} \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, D\\Sigma D^T).\\]ProofBy Slutsky&#39;s Lemma$$X_n-\\mu = \\frac{X_n-\\mu}{c_n}c_n \\xrightarrow[]{\\mathcal{L}} 0.$$Convergence in distribution to a constant implies convergence in probability: $X_n \\xrightarrow[]{\\mathbb{P}} \\mu$. Then$$\\frac{g(X_n)-g(\\mu)}{c_n}=g&#39;(\\mu)\\frac{X_n-\\mu}{c_n}+(g&#39;(\\xi_n)-g&#39;(\\mu))\\frac{X_n-\\mu}{c_n},$$for some intermediate point $\\xi_n$, such that $\\|\\xi_n-\\mu \\| \\leq \\|X_n-\\mu \\|$. From $X_n \\xrightarrow[]{\\mathbb{P}} \\mu$ we have $\\xi_n \\xrightarrow[]{\\mathbb{P}} \\mu$ and $g&#39;(\\xi_n) \\xrightarrow[]{\\mathbb{P}} g&#39;(\\mu)$ (because $g$ is continuously differentiable). Applying again Slutsky&#39;s Lemma:$$ g&#39;(\\mu) \\frac{X_n-\\mu}{c_n} \\xrightarrow[]{\\mathcal{L}} g&#39;(\\mu) \\cdot \\mathcal{N}(0, \\Sigma) $$finishes the proof. Recall example with method of moments, but now with additional conditions on $\\mathbb{E}[X_1^{2k}] &amp;lt; \\infty$ for all $\\vartheta \\in \\Theta$ and $\\gamma$ being continuously differentiable with Jacobian matrix $D$. We know from CLT that\\[\\sqrt{n}((\\hat{m}_1, \\dots, \\hat{m}_k)^T - (m_1, \\dots, m_k)^T) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, \\Sigma),\\]where\\[\\Sigma = (\\Sigma)_{i,j=1}^k = (m_{i+j} - m_i m_j)_{i,j=1}^k.\\]Then\\[\\sqrt{n}(\\gamma(\\hat{m}_1, \\dots, \\hat{m}_k) - \\gamma(m_1, \\dots, m_k)) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, D \\Sigma D^T).\\] Take another example: let $X_1, \\dots X_n$ be i.i.d. with\\[\\mathbb{E}_\\vartheta[X_i] = \\mu \\quad \\text{and} \\quad \\operatorname{Var}_\\vartheta(X_i) = \\sigma^2.\\]From CLT we have\\[\\sqrt{n}(\\overline{X}_n - \\mu) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, \\sigma^2).\\]Choose $\\overline{X}_n^2$ as an estimator for $\\mu^2$. Applying Delta-method we get\\[\\sqrt{n}(\\overline{X}_n^2-\\mu^2) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, 4\\mu^2\\sigma^2).\\] Let\\[(X_i, Y_i)^T \\sim \\mathcal{N} \\begin{pmatrix} \\begin{pmatrix} \\mu_1 \\\\ \\mu_2 \\end{pmatrix}, \\begin{pmatrix} \\sigma^2 &amp;amp; \\rho \\sigma \\tau \\\\ \\rho \\sigma \\tau &amp;amp; \\tau^2 \\end{pmatrix} \\end{pmatrix}, \\quad i = 1, \\dots, n,\\]be i.i.d with parameter $\\vartheta = (\\mu_1, \\mu_2, \\sigma^2, \\tau^2, \\rho)^T$. The estimator\\[\\hat{\\rho}_n = \\frac{SQ_{xy}}{\\sqrt{SQ_{xx} SQ_{yy}}},\\]where\\[SQ_{xy} = \\frac{1}{n} \\sum_{i=1}^{n}(X_i-\\overline{X}_n)(Y_i - \\overline{Y}_n),\\]$SQ_{xx}, SQ_{yy}$ - likewise, is called the Pearson correlation coefficient. Without loss of generality, assume $\\mu_1=\\mu_2=0$, $\\sigma=\\tau=1$, because $\\hat{\\rho}_n$ is invariant under affine transformation.Prove first that $S_n = (SQ_{xx}, SQ_{yy}, SQ_{xy})^T$ satisifies\\[\\sqrt{n}(S_n - m) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, V),\\]where $m=(1, 1, \\rho)^T$ and\\[V = 2 \\begin{pmatrix} 1 &amp;amp; \\rho^2 &amp;amp; \\rho \\\\ \\rho^2 &amp;amp; 1 &amp;amp; \\rho \\\\ \\rho &amp;amp; \\rho &amp;amp; (1 + \\rho^2)/2 \\end{pmatrix}.\\] Sketch of the proof We use Slutsky&#39;s Lemma and CLT to show that $$\\sqrt{n}(\\overline{X}_n \\overline{Y}_n) \\xrightarrow[]{\\mathbb{P}} 0, \\quad \\sqrt{n}(\\overline{X}_n)^2 \\xrightarrow[]{\\mathbb{P}} 0, \\quad \\sqrt{n}(\\overline{Y}_n)^2 \\xrightarrow[]{\\mathbb{P}} 0. $$Then it is simple to conclude$$\\sqrt{n}(S_n - m) - \\sqrt{n}\\Big(\\frac{1}{n}\\sum_{i=1}^{n}Z_i - m \\Big) \\xrightarrow[]{\\mathbb{P}} 0,$$with $Z_i = (X_i^2, Y_i^2, X_iY_i)^T$. Then prove that $$\\operatorname{Cov}(Z_i) = \\mathbb{E}[Z_i Z_i^T]-\\mathbb{E}[Z_i]\\mathbb{E}[Z_i]^T = V. $$The rest follows from multidimensional CLT.Then take $g(S_n)=\\hat{\\rho}_n$ with $g(x_1, x_2, x_3) = \\frac{x_3}{\\sqrt{x_1 x_2}}$. Jacobian matrix of $g$ at $m$:\\[D = (-\\rho/2, -\\rho/2, 1).\\]In total,\\[\\sqrt{n}(\\hat{\\rho}_n - \\rho) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, DVD^T) = \\mathcal{N}(0, (1-\\rho^2)^2).\\]Sample ResetFig. 2. Visualization of asymptotic normality for Pearson correlation coefficient. Drag sample dots to observe how it affects $SQ$ coefficients and $\\hat{\\rho}_n$.Asympotic efficiencyLet $g_n \\subset \\mathbb{R}^l$ be a sequence of estimators with\\[\\mu_n(\\vartheta)=\\mathbb{E}_\\vartheta[g_n] \\in \\mathbb{R}^l \\quad \\text{and} \\quad \\Sigma_n(\\vartheta)=\\operatorname{Cov}(\\vartheta) \\in \\mathbb{R}^{l \\times l},\\]such that $\\lVert \\Sigma_n(\\vartheta) \\rVert \\rightarrow 0$. Then $g_n$ is called asymptotically unbiased for $\\gamma(\\vartheta)$ if\\[\\mu_n(\\vartheta) \\rightarrow \\gamma(\\vartheta),\\] $g_n$ is called asymptotically normal if\\[\\Sigma_n^{-\\frac{1}{2}}(\\vartheta)(g_n-\\mu_n(\\vartheta)) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, \\mathbb{I}_l),\\]where $\\mathbb{I}_l$ is identity matrix.Let $f_n: \\mathcal{X} \\rightarrow \\mathbb{R}^l$ be asymptotically unbiased and asymptotically normal sequence of estimators. Under regularity conditions from Cramér–Rao theorem we call $g_n$ asymptotically efficient, if\\[\\lim\\limits_{n \\rightarrow \\infty} \\Sigma_n(\\vartheta) \\mathcal{I}(f_n(\\cdot, \\vartheta))=\\mathbb{I}_l \\quad \\forall \\vartheta \\in \\Theta,\\]where $\\mathcal{I}(f_n(\\cdot, \\vartheta))$ is Fisher information.The intuition behind definition above is the following: if $g_n$ is unbiased, then by Cramér–Rao theorem $\\operatorname{Cov}_\\vartheta(g_n) \\geq \\mathcal{I}^{-1}(f_n(\\cdot, \\vartheta))$. Due to asymptotic normality:\\[\\Sigma_n^{-\\frac{1}{2}}(\\vartheta)(g_n-\\mu_n(\\vartheta)) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, \\mathbb{I}_l)\\]we have approximately\\[\\operatorname{Cov}_\\vartheta(g_n) \\approx \\Sigma_n(\\vartheta) \\approx \\mathcal{I}^{-1}(f_n(\\cdot, \\vartheta))\\]and $g_n$ is asymptotically unbiased and asymptotically efficient.Recall example from Part I: for $X_1, \\dots X_n$ i.i.d. $\\sim \\mathcal{N}(\\mu, \\sigma^2)$ estimator\\[g_n(X) = \\begin{pmatrix} \\overline{X}_n \\\\ \\frac{1}{n-1} \\sum_{i=1}^{n} (X_i - \\overline{X}_n)^2 \\end{pmatrix}\\]satisfies the equality\\[\\operatorname{Cov}_\\vartheta(g_n) = \\begin{pmatrix} \\sigma^2/n &amp;amp; 0 \\\\ 0 &amp;amp; 2\\sigma^4 / (n - 1) \\end{pmatrix} = \\Sigma_n(\\vartheta).\\]But Fisher information is\\[\\mathcal{I}^{-1}(f_n(\\cdot, \\vartheta)) = \\begin{pmatrix} \\sigma^2/n &amp;amp; 0 \\\\ 0 &amp;amp; 2\\sigma^4 / n \\end{pmatrix}\\]and $g_n$ is not efficient, but asymptotically efficient.Asymptotic properties of maximum-likelihood estimatorsIn Part I we briefly mentioned maximum-likelihood estimators as one of the most common estimation methods in statistic. It is worth knowing what their asymptotic properties are. Let’s rewrite the definition here: let $X_1, \\dots X_n$ be i.i.d. $\\sim P_\\vartheta$, $\\vartheta \\in \\Theta$ with densities $f(\\cdot, \\vartheta)$. We call\\[\\ell(\\cdot, \\vartheta) = \\log f(\\cdot, \\vartheta)\\]the log-likelihood function and set\\[\\begin{aligned}\\hat{\\theta}_n(X) &amp;amp;= \\arg \\sup_{\\vartheta \\in \\Theta} f(X, \\vartheta) \\\\&amp;amp;= \\arg \\sup_{\\vartheta \\in \\Theta} \\ell (X, \\vartheta) \\\\&amp;amp;= \\arg \\sup_{\\vartheta \\in \\Theta} \\frac{1}{n} \\sum_{i=1}^{n} \\ell (X_i, \\vartheta) \\end{aligned}\\]as the maximum-likelihood estimator for $\\vartheta$.Now, say the following conditions are satisfied: $\\Theta \\subset \\mathbb{R}^k$ is compact space $L(\\eta, \\vartheta) = \\mathbb{E}[\\ell(X_i, \\eta)]$ and $L_n(\\eta) = \\frac{1}{n}\\sum_{i=1}^n\\ell(X_i, \\eta)$ are a.s. continuous functions over $\\eta$. \\[\\sup_{\\eta \\in \\Theta} | L_n(\\eta)-L(\\eta, \\vartheta)|\\xrightarrow{\\mathcal{L}}0.\\] Then maximum-likelihood estimator $\\hat{\\theta}_n$ is consistent.Proof: for any $\\eta \\in \\Theta$:\\[L(\\eta, \\vartheta) = \\int \\ell(x, \\eta) f(x,\\vartheta) dx = \\int \\ell(x,\\vartheta)f(x,\\vartheta)dx - KL(\\vartheta | \\eta),\\]where $KL$ is Kullback-Leibler divergence:\\[KL(\\vartheta | \\eta) = \\int_{\\mathcal{X}} \\log\\Big(\\frac{f(x,\\vartheta)}{f(x,\\eta)}\\Big) f(x,\\vartheta)dx.\\]It can be shown that\\[\\begin{aligned} KL(\\vartheta | \\eta) &amp;amp; = \\int_{\\mathcal{X}} -\\log\\Big(\\frac{f(x,\\eta)}{f(x,\\vartheta)}\\Big) f(x,\\vartheta)dx \\\\ \\color{\\Salmon}{\\text{Jensen inequality} \\rightarrow } &amp;amp; \\geq -\\log\\int_{\\mathcal{X}} \\frac{f(x,\\eta)}{f(x,\\vartheta)} f(x,\\vartheta)dx\\\\ &amp;amp; = 0,\\end{aligned}\\]and it turns into equality only when $f(x,\\vartheta) = f(x,\\eta)$ for almost every $x$. Therefore we conclude that $L(\\eta, \\vartheta)$ reaches maximum at $\\eta = \\vartheta$.Using the fact that function $m_f = \\arg\\max_{\\eta \\in \\Theta} f(\\eta)$ is continuous if $m_f$ is unique, we finish the proof from\\[\\vartheta = \\arg \\max L(\\eta, \\vartheta)\\quad \\text{and} \\quad \\hat{\\theta}_n=\\arg \\max L_n(\\eta)\\]and condition 3.Asymptotic efficiency of maximum-likelihood estimatorsIf the following conditions are satisfied: $\\Theta \\subset \\mathbb{R}^k$ is compact and $\\vartheta \\subset \\operatorname{int}(\\Theta)$. $\\ell(x, \\eta)$ is continuous $\\forall \\eta \\in \\Theta$ and twice continuously differentiable over $\\vartheta$ for almost every $x \\in \\mathcal{X}$. $\\ell(x, \\eta)$ is Lipschitz: there exist functions $H_0, H_2 \\in L^1(P_\\vartheta)$ and $H_1 \\in L^2(P_\\vartheta)$, such that:\\[\\sup_{\\eta \\in \\Theta} \\|\\ell(x, \\eta)\\| \\leq H_0(x), \\quad \\sup_{\\eta \\in \\Theta} \\|\\dot{\\ell}(x, \\eta)\\| \\leq H_1(x), \\quad \\sup_{\\eta \\in \\Theta} \\|\\ddot{\\ell}(x, \\eta)\\| \\leq H_2(x) \\quad \\forall x \\in \\mathcal{X}.\\] Fisher information\\[\\mathcal{I}(f(\\cdot, \\vartheta))=\\mathbb{E}_\\vartheta[\\dot{\\ell}(X,\\vartheta)\\dot{\\ell}(X,\\vartheta)^T]\\]is positive definite (and therefore invertible),then $\\hat{\\theta}_n$ is asymptotically normal:\\[\\sqrt{n}(\\hat{\\theta}_n-\\vartheta) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, \\mathcal{I}(f(\\cdot, \\vartheta))^{-1}).\\]We will prove it in 4 steps:Step 1. Prove the constistency of $\\hat{\\theta}_n$. For this we need to verify that all conditions from theorem about consistency of maximum-likelihood estimator are satisfied: Satisfied by the assumption. $L_n(\\eta)$ is a.s. continuous. Using 2-3 conditions and dominated convergence we get\\(|L(\\eta_1, \\vartheta) - L(\\eta_2, \\vartheta)| \\leq \\int_{\\mathcal{X}} |\\ell(x, \\eta_1) - \\ell(x,\\eta_2)| f(x,\\vartheta) \\mu(dx) \\rightarrow 0,\\)for $\\eta_1 \\rightarrow \\eta_2$. By Law of Large Numbers:\\(\\begin{aligned} \\limsup_{n \\rightarrow \\infty} \\sup_{\\| \\eta_1 - \\eta_2 \\| &amp;lt; \\delta} | L_n(\\eta_1) - L_n(\\eta_2)| &amp;amp; \\leq \\limsup_{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{i=1}^{n} \\sup_{\\| \\eta_1 - \\eta_2 \\| &amp;lt; \\delta} |\\ell(X_i, \\eta_1) - \\ell(X_i, \\eta_2) |\\\\ &amp;amp; = \\mathbb{E}_\\vartheta[\\sup_{\\| \\eta_1 - \\eta_2 \\| &amp;lt; \\delta}|\\ell(X,\\eta_1) - \\ell(X, \\eta_2)|] \\end{aligned}\\)Because $\\Theta$ is compact, function $\\ell(X, \\eta)$ is a.s. uniformly continuous in $\\eta$. As a consequence, the last statement converges to zero for $\\delta \\rightarrow 0$ (using again dominated convergence).Step 2. Let\\[\\dot{L}_n(\\vartheta) := \\frac{1}{n} \\sum_{i=1}^{n} \\dot{\\ell}(X_i, \\vartheta).\\]Prove that $\\sqrt{n}\\dot{L}_n(\\vartheta) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, \\mathcal{I}(f(\\cdot, \\vartheta))).$Let $A_n$ be $k$-dimensional rectangle with vertices in $\\hat{\\theta}_n$ and $\\vartheta$. Because $\\hat{\\theta}_n \\xrightarrow{\\mathcal{L}} \\vartheta$ and $\\vartheta \\in \\operatorname{int}(\\Theta)$, we have\\[P_\\vartheta(A_n \\subset \\operatorname{int}(\\Theta)) \\rightarrow 1.\\]Also\\[\\dot{L}_n(\\hat{\\theta}_n) = \\frac{1}{n} \\sum_{i=1}^{n} \\dot{\\ell}(X_i, \\hat{\\theta}_n) = 0\\]by definition of $\\hat{\\theta}_n$, and\\[\\begin{aligned} \\mathbb{E}[\\dot{\\ell}(X_i, \\vartheta)] &amp;amp; = \\int_{\\mathcal{X}} \\dot{\\ell}(x, \\vartheta) f(x, \\vartheta) dx \\\\ &amp;amp; = \\int_{\\mathcal{X}} \\dot{f}(x, \\vartheta) dx \\\\ &amp;amp; =\\frac{\\partial}{\\partial \\vartheta}\\int_{\\mathcal{X}} f(x, \\vartheta) dx = 0. \\end{aligned}\\]By definition\\[\\operatorname{Cov}(\\dot{\\ell}(X_i, \\vartheta)) = \\mathcal{I}(f(\\cdot, \\vartheta)).\\]Then by CLT:\\[\\sqrt{n} \\dot{L}_n(\\vartheta) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, \\mathcal{I}(f(\\cdot, \\vartheta))).\\]Step 3. By Mean Theorem:\\[-\\dot{L}_n(\\vartheta) = \\dot{L}_n(\\hat{\\theta}_n) - \\dot{L}_n(\\vartheta) = \\ddot{L}_n(\\widetilde{\\theta}_n)(\\hat{\\theta}_n - \\vartheta)\\]for some $\\widetilde{\\theta}_n \\in A_n$. Prove that $\\ddot{L}_n(\\widetilde{\\theta}_n) \\xrightarrow{\\mathcal{L}} -\\mathcal{I}(f(\\cdot, \\vartheta))$.We use the equation\\[\\ddot{\\ell}(x, \\vartheta) = \\frac{\\ddot{f}(x, \\vartheta)}{f(x,\\vartheta)} - \\dot{\\ell}(x, \\vartheta)\\dot{\\ell}(x, \\vartheta)^T.\\]to show that\\[\\mathbb{E}_\\vartheta[\\ddot{\\ell}(X, \\vartheta)] + \\mathcal{I}(f(\\cdot, \\vartheta)) = \\mathbb{E}_\\vartheta\\Big[ \\frac{\\ddot{f}(X, \\vartheta)}{f(X,\\vartheta)} \\Big] = 0,\\]From Law of Large Numbers it follows that\\[\\ddot{L}_n(\\vartheta) \\xrightarrow{\\mathcal{L}} - \\mathcal{I}(f(\\cdot, \\vartheta)).\\]Finally, we use the equality\\[\\lim\\limits_{\\delta \\rightarrow 0} \\lim\\limits_{n \\rightarrow \\infty} P_\\vartheta(\\| \\widetilde{\\theta}_n - \\vartheta \\| &amp;lt; \\delta) = 1\\]and continuity of $\\ddot{\\ell}$ over $\\vartheta$ to finish the proof.Step 4. Now we conclude that\\[\\lim\\limits_{n \\rightarrow \\infty} P_\\vartheta(\\ddot{L}_n(\\widetilde{\\theta}_n) \\text{ is invertible}) = 1.\\]and applying Slutsky’s lemma we get\\[\\begin{aligned} \\sqrt{n}(\\hat{\\theta}_n - \\vartheta) &amp;amp; = -\\ddot{L}_n(\\widetilde{\\theta}_n)^{-1} \\sqrt{n} \\dot{L}_n(\\vartheta) \\\\ &amp;amp; \\rightarrow \\mathcal{I}(f(\\cdot, \\vartheta))^{-1} \\mathcal{N}(0, \\mathcal{I}(f(\\cdot, \\vartheta))) \\\\&amp;amp;= \\mathcal{N}(0, \\mathcal{I}(f(\\cdot, \\vartheta))^{-1}). \\color{Salmon}{\\square} \\end{aligned}\\]Take an example: let $X_1, \\dots X_n$ be i.i.d. $\\sim \\operatorname{Exp}(\\lambda)$ with joint density\\[f_n(X, \\lambda) = \\lambda^n \\exp \\Big(-\\lambda \\sum_{i=1}^n X_i \\Big) \\quad \\forall x \\in \\mathbb{R}^+.\\]To find maximum-likelihood estimator one must maximize log-density\\[\\ell_n(X, \\lambda) = n \\log(\\lambda) - \\lambda \\sum_{i=1}^n X_i \\quad \\forall x \\in \\mathbb{R}^+\\]with respect to $\\lambda$. Taking the derivative and equating it to zero we get\\[\\frac{n}{\\lambda} = \\sum_{i=1}^{n} X_i,\\]and estimator is\\[\\hat{\\lambda}_n = \\frac{1}{\\overline{X}_n}.\\]Next, using the fact that\\[\\mathbb{E}[X] = \\lambda^{-1} \\quad \\text{and} \\quad \\operatorname{Var}(X) = \\lambda^{-2},\\]and $\\dot{\\ell}_1(X, \\lambda) = -(X - \\lambda^{-1})$, we calculate Fisher information:\\[\\mathcal{I}(f(\\cdot, \\lambda)) = \\mathbb{E}\\Big[\\Big(X - \\frac{1}{\\lambda}\\Big)^2\\Big]=\\frac{1}{\\lambda^2}.\\]By theorem of asymptotic efficiency of maximum-likelihood estimators we get\\[\\sqrt{n}(\\hat{\\lambda}_n - \\lambda) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, \\lambda^2),\\]On the other hand by CLT\\[\\sqrt{n}\\Big(\\overline{X}_n - \\frac{1}{\\lambda}\\Big) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}\\Big(0, \\frac{1}{\\lambda^2}\\Big).\\]Using Delta-method for $g(x) = x^{-1}$ we get the same result:\\[\\sqrt{n}(\\overline{X}_n^{-1} - \\lambda) \\xrightarrow[]{\\mathcal{L}} \\mathcal{N}(0, \\lambda^2).\\]" }, { "title": "Visual Guide to Statistics. Part II: Bayesian Statistics", "url": "/posts/visual-guide-to-statistics-part-ii-bayesian-statistics/", "categories": "Statistics, Visual Guide", "tags": "statistics, parameter estimation, bayesian inference, bayes estimator, minimax estimator, least favorable prior", "date": "2022-03-21 06:11:00 +0300", "snippet": " Part II introduces different approach to parameters estimation called Bayesian statistics.Basic definitionsWe noted in the previous part that it is extremely unlikely to get a uniformly best estimator. An alternative way to compare risk functions is to look at averaged values (weighting over parameters probabilities) or at maximum values for worst-case scenarios.In Bayes interpretation parameter $\\vartheta$ is random, namely instance of random variable $\\theta: \\Omega \\rightarrow \\Theta$ with distribution $\\pi$. We call $\\pi$ a prior distribution for $\\vartheta$. For an estimator $g \\in \\mathcal{K}$ and its risk $R(\\cdot, g)$\\[R(\\pi, g) = \\int_{\\Theta} R(\\theta, g) \\pi(d \\vartheta)\\]is called the Bayes risk of $g$ with respect to $\\pi$. An estimator $\\tilde{g} \\in \\mathcal{K}$ is called a Bayes estimator if it minimizes the Bayes risk over all estimators, that is\\[R(\\pi, \\tilde{g}) = \\inf_{g \\in \\mathcal{K}} R(\\pi, g).\\]The right hand side of the equation above is call the Bayes risk. The function $R(\\pi, g)$ plays the role of the average value over all risk functions, where the possible values ​​of $\\theta$ are weighted according to their probabilities. Distribution $\\pi$ can interpreted as prior knowledge of statistician about unknown parameter.In the following we will denote conditional distribution of $X$ (under condition $\\theta = \\vartheta$) as\\[P_\\vartheta = Q^{X \\mid \\theta=\\vartheta}\\]and joint distribution of $(X, \\theta)$ as $Q^{X, \\theta}$:\\[Q^{X, \\theta}(A) = \\int_\\Theta \\int_\\mathcal{X} 1_A(x,\\vartheta) P_\\vartheta (dx) \\pi(d \\vartheta).\\]Before experiment we have $\\pi = Q^\\theta$, marginal distribution of $\\theta$ under $Q^{X, \\theta}$, assumed distribution of parameter $\\vartheta$. After observation $X(\\omega)=x$ the information about $\\theta$ changes from $\\pi$ to $Q^{\\theta \\mid X=x}$, which we will call a posterior distribution of random variable $\\theta$ under condition $X=x$.Posterior riskRecall that risk function is an expected value of a loss function $L$:\\[R(\\vartheta, g) = \\int_{\\mathcal{X}} L(\\gamma(\\vartheta), g(x)) P_\\vartheta(dx).\\]Then\\[\\begin{aligned}R(\\pi,g) &amp;amp; =\\int_\\Theta R(\\vartheta, g) \\pi(d\\vartheta) \\\\&amp;amp;=\\int_{\\Theta} \\int_{\\mathcal{X}} L(\\gamma(\\vartheta), g(x)) P_\\vartheta(dx) \\pi(d\\vartheta)\\\\&amp;amp; = \\int_{\\Theta \\times \\mathcal{X}} L(\\gamma(\\vartheta), g(x)) Q^{X,\\theta} (dx, d\\vartheta) \\\\&amp;amp;=\\int_{\\mathcal{X}} {\\color{Salmon}{ \\int_{\\Theta} L(\\gamma(\\vartheta), g(x)) Q^{\\theta \\mid X = x} (d\\vartheta)}} Q^X(dx) \\\\&amp;amp; = \\int_{\\mathcal{X}} {\\color{Salmon}{R_{\\pi}^x(g)}} Q^X(dx).\\end{aligned}\\]The term\\[R_{\\pi}^x(g) :=\\int_{\\Theta} L(\\gamma(\\vartheta), g(x)) Q^{\\theta | X = x} (d\\vartheta)\\]is called a posterior risk of $g$ with given $X=x$. It can be shown that for an estimator $g^*$ of $\\vartheta$ to be Bayes, it must provide minimum posterior risk:\\[R_{\\pi}^x(g^*)=\\inf_{g \\in \\mathcal{K}}R_{\\pi}^x(g)=\\inf_{a \\in \\Theta} \\int L(\\vartheta, a) Q^{\\theta \\mid X = x}(d\\vartheta),\\]because $R(\\pi, g)$ is minimal if and only if $R_\\pi^x(g)$ is minimal. In particular, for quadratic loss $L(\\vartheta,a) = (\\vartheta-a)^2$ Bayes estimator is\\[g^*(x) = \\mathbb{E}[\\theta \\mid X = x] = \\int_{\\Theta} \\vartheta Q^{\\theta \\mid X=x} (d \\vartheta).\\]Say for $P_\\vartheta$ we have density function $f(x \\mid \\vartheta)$, and for $\\pi$ density is $h(\\vartheta)$. Then posterior distribution of $Q^{\\theta \\mid X=x}$ has density\\[f(\\vartheta|x) = \\frac{f(x|\\vartheta) h(\\vartheta)}{ \\int_\\Theta f(x|\\vartheta) h(\\vartheta) d\\vartheta }.\\]Posterior and Bayes risks respectively\\[R_\\pi^x(g) = \\frac{\\int_\\Theta L(\\vartheta, g(x))f(x|\\vartheta) h(\\vartheta) d\\vartheta}{\\int_\\Theta f(x|\\vartheta) h(\\vartheta) d\\vartheta}\\]and\\[R(\\pi, g)=\\int_{\\mathcal{X}}\\int_\\Theta L(\\vartheta, g(x))f(x|\\vartheta) h(\\vartheta) d\\vartheta dx.\\]Let’s take an example of an estimation of probability parameter for binomial distribution. Let $\\Theta = (0, 1)$, $\\mathcal{X} = \\lbrace 0, \\dots, n \\rbrace$ and\\[P_\\vartheta(X=x) = \\binom n x \\vartheta^x (1-\\vartheta)^{n-x}.\\]We take quadratic loss function $L(x,y)=(x-y)^2$. Say we only have observed one sample $X=x$. From previous post we know that binomial distribution belongs to exponential family and therefore $g(x) = \\frac{x}{n}$ is an UMVU estimator for $\\vartheta$ with\\[\\operatorname{Var}(g(X)) = \\frac{\\vartheta(1-\\vartheta)}{n}.\\]On the other hand, we have density\\[f(x | \\vartheta) = \\binom n x \\vartheta^x (1-\\vartheta)^{n-x} 1_{ \\lbrace 0, \\dots n \\rbrace }(x).\\]If we take prior uniform distribution $\\pi \\sim \\mathcal{U}(0, 1)$, then $ h(\\vartheta) = 1_{(0, 1)}(\\vartheta)$ and posterior density\\[f(\\vartheta \\mid x) = \\frac{\\vartheta^x (1-\\vartheta)^{n-x} 1_{(0,1)}(\\vartheta)}{B(x+1, n-x+1)},\\]where we have beta-function in denominator:\\[B(a,b)=\\int_{0}^{1} \\vartheta^{a-1} (1-\\vartheta)^{b-1} d \\vartheta.\\]Then Bayes estimator will be\\[\\begin{aligned}g^*(x)&amp;amp;=\\mathbb{E}[\\theta|X=x]\\\\&amp;amp;=\\int_0^1 \\frac{\\vartheta^{x+1}(1-\\vartheta^{n-x})}{B(x+1, n-x+1)}\\\\&amp;amp;=\\frac{B(x+2, n-x+1)}{B(x+1, n-x+1)} =\\frac{x+1}{n+2},\\end{aligned}\\]and Bayes risk:\\[\\begin{aligned} R(\\pi,g^*) &amp;amp; =\\int_0^1 R(\\vartheta, g^*) d\\vartheta\\\\ &amp;amp;=\\int_0^1 \\mathbb{E}\\Big[\\Big(\\frac{X+1}{n+2}-\\vartheta \\Big)^2\\Big]d\\vartheta \\\\ &amp;amp; =\\frac{1}{(n+2)^2} \\int_0^1 (n\\vartheta - n\\vartheta^2+1-4\\vartheta+4\\vartheta^2)\\ d\\vartheta\\\\ &amp;amp;=\\frac{1}{6(n+2)}. \\end{aligned}\\]Let’s take another example: $X_1, \\dots X_n$ i.i.d. $\\sim P_\\mu^1 = \\mathcal{N}(\\mu, \\sigma^2)$ with $\\sigma^2$ known in advance. Take for $\\mu$ prior distribution with gaussian density\\[h(\\mu) = \\frac{1}{\\sqrt{2 \\pi \\tau^2}} \\exp \\Big( -\\frac{(\\mu-\\nu)^2}{2\\tau^2} \\Big).\\]Taking density for $X$\\[f(x|\\mu)=\\Big( \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\Big)^n \\exp \\Big( \\frac{1}{2\\sigma^2}\\sum_{j=1}^n(x_j-\\mu)^2 \\Big ),\\]we get posterior distribution\\[Q^{\\mu|X=x} \\sim \\mathcal{N} \\Big( g_{\\nu, \\tau^2}(x), \\Big( \\frac{n}{\\sigma^2} + \\frac{1}{\\tau^2}\\Big)^{-1} \\Big),\\]where\\[g_{\\nu, \\tau^2}(x)=\\Big( 1 + \\frac{\\sigma^2}{n \\tau^2} \\Big)^{-1} \\overline{x}_n+\\Big( \\frac{n \\tau^2}{\\sigma^2}+1 \\Big)^{-1} \\nu.\\]For quadratic loss function $g_{\\nu, \\tau^2}(x)$ is a Bayes estimator. It can be interpreted as following: for large values of $\\tau$ (not enough prior information) estimator $g_{\\nu, \\tau^2}(x) \\approx \\overline{x}_n$.Otherwise, $g_{\\nu, \\tau^2}(x)$ $\\approx \\nu$.Samplen:Fig. 1. Bayesian inference for normal distribution.Minimax estimatorFor an estimator $g$\\[R^*(g) = \\sup_{\\vartheta \\in \\Theta} R(\\vartheta, g)\\]is called the maximum risk and\\[R^*(g^*) = \\inf_{g \\in \\mathcal{K}} R^*(g)\\]is minimax risk and corresponding $g$ - minimax estimator. The use of minimax estimator is aimed at protecting against large losses. Also it’s not hard to see, that\\[R^*(g) = \\sup_{\\pi \\in \\mathcal{M}} R(\\pi, g),\\]where $\\mathcal{M}$ is a set of all prior measures $\\pi$. If for some $\\pi^*$ we have\\[\\inf_{g \\in \\mathcal{K}} R(\\pi^*, g) \\geq \\inf_{g \\in \\mathcal{K}} R(\\pi, g) \\quad \\forall \\pi \\in \\mathcal{M},\\]then $\\pi^*$ is called the least favorable prior. If $g_\\pi$ is a Bayes estimator for prior $\\pi$ and also\\[R(\\pi, g_\\pi) = \\sup_{\\vartheta \\in \\Theta} R(\\vartheta, g_\\pi),\\]then for any $g \\in \\mathcal{K}$:\\[\\sup_{\\vartheta \\in \\Theta}R(\\vartheta, g) \\geq \\int_{\\Theta}R(\\vartheta, g)\\pi(d\\vartheta) \\geq \\int_{\\Theta}R(\\vartheta, g_\\pi)\\pi(d\\vartheta)=R(\\pi, g_\\pi)=\\sup_{\\vartheta \\in \\Theta}R(\\vartheta, g_\\pi)\\]and therefore $g_\\pi$ is a minimax estimator. Also, $\\pi$ is a least favorable prior, because for any distribution $\\mu$\\[\\begin{aligned}\\inf_{g \\in \\mathcal{K}} \\int_{\\Theta} R(\\vartheta, g)\\mu(d\\vartheta) &amp;amp;\\leq \\int_{\\Theta}R(\\vartheta, g_\\pi)\\mu(d\\vartheta) \\\\&amp;amp; \\leq \\sup_{\\vartheta \\in \\Theta} R(\\vartheta, g_\\pi) \\\\&amp;amp;= R(\\pi, g_\\pi) \\\\ &amp;amp;= \\inf_{g \\in \\mathcal{K}} \\int_{\\Theta}R(\\vartheta, g) \\pi(d\\vartheta).\\end{aligned}\\]Sometimes Bayes risk can be constant:\\[R(\\vartheta, g_\\pi) = c \\quad \\forall \\vartheta \\in \\Theta.\\]Then\\[\\sup_{\\vartheta \\in \\Theta} R(\\vartheta, g_\\pi) = c = \\int_{\\Theta} R(\\vartheta, g_\\pi) \\pi(d\\vartheta) = R(\\pi, g_\\pi),\\]$g_\\pi$ is minimax and $\\pi$ is least favorable prior.Let’s get back to an example with binomial distribution:\\[P_\\vartheta(X = x) = \\binom{n}{x} \\vartheta^x (1-\\vartheta)^{n-x}.\\]Again we use quadratic loss, but only this time we take parameterized beta distrubution $B(a, b)$ as our prior:\\[h(\\vartheta) = \\frac{\\vartheta^{a-1}(1-\\vartheta)^{b-1}1_{[0,1]}(\\vartheta)}{B(a, b)}.\\]Note that for $a = b = 1$ we have $\\theta \\sim \\mathcal{U}(0, 1)$. Now posterior distribution will be $Q^{\\vartheta \\mid X=x} \\sim B(x+a,n-x+b)$ with density\\[f(\\vartheta | x)= \\frac{\\vartheta^{x+a-1}(1-\\vartheta)^{n-x+b-1}1_{[0,1](\\vartheta)}}{B(x+a,n-x+b)}.\\]We use our prior knowledge that for random variable $Z \\sim B(p, q)$\\[\\mathbb{E}[Z] = \\frac{p}{p+q} \\quad \\text{and} \\quad \\operatorname{Var}(Z)=\\frac{pq}{(p+q)^2(p+q+1)}.\\]Recall that for quadratic loss expected value of $\\theta$ is Bayes estimator. Therefore,\\[g_{a,b}(x)=\\frac{x+a}{n+a+b}\\]is a Bayes estimator and it provides risk\\[\\begin{aligned} R(\\vartheta, g_{a,b})&amp;amp;=\\mathbb{E}[(g_{a,b}(X)-\\vartheta)^2] \\\\ &amp;amp;=\\frac{\\vartheta^2(-n+(a+b)^2+\\vartheta(n-2a(a+b))+a^2}{(n+a+b)^2}. \\end{aligned}\\]If we choose $\\hat{a}=\\hat{b}=\\frac{\\sqrt{n}}{2}$ then risk will be\\[R(\\vartheta, g_{\\hat{a}, \\hat{b}})=\\frac{1}{4(\\sqrt{n} + 1)^2}.\\]Such risk doesn’t depend on $\\vartheta$ and hence an estimator $g_{\\hat{a}, \\hat{b}}(x) = \\frac{x+\\sqrt{n}/2}{n+\\sqrt{n}}$ is minimax and $B(\\hat{a}, \\hat{b})$ is least favorable prior. Least fav. priorFig. 2. Bayesian inference for binomial distribution. Note that when least favorable prior is chosen, Bayes and minimax estimators coincide regardless of the sample value.Least favorable sequence of priorsLet\\[r_\\pi = \\inf_{g \\in \\mathcal{K}} R(\\pi, g), \\quad \\pi \\in \\mathcal{M}.\\]Then sequence $(\\pi_m)_{m \\in \\mathbb{N}}$ in $\\mathcal{M}$ is called least favorable sequence of priors if $\\lim_{m \\rightarrow \\infty} r_{\\pi_m} = r$, $r_\\pi \\leq r\\ $ $\\ \\forall \\pi \\in \\mathcal{M}$.Let $(\\pi_m)$ in $\\mathcal{M}$ be a sequence, such that $r_{\\pi_m} \\rightarrow r \\in \\mathbb{R}$. Also let there be an estimator $g^* \\in \\mathcal{K}$, such that\\[\\sup_{\\vartheta \\in \\Theta}R(\\theta, g^*) = r.\\]Then\\[\\sup_{\\vartheta \\in \\Theta} R(\\vartheta, g) \\geq \\int_{\\Theta} R(\\vartheta, g) \\pi_m(d \\vartheta) \\geq r_{\\pi_m} \\rightarrow r = \\sup_{\\vartheta \\in \\Theta}R(\\theta, g^*)\\]and therefore $g^*$ is minimax. Also for any $\\pi \\in \\mathcal{M}$\\[r_\\pi \\leq R(\\pi, g^*) = \\int_\\Theta R(\\vartheta, g^*) \\pi (d\\vartheta) \\leq \\sup_{\\vartheta \\in \\Theta} R(\\vartheta, g^*) = r,\\]hence $(\\pi_m)$ is a least favorable sequence of priors.Let’s get back to our previous example of estimating mean for normal distribution with known $\\sigma^2$. Say, we have prior distribution\\[h_m(\\mu)=\\frac{1}{\\sqrt{2 \\pi m}} \\exp \\Big \\{ -\\frac{(\\mu-\\nu)^2}{2m}\\Big \\}.\\]with $m \\in \\mathbb{N}$. Recall that Bayes estimator is\\[g_{\\nu, m}(x)=\\Big( 1 + \\frac{\\sigma^2}{n m} \\Big)^{-1} \\overline{x}_n+\\Big( \\frac{n m}{\\sigma^2}+1 \\Big)^{-1} \\nu.\\]For any $\\mu \\in \\mathbb{R}$\\[\\begin{aligned} R(\\mu, g_{\\nu, m}) &amp;amp; = \\mathbb{E}[(g_{\\nu, m}(X)-\\mu)^2] \\\\ &amp;amp; = \\mathbb{E}\\Bigg[\\bigg(\\Big( 1 + \\frac{\\sigma^2}{n m} \\Big)^{-1} (\\overline{X}_n-\\mu)+\\Big( \\frac{n m}{\\sigma^2}+1 \\Big)^{-1} (\\nu-\\mu)\\bigg)^2\\Bigg] \\\\ &amp;amp; = \\Big(1 + \\frac{\\sigma^2}{nm}\\Big)^{-2} \\frac{\\sigma^2}{n} + \\Big( 1+\\frac{nm}{\\sigma^2} \\Big)^{-2}(\\nu-\\mu)^2 \\xrightarrow[m \\ \\rightarrow \\infty]{} \\frac{\\sigma^2}{n} \\end{aligned}\\]Since the risk is bounded from above:\\[R(\\mu, g_{\\nu, m}) \\leq \\frac{\\sigma^2}{n} + (\\mu - \\nu)^2,\\]by Lebesgue Dominated Convergence Theorem 1 we have\\[r_{\\pi_{m}}=R(\\pi_{m}, g_{\\nu, m})=\\int_{\\mathbb{R}}R(\\mu, g_{\\nu, m})\\pi_{m}(d\\mu) \\longrightarrow \\frac{\\sigma^2}{n}.\\]Since for estimator $g^*(x)=\\overline{x}_n$ the equality\\[R(\\mu, g^*)=\\mathbb{E}[(\\overline{X}_n-\\mu)^2]=\\frac{\\sigma^2}{n},\\]holds, $g^*(x)$ is minimax and $\\pi_{m}$ is sequence of least favorable priors. Suppose there is measurable space $X$ with measure $\\mu$. Also let $\\lbrace f_n \\rbrace_{n=1}^\\infty$ and $f$ be measurable functions on $X$ and $f_n(x) \\rightarrow f(x)$ almost everywhere. Then if there exists an integrable function $g$ defined on the same space such that\\[|f_n(x)| \\leq g(x) \\quad \\forall n \\in \\mathbb{N}\\] almost everywhere, then $f_n$ and $f$ are integrable and\\[\\lim\\limits_{n \\rightarrow \\infty} \\int_X f_n(x) \\mu(dx) = \\int_X f(x) \\mu(dx).\\] &amp;#8617; " }, { "title": "Visual Guide to Statistics. Part I: Basics of Point Estimation", "url": "/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/", "categories": "Statistics, Visual Guide", "tags": "statistics, parameter estimation, frequentist inference, exponential family, cramer-rao inequality, fisher information, maximum-likelihood estimator, method of moments", "date": "2022-03-21 06:00:00 +0300", "snippet": " This series of posts is a guidance for those who already have knowledge in probability theory and would like to become familiar with mathematical statistics. Basically, these are notes from lectures I attended while being a student in Christian-Albrechts University in Kiel, Germany. They helped me close all the gaps in my knowledge of math under the hood of modern statistics. For those who are interested in the lectures themselves can refer to the original material or my translation to Russian. This post in particular focuses on point estimators of distribution parameters and their characteristics.IntroImagine that you are a pharmaceutical company, which is about to introduce a new drug into production. Prior to launch you need to carry out experiments to assess its quality depending on the dosage. Say you give this medicine to an animal, after which the animal is examined and checked whether it has recovered or not by taking a dose of $X$. You can think of the result as random variable $Y$ following Bernoulli distribution:\\[Y \\sim \\operatorname{Bin}(1, p(X)),\\]where $p(X)$ is a probability of healing given dose $X$.Typically, several independent experiments $Y_1, \\dots, Y_n$ with different doses $X_1, \\dots, X_n$ are made, such that\\[Y_i \\sim \\operatorname{Bin}(1, p(X_i)).\\]Our goal is to estimate function $p: [0, \\infty) \\rightarrow [0, 1]$. For example, we can simplify to parametric model\\[p(x) = 1 - e^{-\\vartheta x}, \\quad \\vartheta &amp;gt; 0.\\]Then estimating $p(x)$ is equal to estimating parameter $\\vartheta $.SampleFig. 1. Visualization of statistical experiments. The question arises: how do we estimate the value of $\\vartheta$ based on our observations?Formally, we can define parameter space $\\Theta$ with $\\vert \\Theta \\vert \\geq 2$ and family of probability measures $\\mathcal{P} = \\lbrace P_\\vartheta \\mid \\vartheta \\in \\Theta \\rbrace$, where $P_\\vartheta \\neq P_{\\vartheta’} \\ \\forall \\vartheta \\neq \\vartheta’$. Then we are interested in the true distribution $P \\in \\mathcal{P}$ of random variable $X$.Recall from probability theory that random variable $X$ is a mapping from set of all possible outcomes $\\Omega$ to a sample space $\\mathcal{X}$. On the basis of given sample $x = X(\\omega)$, $\\omega \\in \\Omega$ we make a decision about the unknown $P$. By identifying family $\\mathcal{P}$ with the parameter space $\\Theta$, a decision for $P$ is equivalent to a decision for $\\vartheta$. In our example above\\[Y_i \\sim \\operatorname{Bin}(1, 1 - e^{-\\vartheta X_i}) = P_\\vartheta^i\\]and\\[\\mathcal{X} = \\{0, 1\\}^n, \\quad \\Theta=\\left[0, \\infty\\right), \\quad \\mathcal{P}=\\{\\otimes_{i=1}^nP_{\\vartheta}^i \\mid \\vartheta&amp;gt;0 \\}.\\]Uniformly best estimatorMandatory parameter estimation example which can be found in every statistics handbook is mean and variance estimation for Normal distribution. Let $X_1, \\dots, X_n$ i.i.d. $\\sim \\mathcal{N}(\\mu, \\sigma^2) = P_{\\mu, \\sigma^2}$. The typical estimation for $\\vartheta = (\\mu, \\sigma^2)$ would be\\[g(x) = \\begin{pmatrix} \\overline{x}_n \\\\ \\hat{s}_n^2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{n} \\sum_{i=1}^n x_i \\\\ \\frac{1}{n} \\sum_{i=1}^n (x_i-\\overline{x}_n)^2 \\end{pmatrix}.\\]We will get back to characteristics of this estimation later. But now it is worth noting that we are not always interested in $\\vartheta$ itself, but in an appropriate functional $\\gamma(\\vartheta)$. We can see it in another example.Let $X_1, \\dots, X_n$ i.i.d. $\\sim F$, where $F(x) = \\mathbb{P}(X \\leq x)$ is unknown distribution function. Here $\\Theta$ is an infinite-dimensional family of distribution functions. Say we are interested in value of this function at point $k$:\\[\\gamma(F) = F(k).\\]Then a point estimator could be $g(x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}_{\\lbrace X_i \\leq k \\rbrace }$.Now we are ready to construct formal definition of parameter estimation. Let’s define measurable space $\\Gamma$ and mapping $\\gamma: \\Theta \\rightarrow \\Gamma$. Then measurable function $ g: \\mathcal{X} \\rightarrow \\Gamma $ is called (point) estimation of $\\gamma(\\vartheta)$.But how do we choose point estimator and how we can measure its goodness? Let’s define a criteria, non-negative function $L: \\Gamma \\times \\Gamma \\rightarrow [0, \\infty)$, which we will call loss function, and for estimator $g$ function\\[R(\\vartheta, g) = \\mathbb{E}[L(\\gamma(\\vartheta), g(X))] = \\int_\\mathcal{X} L(\\gamma(\\vartheta), g(X)) P_\\vartheta(dx)\\]we will call the risk of $g$ under $L$.If $\\vartheta$ is the true parameter and $g(x)$ is an estimation, then $L(\\gamma(\\vartheta), g(x))$ measures the corresponding loss. If $\\Gamma$ is a metric space, then loss functions typically depend on the distance between $\\gamma(\\vartheta)$ and $g(x)$, like the quadratic loss $L(x, y)=(x-y)^2$ for $\\Gamma = \\mathbb{R}$. The risk then is the expected loss.Suppose we have a set of all possible estimators $g$ called $\\mathcal{K}$. Then it is natural to search for an estimator, which mimimizes our risk, namely $\\tilde{g} \\in \\mathcal{K}$, such that\\[R(\\vartheta, \\tilde{g}) = \\inf_{g \\in \\mathcal{K}} R(\\vartheta, g), \\quad \\forall \\vartheta \\in \\Theta.\\]Let’s call $\\tilde{g}$ an uniformly best estimator.Sadly, in general, neither uniformly best estimators exist nor is one estimator uniformly better than another. For example, let’s take normal random variable with unit variance and estimate its mean $\\gamma(\\mu) = \\mu$ with quadratic loss. Pick the trivial constant estimator $g_\\nu(x)=\\nu$. Then\\[R(\\mu, g_\\nu) = \\mathbb{E}[(\\mu - \\nu)^2] = (\\mu - \\nu)^2.\\]In particular, $R(\\nu, g_\\nu)=0$. Thus no $g_\\nu$ is uniformly better than some $g_\\mu$. Also, in order to obtain a uniformly better estimator $\\tilde{g}$,\\[\\mathbb{E}[(\\tilde{g}(X)-\\mu)^2]=0 \\quad \\forall \\mu \\in \\mathbb{R}\\]has to hold, which basically means that $\\tilde{g}(x) = \\mu$ with probability $1$ for every $\\mu \\in \\mathbb{R}$, which of course is impossible.UMVU estimatorIn order to still get optimal estimators we have to choose other criteria than a uniformly smaller risk. What should be our objective properties of $g$?Let’s think of difference between this estimator’s expected value and the true value of $\\gamma$ being estimated:\\[B_\\vartheta(g) = \\mathbb{E}[g(X)] - \\gamma(\\vartheta).\\]This value in is called bias of $g$ and estimator $g$ is called unbiased if\\[B_\\vartheta(g) = 0 \\quad \\forall \\vartheta \\in \\Theta.\\]It is reasonable (at least at the start) to put constraint on unbiasedness for $g$ and search only in\\[\\mathcal{E}_\\gamma = \\lbrace g \\in \\mathcal{K} \\mid B_\\vartheta(g) = 0 \\rbrace.\\]Surely there can be infinite number of unbiased estimators, and we not only interested in expected value of $g$, but also in how $g$ can vary from it. Variance of $g$ can be chosen as our metric for goodness. We call estimator $\\tilde{g}$ uniformly minimum variance unbiased (UMVU) if\\[\\operatorname{Var}(\\tilde{g}(X)) = \\mathbb{E}[(\\tilde{g}(X) - \\gamma(\\theta))^2] = \\inf_{g \\in \\mathcal{E}_\\gamma} \\operatorname{Var}(g(X)).\\]In general, if we choose $L(x, y) = (x - y)^2$, then\\[MSE_\\vartheta(g) = R(\\vartheta, g)=\\mathbb{E}[(g(X)-\\gamma(\\vartheta))^2]=\\operatorname{Var}_\\vartheta(g(X))+B_\\vartheta^2(g)\\]is called the mean squared error. Note that in some cases biased estimators have lower MSE because they have a smaller variance than does any unbiased estimator.Chi-squared and t-distributionsRemember we talked about $\\overline{x}_n$ and $\\hat{s}_n^2$ being typical estimators for mean and standard deviation of normally distributed random variable? Now we are ready to talk about their properties, but firstly we have to introduce two distributions: Let $X_1, \\dots, X_n$ be i.i.d. $\\sim \\mathcal{N}(0, 1)$. Then random variable $Z = \\sum_{i=1}^n X_i^2$ has chi-squared distribution with $n$ degrees of freedom (notation: $Z \\sim \\chi_n^2$). Its density:\\[f_{\\chi_n^2}(x) = \\frac{x^{\\frac{n}{2}-1} e^{-\\frac{x}{2}}}{2^{\\frac{n}{2}}\\Gamma\\big(\\frac{n}{2}\\big)}, \\quad x &amp;gt; 0,\\] where $\\Gamma(\\cdot)$ is a gamma function:\\[\\Gamma(\\alpha) = \\int_0^\\infty x^{\\alpha-1} e^{-x} dx, \\quad \\alpha &amp;gt; 0.\\] It’s easy to see that $\\mathbb{E}[Z] = \\sum_{i=1}^n \\mathbb{E}[X_i^2] = n$ and\\[\\operatorname{Var}(Z) = \\sum_{i=1}^n \\operatorname{Var}(X_i^2) = n\\big(\\mathbb{E}[X_1^4]) - \\mathbb{E}[X_1^2]^2\\big) = 2n.\\] Let $Y \\sim \\mathcal{N}(0, 1)$ and $Z \\sim \\chi_n^2$, then\\[T = \\frac{Y}{\\sqrt{Z/n}}\\] has t-distribution with $n$ degrees of freedom (notation $T \\sim t_n$). Its density:\\[f_{t_n}(x) = \\frac{\\Gamma \\big( \\frac{n+1}{2} \\big) } { \\sqrt{n \\pi} \\Gamma \\big( \\frac{n}{2} \\big) } \\Big( 1 + \\frac{x^2}{n} \\Big)^{\\frac{n+1}{2}}.\\] Fig. 2. Probability density functions for $\\chi_n^2$ and $t_n$-distributions. Move slider to observe how they look for different degrees of freedom $n$. Note that with large $n$ $t_n$ converges to normal distribution.It can now be shown that\\[\\overline{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i \\sim \\mathcal{N} \\Big( \\mu, \\frac{\\sigma^2}{n} \\Big)\\]and\\[\\hat{s}_n^2(X) = \\frac{1}{n}\\sum_{i=1}^n (X_i - \\overline{X}_n)^2 \\sim \\frac{\\sigma^2}{n} \\chi^2_{n-1}.\\]As a consequence:\\[\\frac{(n-1)(\\overline{X}_n-\\mu)}{\\sqrt{n}s_n^2(X)} \\sim t_{n-1}.\\]ProofDistribution of $\\overline{X}_n$ follows from properties of Normal distribution. Let$$ Y_i = \\frac{X_i - \\mu}{\\sigma} \\sim \\mathcal{N}(0, 1)$$and $Y = (Y_1, \\dots, Y_n)^T$. Choose orthogonal matrix $A$ such that its last row:$$ v^T = \\Big( \\frac{1}{\\sqrt{n}} \\dots \\frac{1}{\\sqrt{n}} \\Big).$$Then for $Z = AY$ the following equality holds:$$ \\sum_{i=1}^n Z_i^2 = Z^TZ = Y^TA^TAY = Y^TY= \\sum_{i=1}^n Y_i^2.$$From $\\operatorname{Cov}(Z)=A^TA = \\mathbb{I}_n$ we have $Z \\sim \\mathcal{N}(0, \\mathbb{I}_n).$ Also$$ \\begin{aligned} \\sqrt{n} \\overline{X}_n &amp;amp;= \\frac{1}{\\sqrt{n}} \\sum_{i=1}^n (\\sigma Y_i + \\mu) \\\\ &amp;amp; = \\sigma v^T Y + \\sqrt{n} \\mu \\\\&amp;amp;= \\sigma Z_n + \\sqrt{n} \\mu \\end{aligned} $$ and$$ \\begin{aligned} n \\hat{s}_n^2(X) &amp;amp;= \\sum_{i=1}^n (X_i - \\overline{X}_n)^2 = \\sigma^2 \\sum_{i=1}^n(Y_i - \\overline{Y}_n)^2 \\\\&amp;amp; = \\sigma^2 \\big(\\sum_{i=1}^n Y_i^2 - n \\overline{Y}_n^2\\big) = \\sigma^2 \\big(\\sum_{i=1}^n Y_i^2 - \\big(\\frac{1}{n} \\sum_{i=1}^n Y_i^2 \\big)^2 \\big) \\\\&amp;amp; = \\sigma^2 (\\sum_{i=1}^n Z_i^2 - Z_n^2) = \\sigma^2 \\sum_{i=1}^{n-1} Z_i^2 \\sim \\chi_{n-1}^2. \\end{aligned} $$Both estimators are independent as functions of $Z_n$ and $Z_1, \\dots, Z_{n-1}$ respectively.Let’s check which of these estimators are unbiased. We have $\\mathbb{E}[\\overline{X}_n] = \\mu$, therefore $\\overline{X}_n$ is unbiased. On the other hand\\[\\mathbb{E}[\\hat{s}_n^2(X)] = \\frac{\\sigma^2}{n} (n - 1) \\neq \\sigma^2.\\]Sample ResetFig. 3. Statistical experiments in estimating $\\sigma$ for $X_1, \\dots, X_n$ i.i.d. $\\sim \\mathcal{N}(0, 1)$. We see here that while $\\overline{X}_n$ varies around $\\mu=0$, expected value of estimator $\\hat{s}_n^2(X)$ is lower than $\\sigma^2 = 1$.So far we figured the unbiasedness of $g(X) = \\overline{X}_n$. But how can we tell if $\\overline{X}_n$ is an UMVU estimator? Can we find an estimator of $\\mu$ with variance lower than $\\frac{\\sigma^2}{n}$?Efficient estimatorGiven a set of unbiased estimators, it is not an easy task to determine which one provides the smallest variance. Luckily, we have a theorem which gives us a lower bound for an estimator variance.Suppose we have a family of densities $f(\\cdot, \\vartheta)$, such that following regularity conditions are satisfied: Set $M_f=\\lbrace x \\in \\mathcal{X} \\mid f(x, \\vartheta) &amp;gt; 0 \\rbrace$ doesn’t depend on $\\vartheta$ Partial derivative $\\frac{\\partial}{\\partial \\vartheta} \\log f(x, \\vartheta)$ exists $\\forall x \\in \\mathcal{X}$. The following equalities hold: 1 $\\mathbb{E} \\big[\\frac{\\partial}{\\partial \\vartheta} \\log f(X, \\vartheta)\\big] = 0,$ $\\mathbb{E} \\big[g(X) \\frac{\\partial}{\\partial \\vartheta} \\log f(X, \\vartheta)\\big] = \\frac{\\partial}{\\partial \\vartheta} \\mathbb{E}[g(X)].$ $0&amp;lt;\\mathbb{E} \\big[\\big(\\frac{\\partial}{\\partial \\vartheta} \\log f(X, \\vartheta)\\big)^2\\big]&amp;lt;\\infty$Let’s define functions\\[U_\\vartheta(x) = \\left\\{\\begin{array}{ll} \\frac{\\partial}{\\partial \\vartheta} \\log f(x, \\vartheta), &amp;amp; \\text{if } x \\in M_f, \\\\ 0, &amp;amp; \\text{otherwise,} \\end{array} \\right.\\]and\\[\\mathcal{I}(f(\\cdot, \\vartheta))=\\mathbb{E} \\big[\\big(\\frac{\\partial}{\\partial \\vartheta} \\log f(X, \\vartheta)\\big)^2\\big].\\]Under given regularity conditions we have\\[\\mathbb{E}[U_\\vartheta(X)] = \\mathbb{E}\\big[\\frac{\\partial}{\\partial \\vartheta} \\log f(x, \\vartheta)\\big] = \\frac{\\partial}{\\partial \\vartheta} \\mathbb{E}[\\log f(x, \\vartheta)] = 0\\]and\\[\\operatorname{Var}(U_\\vartheta(X)) = \\mathbb{E}[(U_\\vartheta(X))^2]=\\mathcal{I}(f(\\cdot, \\vartheta)).\\]Then using Cauchy-Schwartz inequality we get\\[\\begin{aligned} \\big( \\frac{\\partial}{\\partial \\vartheta} \\mathbb{E}[g(X)] \\big)^2 &amp;amp;= \\big( \\mathbb{E}[g(X) \\cdot U_\\vartheta(X)] \\big)^2 \\\\ &amp;amp; = \\big(\\operatorname{Cov}(g(X), U_\\vartheta(X)) \\big)^2 \\\\&amp;amp; \\leq \\operatorname{Var}(g(X))\\cdot \\operatorname{Var}(U_\\vartheta(X)) \\\\ &amp;amp;= \\mathcal{I}(f(\\cdot, \\vartheta))\\cdot \\operatorname{Var}(g(X)). \\end{aligned}\\]The resulting inequality:\\[\\operatorname{Var}(g(X)) \\geq \\frac{\\big(\\frac{\\partial}{\\partial \\vartheta} \\mathbb{E}[g(X)]\\big)^2}{\\mathcal{I}(f(\\cdot, \\vartheta))} \\quad \\forall \\vartheta \\in \\Theta\\]gives us Cramér–Rao bound. Function $\\mathcal{I}(f(\\cdot, \\vartheta))$ is called Fisher information for family $\\mathcal{P} = \\lbrace P_\\vartheta \\mid \\vartheta \\in \\Theta \\rbrace$. If an unbiased estimator $g$ satisfies the upper equation with equality, then it is called efficient.This theorem gives a lower bound for the variance of an estimator for $\\gamma(\\vartheta) = \\mathbb{E}[g(X)]$ and can be used in principle to obtain UMVU estimators. Whenever the regularity conditions are satisfied for all $g \\in \\mathcal{E}_\\gamma$, then any efficient and unbiased estimator is UMVU.Also, for a set of i.i.d. variables $X_1, \\dots X_n$, meaning that their joint density distribution is\\[f(x,\\vartheta) = \\prod_{i=1}^n f^i(x,\\vartheta),\\]we have\\[\\mathcal{I}(f(\\cdot, \\vartheta))=n\\mathcal{I}(f^1(\\cdot, \\vartheta)).\\]Let’s get back to the example with $X_1, \\dots, X_n$ i.i.d. $\\sim \\mathcal{N}(\\mu, 1)$ having the density\\[f^1(x, \\vartheta) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{(x-\\mu)^2}{2}}.\\]Then\\[\\mathcal{I}(f^1(\\cdot, \\mu)) = \\mathbb{E} \\Big[ \\big( \\frac{\\partial}{\\partial \\mu} \\log f^1 (X_1, \\mu)\\big)^2 \\Big] = \\mathbb{E}[(X_1 - \\mu)^2] = 1.\\]In particular, for $X = (X_1, \\dots, X_n)$ Fisher information $\\mathcal{I}(f(X, \\mu)) = n$ and Cramér–Rao bound for unbiased estimator:\\[\\operatorname{Var}(g(X)) \\geq \\frac{1}{n} \\big( \\frac{\\partial}{\\partial \\mu} \\mathbb{E}[g(X)] \\big)^2 = \\frac{1}{n}.\\]Therefore, $g(x) = \\overline{x}_n$ is an UMVU estimator.Multidimensional Cramér–Rao inequalityDefine function\\[G(\\vartheta)=\\Big( \\frac{\\partial}{\\partial \\vartheta_j} \\mathbb{E}[g_i(X)] \\Big)_{i,j} \\in \\mathbb{R}^{k \\times d}.\\]Then with multidimensional Cauchy-Shwartz inequality one can prove that under similar regularity conditions we have:\\[\\operatorname{Cov}(g(X)) \\geq G(\\vartheta) \\mathcal{I}^{-1}(f(\\cdot, \\vartheta))G^T(\\vartheta) \\in \\mathbb{R}^{k \\times k},\\]in the sense of Löwner ordering2, where\\[\\mathcal{I}(f(\\cdot, \\vartheta))=\\Big( \\mathbb{E}\\Big[\\frac{\\partial}{\\partial \\vartheta_i} \\log f(X, \\vartheta) \\cdot \\frac{\\partial}{\\partial \\vartheta_j} \\log f(X, \\vartheta) \\Big] \\Big)_{i,j=1}^d \\in \\mathbb{R}^{d \\times d}.\\]For an example with $X_1, \\dots X_n$ i.i.d. $\\sim \\mathcal{N}(\\mu, \\sigma^2)$ with density\\[f^1(x,\\vartheta)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\Big(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\Big)\\]we have\\[U_\\vartheta = \\Big(\\frac{\\partial}{\\partial \\mu} \\log f^1(X_1,\\vartheta), \\frac{\\partial}{\\partial \\sigma^2} \\log f^1(X_1,\\vartheta)\\Big)^T = \\begin{pmatrix} (X_1-\\mu)/\\sigma^2 \\\\ -\\frac{1}{2\\sigma^2}+\\frac{1}{\\sigma^4}(X_1-\\mu)^2 \\end{pmatrix}.\\]Fisher information then\\[\\mathcal{I}(f^1(\\cdot, \\vartheta))=\\mathbb{E}[U_\\vartheta U_\\vartheta^T]= \\begin{pmatrix} \\sigma^{-2} &amp;amp; 0 \\\\ 0 &amp;amp; \\frac{1}{2}\\sigma^{-4} \\end{pmatrix} = \\frac{1}{n}\\mathcal{I}(f(\\cdot, \\vartheta)).\\]If $g(X)$ is an unbiased estimator, then $G(\\vartheta)$ is identity matrix and Cramér–Rao bound then\\[\\begin{aligned}\\operatorname{Cov}_\\vartheta(g(X)) &amp;amp; \\geq G(\\vartheta) \\ \\mathcal{I}^{-1} (f(\\cdot, \\vartheta)) \\ G^T(\\vartheta) \\\\ &amp;amp;= \\mathcal{I}^{-1}(f(\\cdot, \\vartheta)) = \\begin{pmatrix} \\frac{\\sigma^{2}}{n} &amp;amp; 0 \\\\ 0 &amp;amp; \\frac{2\\sigma^{4}}{n} \\end{pmatrix}. \\end{aligned}\\]In particular for an unbiased estimator\\[\\widetilde{g}(X)=\\Big(\\overline{X}_n, \\frac{1}{n-1} \\sum_{i=1}^n(X_j-\\overline{X}_n)^2 \\Big)^T\\]the following inequality holds\\[\\operatorname{Cov}_\\vartheta(\\widetilde{g}(X)) = \\begin{pmatrix} \\frac{\\sigma^{2}}{n} &amp;amp; 0 \\\\ 0 &amp;amp; \\frac{2\\sigma^{4}}{n-1} \\end{pmatrix} \\geq \\mathcal{I}(f(\\cdot, \\vartheta)),\\]therefore $\\widetilde{g}$ is not efficient.Exponential familyIn the previous examples, we consider without proof the fulfillment of all regularity conditions of the Cramér–Rao inequality. Next, we will discuss a family of distributions for which the Cramér–Rao inequality turns into an equality.Proposition: let $P_\\vartheta$ be distribution with density\\[f(x, \\vartheta) = c(\\vartheta) h(x) \\exp(\\vartheta T(x)) \\quad \\forall \\vartheta \\in \\Theta.\\]Then equality in Cramér–Rao theorem holds for $g(x) = T(x)$.ProofFirst let us note that $\\int_{\\mathcal{X}}f(x)\\mu(dx) = 1$ for all $\\vartheta \\in \\Theta$, hence$$ c(\\vartheta)=\\Big( \\int_{\\mathcal{X}} h(x)\\exp (\\vartheta T(x) ) dx \\Big)^{-1} $$and$$ \\begin{aligned} 0 &amp;amp; = \\frac{\\partial}{\\partial \\vartheta} \\int_{\\mathcal{X}} c(\\vartheta) h(x) \\exp ( \\vartheta T(x) ) dx \\\\ &amp;amp; = \\int_{\\mathcal{X}} (c&#39;(\\vartheta)+c(\\vartheta)T(x)) h(x) \\exp ( \\vartheta T(x) ) dx. \\end{aligned} $$Using these two equations we get$$ \\begin{aligned} \\mathbb{E}[T(X)] &amp;amp; = c(\\vartheta) \\int_{\\mathcal{X}} h(x) T(x) \\exp ( \\vartheta T(x)) dx \\\\ &amp;amp; = -c&#39;(\\vartheta) \\int_{\\mathcal{X}}h(x) \\exp ( \\vartheta T(x) ) dx \\\\ &amp;amp; = -\\frac{c&#39;(\\vartheta)}{c(\\vartheta)}=(-\\log c(\\vartheta))&#39;. \\end{aligned} $$Fisher information:$$ \\mathcal{I}(f(\\cdot, \\vartheta)) = \\mathbb{E}\\Big[\\Big( \\frac{\\partial}{\\partial \\vartheta} \\log f(X, \\vartheta) \\Big)^2\\Big]=\\mathbb{E}[(T(X)+(\\log c(\\vartheta))&#39;)^2]=\\operatorname{Var}(T(X)). $$Also$$ \\begin{aligned} \\frac{\\partial}{\\partial \\vartheta} \\mathbb{E}[T(X)] &amp;amp; =\\int_{\\mathcal{X}} c&#39;(\\vartheta) h(x) T(x) \\exp ( \\vartheta T(x) ) dx + \\int_{\\mathcal{X}} c(\\vartheta) h(x) T^2(x) \\exp ( \\vartheta T(x) ) dx \\\\ &amp;amp; = \\frac{c&#39;(\\vartheta)}{c(\\vartheta)} \\int_{\\mathcal{X}} c(\\vartheta) h(x) T(x) \\exp ( \\vartheta T(x) ) dx + \\mathbb{E}[(T(X))^2] \\\\ &amp;amp; = \\mathbb{E}[(T(X))^2] - (\\mathbb{E}[T(X)])^2. \\end{aligned} $$ Therefore, $$ \\frac{\\Big(\\frac{\\partial}{\\partial\\vartheta}\\mathbb{E}[T(X)] \\Big)^2}{\\mathcal{I}(f(\\cdot, \\vartheta))}= \\operatorname{Var}(T(X)). $$Formally, family $\\mathcal{P} = \\lbrace P_\\vartheta \\mid \\vartheta \\in \\Theta \\rbrace $ is called an exponential family if there exist mappings $c, Q_1, \\dots Q_k: \\Theta \\rightarrow \\mathbb{R}$ and $h, T_1, \\dots T_k: \\mathcal{X} \\rightarrow \\mathbb{R}$ such that\\[f(x,\\vartheta) = c(\\vartheta) h(x) \\exp \\Big( \\sum_{j=1}^k Q_j(\\vartheta) T_j(x) \\Big).\\]$\\mathcal{P}$ is called $k$-parametric exponential family if functions $1, Q_1, \\dots Q_k$ and $1, T_1, \\dots T_k$ are linearly independent. Then we have equality to Cramér–Rao bound for $g = (T_1, \\dots T_k)^T$.Here are some examples: If $X \\sim \\operatorname{Bin}(n, \\vartheta)$, then\\[\\begin{aligned}f(x, \\vartheta) &amp;amp;= \\binom n x \\vartheta^x (1-\\vartheta)^{n-x} \\\\&amp;amp;= (1-\\vartheta)^n \\binom n x \\exp \\Big(x \\log \\frac{\\vartheta}{1-\\vartheta} \\Big).\\end{aligned}\\] Here $c(\\vartheta) = (1-\\vartheta)^n$, $h(x) = \\binom n x$, $T_1(x) = x$ and $Q_1(\\vartheta) = \\log \\frac{\\vartheta}{1-\\vartheta}$. If $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$, then $\\vartheta = (\\mu, \\sigma^2)^T$ and\\[\\begin{aligned}f(x, \\vartheta) &amp;amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\Big( \\frac{(x-\\mu)^2}{2\\sigma^2} \\Big) \\\\&amp;amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\Big( -\\frac{\\mu^2}{2\\sigma^2} \\Big) \\exp\\Big( -\\frac{x^2}{2\\sigma^2} + \\frac{\\mu x}{\\sigma^2} \\Big),\\end{aligned}\\] where $c(\\vartheta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp \\big( -\\frac{\\mu^2}{2\\sigma^2} \\big) $, $Q_1(\\vartheta) = -\\frac{1}{2\\sigma^2}$, $Q_2(\\vartheta) = \\frac{\\mu}{\\sigma^2}$, $T_1(x)=x^2$ and $T_2(x)=x$. If $X \\sim \\operatorname{Poisson}(\\lambda)$, then \\[f(x, \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!} = e^{-\\lambda} \\frac{1}{x!} \\exp \\big(x \\log \\lambda \\big).\\]Denoting $Q(\\vartheta) = (Q_1(\\vartheta), \\dots, Q_k(\\vartheta))^T$ we get transformed parametric space $ \\Theta^* = Q(\\Theta) $, which we call natural parametric space. In examples above $X \\sim \\operatorname{Bin}(n, \\vartheta)$: $\\Theta^* = \\lbrace \\log \\frac{\\vartheta}{1-\\vartheta} \\mid \\vartheta \\in (0, 1) \\rbrace = \\mathbb{R}$. $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$: $\\Theta^* = \\big\\lbrace \\big( \\frac{\\mu}{\\sigma^2}, -\\frac{1}{\\sigma^2} \\big) \\mid \\mu \\in \\mathbb{R}, \\sigma^2 \\in \\mathbb{R}^+ \\big\\rbrace = \\mathbb{R} \\times \\mathbb{R}^-.$ $X \\sim \\operatorname{Poisson}(\\lambda)$: $\\Theta^* = \\lbrace \\log \\lambda \\mid \\lambda \\in \\mathbb{R}^+ \\rbrace = \\mathbb{R}$.It must be noted that for an exponential family $\\mathcal{P}$ estimator $T(X) = (T_1(X), \\dots T_k(X))$ is UMVU for $\\mathbb{E}[T(X)]$. For example, if $X_1, \\dots X_n$ i.i.d. $\\sim \\mathcal{N}(\\mu, \\sigma^2)$ with joint density\\[f(x,\\vartheta) = c(\\vartheta) \\exp \\Big( -\\frac{n}{2\\sigma^2}\\Big( \\frac{1}{n} \\sum_{i=1}^n x_i^2 \\Big) + \\frac{n\\mu}{\\sigma^2}\\Big( \\frac{1}{n}x_i \\Big) \\Big),\\]then estimator\\[T(X) = \\Big( \\frac{1}{n} \\sum_{i=1}^n X_i, \\frac{1}{n} \\sum_{i=1}^n X_i^2 \\Big)\\]is efficient for $(\\mu, \\mu^2 + \\sigma^2)^T$.Common estimation methodsIf distribution doesn’t belong to exponential family, then for such case there exist two classical estimation methods: Method of moments. Let $X_1, \\dots X_n$ i.i.d. $\\sim P_\\vartheta$ and\\[\\gamma(\\vartheta) = f(m_1, \\dots, m_k),\\] where $m_j = \\mathbb{E}[X_1^j]$. Then estimation by method of moments will be\\[\\hat{\\gamma} (X) = f(\\hat{m}_1, \\dots, \\hat{m}_k),\\] where $m_j = \\frac{1}{n}\\sum_{i=1}^nX_i^j$. Maximum likelihood method. Say $\\gamma(\\vartheta) = \\vartheta \\in \\mathbb{R}^k$. Then $\\hat{\\vartheta}(x)$ is a maximum likelihood estimator if\\[f(x, \\hat{\\vartheta}) = \\sup_{\\vartheta \\in \\Theta} f(x, \\vartheta).\\] Again in example $X_1, \\dots X_n$ i.i.d. $\\sim \\mathcal {N}(\\mu, \\sigma^2)$ an estimator for $\\vartheta = (\\mu, \\sigma^2)^T = (m_1, m_2 - m_1^2)^T$ by method of moments will be\\[\\hat{\\gamma}(\\vartheta)=(\\hat{m}_1, \\hat{m}_2-\\hat{m}_1^2)^T=(\\overline{x}_n, \\hat{s}_n^2)^T.\\]It’s easy to show that this estimator coincides with the estimation obtained by the maximum likelihood method.Let’s take another example, $X_1, \\dots X_n$ i.i.d. $\\sim \\mathcal{U}(0, \\vartheta)$, where estimated parameter $\\vartheta &amp;gt; 0$. One can show that estimator\\[g_{ML}(X) = X_{(n)} = \\max \\lbrace X_1, \\dots X_n \\rbrace\\]is a maximum-likelihood estimator. On the other hand,\\[g_{MM}(X) = 2 \\overline{X}_n\\]is an estimator by method of moments. Also, maximum-likelihood estimator follows scaled Beta-distribution, $g_{ML}(X) \\sim \\vartheta B(n, 1)$, and therefore it is biased:\\[\\mathbb{E}[g_{ML}(X)] = \\vartheta\\frac{n}{n+1}.\\]UMVU estimator is $g(X) = X_{(n)} (1 + \\frac{1}{n})$, and its variance:\\[\\operatorname{Var}[g(X)] = \\vartheta^2\\frac{1}{n(n+2)} &amp;lt; \\frac{\\vartheta^2}{n}\\]However, the Cramér-Rao lower bound is $\\frac{\\vartheta^2}{n}$. This shows importance of regularity conditions for Cramér-Rao theorem. Here, invariance of $M_f$ is not satisified and Cramér-Rao inequality doesn’t hold. Let’s rewrite these equations in equivalent forms:\\[\\int_\\mathcal{X} \\frac{\\partial}{\\partial \\vartheta} \\log f(x, \\vartheta) f(x, \\vartheta) d x = \\int_\\mathcal{X} \\frac{\\partial}{\\partial \\vartheta} f(x, \\vartheta) d x=\\frac{\\partial}{\\partial \\vartheta}\\int_\\mathcal{X} f(x, \\vartheta) d x =0,\\]\\[\\int_\\mathcal{X} g(x) \\frac{\\partial}{\\partial \\vartheta} f(x, \\vartheta) dx = \\frac{\\partial}{\\partial \\vartheta} \\int_\\mathcal{X} g(x) f(x,\\vartheta) dx.\\] In both cases it means that we can interchange differentiation and integration. &amp;#8617; For symmetric matrices $A$ and $B$ we say that\\[A \\geq 0 \\Longleftrightarrow A \\text{ is positive semi-definite},\\]\\[A \\geq B \\Longleftrightarrow A - B \\geq 0.\\] &amp;#8617; " } ]
