<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Visual Guide to Statistics. Part I: Basics of Point Estimation" /><meta property="og:locale" content="en" /><meta name="description" content="This series of posts is a guidance for those who already have knowledge in probability theory and would like to become familiar with mathematical statistics. Basically, these are notes from lectures I attended while being a student in Christian-Albrechts University in Kiel, Germany. They helped me close all the gaps in my knowledge of math under the hood of modern statistics. For those who are interested in the lectures themselves can refer to the original material or my translation to Russian. This post in particular focuses on point estimators of distribution parameters and their characteristics." /><meta property="og:description" content="This series of posts is a guidance for those who already have knowledge in probability theory and would like to become familiar with mathematical statistics. Basically, these are notes from lectures I attended while being a student in Christian-Albrechts University in Kiel, Germany. They helped me close all the gaps in my knowledge of math under the hood of modern statistics. For those who are interested in the lectures themselves can refer to the original material or my translation to Russian. This post in particular focuses on point estimators of distribution parameters and their characteristics." /><link rel="canonical" href="https://astralord.github.io/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/" /><meta property="og:url" content="https://astralord.github.io/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/" /><meta property="og:site_name" content="AstraBlog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-03-21T06:00:00+03:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Visual Guide to Statistics. Part I: Basics of Point Estimation" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-07-03T23:09:13+03:00","datePublished":"2022-03-21T06:00:00+03:00","description":"This series of posts is a guidance for those who already have knowledge in probability theory and would like to become familiar with mathematical statistics. Basically, these are notes from lectures I attended while being a student in Christian-Albrechts University in Kiel, Germany. They helped me close all the gaps in my knowledge of math under the hood of modern statistics. For those who are interested in the lectures themselves can refer to the original material or my translation to Russian. This post in particular focuses on point estimators of distribution parameters and their characteristics.","headline":"Visual Guide to Statistics. Part I: Basics of Point Estimation","mainEntityOfPage":{"@type":"WebPage","@id":"https://astralord.github.io/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/"},"url":"https://astralord.github.io/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/"}</script><title>Visual Guide to Statistics. Part I: Basics of Point Estimation | AstraBlog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="AstraBlog"><meta name="application-name" content="AstraBlog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/marvel-icon.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">AstraBlog</a></div><div class="site-subtitle font-italic">A place to learn and share knowledge about AI-related things</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/astralord" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['samarin_ad','mail.ru'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/aleksandr-samarin-b8a35496" aria-label="linkedin" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://t.me/astrlrd" aria-label="telegram" class="order-6" target="_blank" rel="noopener"> <i class="fab fa-telegram"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG â€º <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Visual Guide to Statistics. Part I: Basics of Point Estimation</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Visual Guide to Statistics. Part I: Basics of Point Estimation</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Aleksandr Samarin </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Mon, Mar 21, 2022, 6:00 AM +0300" >Mar 21, 2022<i class="unloaded">2022-03-21T06:00:00+03:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Jul 3, 2023, 11:09 PM +0300" >Jul 3, 2023<i class="unloaded">2023-07-03T23:09:13+03:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3071 words">17 min read</span></div></div><div class="post-content"><blockquote><p>This series of posts is a guidance for those who already have knowledge in probability theory and would like to become familiar with mathematical statistics. Basically, these are notes from lectures I attended while being a student in Christian-Albrechts University in Kiel, Germany. They helped me close all the gaps in my knowledge of math under the hood of modern statistics. For those who are interested in the lectures themselves can refer to the <a href="https://github.com/astralord/Statistics-lectures/blob/master/vorlesungonline.pdf">original material</a> or my <a href="https://github.com/astralord/Statistics-lectures/blob/master/book.pdf">translation to Russian</a>.</p><p>This post in particular focuses on point estimators of distribution parameters and their characteristics.</p></blockquote><h3 id="intro">Intro</h3><p>Imagine that you are a pharmaceutical company, which is about to introduce a new drug into production. Prior to launch you need to carry out experiments to assess its quality depending on the dosage. Say you give this medicine to an animal, after which the animal is examined and checked whether it has recovered or not by taking a dose of $X$. You can think of the result as random variable $Y$ following Bernoulli distribution:</p>\[Y \sim \operatorname{Bin}(1, p(X)),\]<p>where $p(X)$ is a probability of healing given dose $X$.</p><p>Typically, several independent experiments $Y_1, \dots, Y_n$ with different doses $X_1, \dots, X_n$ are made, such that</p>\[Y_i \sim \operatorname{Bin}(1, p(X_i)).\]<p>Our goal is to estimate function $p: [0, \infty) \rightarrow [0, 1]$. For example, we can simplify to parametric model</p>\[p(x) = 1 - e^{-\vartheta x}, \quad \vartheta &gt; 0.\]<p>Then estimating $p(x)$ is equal to estimating parameter $\vartheta $.</p><style> .svg-container { display: inline-block; position: relative; width: 100%; padding-bottom: 25%; vertical-align: top; overflow: hidden; } .svg-content { display: inline-block; position: absolute; top: 0; left: 0; } .svg-content-responsive { display: inline-block; position: absolute; top: 0; left: 0; } .ticks { font: 10px arvo; } .track, .track-inset, .track-overlay { stroke-linecap: round; } .track { stroke: #000; stroke-opacity: 0.8; stroke-width: 7px; } .track-inset { stroke: #ddd; stroke-width: 5px; } .track-overlay { pointer-events: stroke; stroke-width: 50px; stroke: transparent; } .handle { fill: #fff; stroke: #000; stroke-opacity: 0.8; stroke-width: 1px; } #sample-button { top: 15px; left: 15px; background: #65AD69; padding-right: 26px; border-radius: 3px; border: none; color: white; margin: 0; padding: 0 1px; width: 60px; height: 25px; font-family: Arvo; font-size: 11px; } #sample-button:hover { background-color: #696969; } #sample-button-2 { top: 15px; left: 15px; background: #65AD69; padding-right: 26px; border-radius: 3px; border: none; color: white; margin: 0; padding: 0 1px; width: 60px; height: 25px; font-family: Arvo; font-size: 11px; } #sample-button-2:hover { background-color: #696969; } #reset-button { top: 15px; left: 15px; background: #E86456; padding-right: 26px; border-radius: 3px; border: none; color: white; margin: 0; padding: 0 1px; width: 60px; height: 25px; font-family: Arvo; font-size: 11px; } #reset-button:hover { background-color: #696969; }</style><script src="https://d3js.org/d3.v4.min.js"></script><link href="https://fonts.googleapis.com/css?family=Arvo" rel="stylesheet" /><p><button id="sample-button">Sample</button></p><div id="drug_exp" class="svg-container"></div><script> d3.select("#drug_exp") .style("position", "relative"); function drug_exp() { var theta = 0.2; var margin = {top: 10, right: 0, bottom: 10, left: 30}, width = 750 - margin.left - margin.right, height = 150 - margin.top - margin.bottom, fig_height = 125 - margin.top - margin.bottom, fig_width = 650; const w = width + margin.left + margin.right; const h = height + margin.top + margin.bottom; var svg = d3.select("div#drug_exp") .append("svg") .attr("preserveAspectRatio", "xMinYMin meet") .attr("viewBox", "0 0 " + w + " " + h) .classed("svg-content", true) .append("g") .attr("transform", "translate(" + margin.left + "," + margin.top + ")"); const g = svg.append("g") .attr("id", "node"); var x = d3.scaleLinear() .domain([0, 10]) .range([10, fig_width]); var xAxis = g.append("g") .attr("transform", "translate(0," + fig_height + ")") .call(d3.axisBottom(x)); xAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var y = d3.scaleLinear() .range([fig_height - 10, 0]) .domain([0, 1]); var yAxis = g.append("g") .call(d3.axisLeft(y).ticks(1)); yAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var xi_text = d3.select("#drug_exp") .append("div") .text("Dose \\(X_i \\)") .style('color', '#696969') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", g.node().getBoundingClientRect().width / 2 + "px") .style("top", g.node().getBoundingClientRect().height + "px"); var y_itext = d3.select("#drug_exp") .append("div") .text("\\(Y_i \\)") .style('color', '#696969') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", margin.left / 2 + "px") .style("top", 0.4 * g.node().getBoundingClientRect().height + "px"); var figs = []; for (var i = 0; i < 11; i += 1) { if (Math.random() < 1 - Math.exp(-theta * i)) { figs.push(g.append("path") .attr("d", d3.symbol().type(d3.symbolCross).size(100)) .attr("transform", function(d) { return "translate(" + x(i) + "," + y(1) + ")"; }) .style("fill", "#65AD69") .style('stroke', 'black') .style('stroke-width', '0.7') .style('opacity', 0.8)); } else { figs.push(g.append("path") .attr("d", d3.symbol().type(d3.symbolCross).size(100)) .attr("transform", function(d) { return "translate(" + x(i) + "," + y(0) + ") rotate(-45)"; }) .style("fill", "#E86456") .style('stroke', 'black') .style('stroke-width', '0.7') .style('opacity', 0.8)); } } function updateSymbols() { for (var i = 0; i < 11; i += 1) { if (Math.random() < 1 - Math.exp(-theta * i)) { figs[i].transition() .duration(1000) .attr("d", d3.symbol().type(d3.symbolCross).size(100)) .attr("transform", function(d) { return "translate(" + x(i) + "," + y(1) + ")"; }) .style("fill", "#65AD69"); } else { figs[i].transition() .duration(1000) .attr("d", d3.symbol().type(d3.symbolCross).size(100)) .attr("transform", function(d) { return "translate(" + x(i) + "," +y(0) + ") rotate(-45)"; }) .style("fill", "#E86456"); } } } var sampleButton = d3.select("#sample-button"); sampleButton .on("click", function() { updateSymbols(); }); } drug_exp(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Fig. 1. Visualization of statistical experiments. The question arises: how do we estimate the value of $\vartheta$ based on our observations?</em></p><p>Formally, we can define <strong>parameter space</strong> $\Theta$ with $\vert \Theta \vert \geq 2$ and family of probability measures $\mathcal{P} = \lbrace P_\vartheta \mid \vartheta \in \Theta \rbrace$, where $P_\vartheta \neq P_{\varthetaâ€™} \ \forall \vartheta \neq \varthetaâ€™$. Then we are interested in the true distribution $P \in \mathcal{P}$ of random variable $X$.</p><p>Recall from probability theory that random variable $X$ is a mapping from set of all possible outcomes $\Omega$ to a <strong>sample space</strong> $\mathcal{X}$. On the basis of given sample $x = X(\omega)$, $\omega \in \Omega$ we make a decision about the unknown $P$. By identifying family $\mathcal{P}$ with the parameter space $\Theta$, a decision for $P$ is equivalent to a decision for $\vartheta$. In our example above</p>\[Y_i \sim \operatorname{Bin}(1, 1 - e^{-\vartheta X_i}) = P_\vartheta^i\]<p>and</p>\[\mathcal{X} = \{0, 1\}^n, \quad \Theta=\left[0, \infty\right), \quad \mathcal{P}=\{\otimes_{i=1}^nP_{\vartheta}^i \mid \vartheta&gt;0 \}.\]<h3 id="uniformly-best-estimator">Uniformly best estimator</h3><p>Mandatory parameter estimation example which can be found in every statistics handbook is mean and variance estimation for Normal distribution. Let $X_1, \dots, X_n$ i.i.d. $\sim \mathcal{N}(\mu, \sigma^2) = P_{\mu, \sigma^2}$. The typical estimation for $\vartheta = (\mu, \sigma^2)$ would be</p>\[g(x) = \begin{pmatrix} \overline{x}_n \\ \hat{s}_n^2 \end{pmatrix} = \begin{pmatrix} \frac{1}{n} \sum_{i=1}^n x_i \\ \frac{1}{n} \sum_{i=1}^n (x_i-\overline{x}_n)^2 \end{pmatrix}.\]<p>We will get back to characteristics of this estimation later. But now it is worth noting that we are not always interested in $\vartheta$ itself, but in an appropriate functional $\gamma(\vartheta)$. We can see it in another example.</p><p>Let $X_1, \dots, X_n$ i.i.d. $\sim F$, where $F(x) = \mathbb{P}(X \leq x)$ is unknown distribution function. Here $\Theta$ is an infinite-dimensional family of distribution functions. Say we are interested in value of this function at point $k$:</p>\[\gamma(F) = F(k).\]<p>Then a point estimator could be $g(x) = \frac{1}{n} \sum_{i=1}^n \mathbf{1}_{\lbrace X_i \leq k \rbrace }$.</p><p>Now we are ready to construct formal definition of parameter estimation. Letâ€™s define measurable space $\Gamma$ and mapping $\gamma: \Theta \rightarrow \Gamma$. Then measurable function $ g: \mathcal{X} \rightarrow \Gamma $ is called <strong>(point) estimation</strong> of $\gamma(\vartheta)$.</p><p>But how do we choose point estimator and how we can measure its goodness? Letâ€™s define a criteria, non-negative function $L: \Gamma \times \Gamma \rightarrow [0, \infty)$, which we will call <strong>loss function</strong>, and for estimator $g$ function</p>\[R(\vartheta, g) = \mathbb{E}[L(\gamma(\vartheta), g(X))] = \int_\mathcal{X} L(\gamma(\vartheta), g(X)) P_\vartheta(dx)\]<p>we will call the <strong>risk of $g$ under $L$</strong>.</p><p>If $\vartheta$ is the true parameter and $g(x)$ is an estimation, then $L(\gamma(\vartheta), g(x))$ measures the corresponding loss. If $\Gamma$ is a metric space, then loss functions typically depend on the distance between $\gamma(\vartheta)$ and $g(x)$, like the quadratic loss $L(x, y)=(x-y)^2$ for $\Gamma = \mathbb{R}$. The risk then is the expected loss.</p><p>Suppose we have a set of all possible estimators $g$ called $\mathcal{K}$. Then it is natural to search for an estimator, which mimimizes our risk, namely $\tilde{g} \in \mathcal{K}$, such that</p>\[R(\vartheta, \tilde{g}) = \inf_{g \in \mathcal{K}} R(\vartheta, g), \quad \forall \vartheta \in \Theta.\]<p>Letâ€™s call $\tilde{g}$ an <strong>uniformly best estimator</strong>.</p><p>Sadly, in general, neither uniformly best estimators exist nor is one estimator uniformly better than another. For example, letâ€™s take normal random variable with unit variance and estimate its mean $\gamma(\mu) = \mu$ with quadratic loss. Pick the trivial constant estimator $g_\nu(x)=\nu$. Then</p>\[R(\mu, g_\nu) = \mathbb{E}[(\mu - \nu)^2] = (\mu - \nu)^2.\]<p>In particular, $R(\nu, g_\nu)=0$. Thus no $g_\nu$ is uniformly better than some $g_\mu$. Also, in order to obtain a uniformly better estimator $\tilde{g}$,</p>\[\mathbb{E}[(\tilde{g}(X)-\mu)^2]=0 \quad \forall \mu \in \mathbb{R}\]<p>has to hold, which basically means that $\tilde{g}(x) = \mu$ with probability $1$ for every $\mu \in \mathbb{R}$, which of course is impossible.</p><h3 id="umvu-estimator">UMVU estimator</h3><p>In order to still get <em>optimal</em> estimators we have to choose other criteria than a uniformly smaller risk. What should be our objective properties of $g$?</p><p>Letâ€™s think of difference between this estimatorâ€™s expected value and the true value of $\gamma$ being estimated:</p>\[B_\vartheta(g) = \mathbb{E}[g(X)] - \gamma(\vartheta).\]<p>This value in is called <strong>bias</strong> of $g$ and estimator $g$ is called <strong>unbiased</strong> if</p>\[B_\vartheta(g) = 0 \quad \forall \vartheta \in \Theta.\]<p>It is reasonable (at least at the start) to put constraint on unbiasedness for $g$ and search only in</p>\[\mathcal{E}_\gamma = \lbrace g \in \mathcal{K} \mid B_\vartheta(g) = 0 \rbrace.\]<p>Surely there can be infinite number of unbiased estimators, and we not only interested in expected value of $g$, but also in how $g$ can vary from it. Variance of $g$ can be chosen as our metric for goodness. We call estimator $\tilde{g}$ <strong>uniformly minimum variance unbiased (UMVU)</strong> if</p>\[\operatorname{Var}(\tilde{g}(X)) = \mathbb{E}[(\tilde{g}(X) - \gamma(\theta))^2] = \inf_{g \in \mathcal{E}_\gamma} \operatorname{Var}(g(X)).\]<p>In general, if we choose $L(x, y) = (x - y)^2$, then</p>\[MSE_\vartheta(g) = R(\vartheta, g)=\mathbb{E}[(g(X)-\gamma(\vartheta))^2]=\operatorname{Var}_\vartheta(g(X))+B_\vartheta^2(g)\]<p>is called the <strong>mean squared error</strong>. Note that in some cases biased estimators have lower MSE because they have a smaller variance than does any unbiased estimator.</p><h3 id="chi-squared-and-t-distributions">Chi-squared and t-distributions</h3><p>Remember we talked about $\overline{x}_n$ and $\hat{s}_n^2$ being typical estimators for mean and standard deviation of normally distributed random variable? Now we are ready to talk about their properties, but firstly we have to introduce two distributions:</p><ul><li><p>Let $X_1, \dots, X_n$ be i.i.d. $\sim \mathcal{N}(0, 1)$. Then random variable $Z = \sum_{i=1}^n X_i^2$ has chi-squared distribution with $n$ degrees of freedom (notation: $Z \sim \chi_n^2$). Its density:</p>\[f_{\chi_n^2}(x) = \frac{x^{\frac{n}{2}-1} e^{-\frac{x}{2}}}{2^{\frac{n}{2}}\Gamma\big(\frac{n}{2}\big)}, \quad x &gt; 0,\]<p>where $\Gamma(\cdot)$ is a gamma function:</p>\[\Gamma(\alpha) = \int_0^\infty x^{\alpha-1} e^{-x} dx, \quad \alpha &gt; 0.\]<p>Itâ€™s easy to see that $\mathbb{E}[Z] = \sum_{i=1}^n \mathbb{E}[X_i^2] = n$ and</p>\[\operatorname{Var}(Z) = \sum_{i=1}^n \operatorname{Var}(X_i^2) = n\big(\mathbb{E}[X_1^4]) - \mathbb{E}[X_1^2]^2\big) = 2n.\]<li><p>Let $Y \sim \mathcal{N}(0, 1)$ and $Z \sim \chi_n^2$, then</p>\[T = \frac{Y}{\sqrt{Z/n}}\]<p>has t-distribution with $n$ degrees of freedom (notation $T \sim t_n$). Its density:</p>\[f_{t_n}(x) = \frac{\Gamma \big( \frac{n+1}{2} \big) } { \sqrt{n \pi} \Gamma \big( \frac{n}{2} \big) } \Big( 1 + \frac{x^2}{n} \Big)^{\frac{n+1}{2}}.\]</ul><div id="chi_t_plt"></div><script> d3.select("#chi_t_plt") .style("position", "relative"); function plt_label_path(svg, color, x, y) { svg.append("path") .attr("stroke", color) .attr("stroke-width", 4) .attr("opacity", ".8") .datum([{x: x, y: y + 2}, {x: x + 25, y: y + 2}]) .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .attr("stroke", "#000") .attr("stroke-width", 1) .datum([{x: x, y: y}, {x: x + 25, y: y}]) .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); } function chi_t_plts() { var margin = {top: 20, right: 0, bottom: 30, left: 50}, width = 700 - margin.left - margin.right, height = 200 - margin.top - margin.bottom, fig_width = 300; var chi_svg = d3.select("#chi_t_plt") .append("svg") .attr("width", width + margin.left + margin.right) .attr("height", height + margin.top + margin.bottom) .append("g") .attr("transform", "translate(" + margin.left + "," + margin.top + ")"); var margin = {top: 0, right: 0, bottom: 35, left: 350}; var t_svg = chi_svg .append("svg") .attr("width", width + margin.left + margin.right) .attr("height", height + margin.top + margin.bottom) .append("g") .attr("transform", "translate(" + margin.left + "," + margin.top + ")"); plt_label_path(chi_svg, "#EDA137", fig_width * 0.72, height * 0.13); var span_chi = d3.select("#chi_t_plt") .append("span") .text("\\(f_{\\chi_n^2}(x)\\)") .style('color', '#EDA137') .style("font-size", "17px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .attr("font-size", 20) .style("position", "absolute") .style("left", fig_width * 0.98 + "px") .style("top", height * 0.18 + "px"); plt_label_path(t_svg, "#348ABD", fig_width * 0.72, height * 0.13); var span_t = d3.select("#chi_t_plt") .append("div") .text("\\(f_{t_n}(x) \\)") .style('color', '#348ABD') .style("font-size", "17px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .attr("font-size", 20) .style("position", "absolute") .style("left", fig_width * 2.145 + "px") .style("top", height * 0.18 + "px"); d3.csv("../../../../assets/chi-t.csv", function(error, data) { if (error) throw error; var chi_x = d3.scaleLinear() .domain([-0, 40]) .range([0, fig_width]); var xAxis = chi_svg.append("g") .attr("transform", "translate(0," + height + ")") .call(d3.axisBottom(chi_x)); xAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var t_x = d3.scaleLinear() .domain([-20, 20]) .range([0, fig_width]); xAxis = t_svg.append("g") .attr("transform", "translate(0," + height + ")") .call(d3.axisBottom(t_x)); xAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var y = d3.scaleLinear() .range([height, 0]) .domain([0, 0.5]); var yAxis = chi_svg.append("g") .call(d3.axisLeft(y).ticks(5)); yAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var t_y = d3.scaleLinear() .range([height, 5]) .domain([0, 0.5]); yAxis = t_svg.append("g") .call(d3.axisLeft(t_y).ticks(5)); yAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var chi_curve = chi_svg .append('g') .append("path") .datum(data) .attr("fill", "#EDA137") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return chi_x(d.chi_x); }) .y(function(d) { return y(d["chi_5"]); }) ); var t_curve = t_svg .append('g') .append("path") .datum(data) .attr("fill", "#348ABD") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return t_x(d.t_x); }) .y(function(d) { return t_y(d["t_5"]); }) ); function updateChart(n) { n = parseInt(n); chi_curve .datum(data) .transition() .duration(1000) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return chi_x(d.chi_x); }) .y(function(d) { return y(d["chi_" + n]); }) ); t_curve .datum(data) .transition() .duration(1000) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return t_x(d.t_x); }) .y(function(d) { return t_y(d["t_" + n]); }) ); } var slider_svg = d3.select("#chi_t_plt") .append("svg") .attr("width", width + 20) .attr("height", 70) .append("g") .attr("transform", "translate(" + 25 + "," + 20 + ")"); var n_x = d3.scaleLinear() .domain([1, 12]) .range([0, width / 2]) .clamp(true); function roundN(x) { return Math.round(x - 0.5); } function createSlider(svg_, parameter_update, x, loc_x, loc_y, letter, color, init_val, round_fun) { var slider = svg_.append("g") .attr("class", "slider") .attr("transform", "translate(" + loc_x + "," + loc_y + ")"); var drag = d3.drag() .on("start.interrupt", function() { slider.interrupt(); }) .on("start drag", function() { handle.attr("cx", x(round_fun(x.invert(d3.event.x)))); parameter_update(round_fun(x.invert(d3.event.x))); }); slider.append("line") .attr("class", "track") .attr("x1", x.range()[0]) .attr("x2", x.range()[1]) .select(function() { return this.parentNode.appendChild(this.cloneNode(true)); }) .attr("class", "track-inset") .select(function() { return this.parentNode.appendChild(this.cloneNode(true)); }) .attr("class", "track-overlay") .call(drag); slider.insert("g", ".track-overlay") .attr("class", "ticks") .attr("transform", "translate(0," + 18 + ")") .selectAll("text") .data(x.ticks(6)) .enter().append("text") .attr("x", x) .attr("text-anchor", "middle") .attr("font-family", "Arvo") .text(function(d) { return d; }); var handle = slider.insert("circle", ".track-overlay") .attr("class", "handle") .attr("r", 6).attr("cx", x(init_val)); svg_ .append("text") .attr("text-anchor", "middle") .attr("y", loc_y + 3) .attr("x", loc_x - 21) .attr("font-family", "Arvo") .attr("font-size", 17) .text(letter) .style("fill", color); return handle; } createSlider(slider_svg, updateChart, n_x, 190, 0.1 * height, "n", "#696969", 5, roundN); }); } chi_t_plts(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Fig. 2. Probability density functions for $\chi_n^2$ and $t_n$-distributions. Move slider to observe how they look for different degrees of freedom $n$. Note that with large $n$ $t_n$ converges to normal distribution.</em></p><p>It can now be shown that</p>\[\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \sim \mathcal{N} \Big( \mu, \frac{\sigma^2}{n} \Big)\]<p>and</p>\[\hat{s}_n^2(X) = \frac{1}{n}\sum_{i=1}^n (X_i - \overline{X}_n)^2 \sim \frac{\sigma^2}{n} \chi^2_{n-1}.\]<p>As a consequence:</p>\[\frac{(n-1)(\overline{X}_n-\mu)}{\sqrt{n}s_n^2(X)} \sim t_{n-1}.\] <details> <summary>Proof</summary> Distribution of $\overline{X}_n$ follows from properties of Normal distribution. Let $$ Y_i = \frac{X_i - \mu}{\sigma} \sim \mathcal{N}(0, 1)$$ and $Y = (Y_1, \dots, Y_n)^T$. Choose orthogonal matrix $A$ such that its last row: $$ v^T = \Big( \frac{1}{\sqrt{n}} \dots \frac{1}{\sqrt{n}} \Big).$$ Then for $Z = AY$ the following equality holds: $$ \sum_{i=1}^n Z_i^2 = Z^TZ = Y^TA^TAY = Y^TY= \sum_{i=1}^n Y_i^2.$$ From $\operatorname{Cov}(Z)=A^TA = \mathbb{I}_n$ we have $Z \sim \mathcal{N}(0, \mathbb{I}_n).$ Also $$ \begin{aligned} \sqrt{n} \overline{X}_n &amp;= \frac{1}{\sqrt{n}} \sum_{i=1}^n (\sigma Y_i + \mu) \\ &amp; = \sigma v^T Y + \sqrt{n} \mu \\ &amp;= \sigma Z_n + \sqrt{n} \mu \end{aligned} $$ and $$ \begin{aligned} n \hat{s}_n^2(X) &amp;= \sum_{i=1}^n (X_i - \overline{X}_n)^2 = \sigma^2 \sum_{i=1}^n(Y_i - \overline{Y}_n)^2 \\ &amp; = \sigma^2 \big(\sum_{i=1}^n Y_i^2 - n \overline{Y}_n^2\big) = \sigma^2 \big(\sum_{i=1}^n Y_i^2 - \big(\frac{1}{n} \sum_{i=1}^n Y_i^2 \big)^2 \big) \\ &amp; = \sigma^2 (\sum_{i=1}^n Z_i^2 - Z_n^2) = \sigma^2 \sum_{i=1}^{n-1} Z_i^2 \sim \chi_{n-1}^2. \end{aligned} $$ Both estimators are independent as functions of $Z_n$ and $Z_1, \dots, Z_{n-1}$ respectively. </details><p>Letâ€™s check which of these estimators are unbiased. We have $\mathbb{E}[\overline{X}_n] = \mu$, therefore $\overline{X}_n$ is unbiased. On the other hand</p>\[\mathbb{E}[\hat{s}_n^2(X)] = \frac{\sigma^2}{n} (n - 1) \neq \sigma^2.\]<p><button id="sample-button-2">Sample</button> <button id="reset-button">Reset</button></p><div id="biased_viz"></div><script> d3.select("#biased_viz") .style("position", "relative"); function biasedness() { var mu = 0, sigma = 1, n = 6, xn_dots = [], sn_dots = []; var avg_dur = 1200; function randn_bm() { var u = 0, v = 0; while(u === 0) u = Math.random(); while(v === 0) v = Math.random(); return Math.sqrt( -2.0 * Math.log( u ) ) * Math.cos( 2.0 * Math.PI * v ); } var margin = {top: 20, right: 0, bottom: 5, left: 70}, width = 750 - margin.left - margin.right, height = 400 - margin.top - margin.bottom, fig_height = 250 - margin.top - margin.bottom, fig_width = 500, cfs = 100; var svg = d3.select("#biased_viz") .append("svg") .attr("width", width + margin.left + margin.right) .attr("height", height + margin.top + margin.bottom) .append("g") .attr("transform", "translate(" + margin.left + "," + margin.top + ")"); var x = d3.scaleLinear() .domain([-4, 4]) .range([cfs, fig_width + cfs]); var xAxis = svg.append("g") .attr("transform", "translate(0," + fig_height + ")") .call(d3.axisBottom(x)); xAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var y = d3.scaleLinear() .range([fig_height, 0]) .domain([0, 5]); var yAxis = svg.append("g") .attr("transform", "translate(" + cfs + ",0)") .call(d3.axisLeft(y).ticks(5)); yAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var gauss_data = [{x: -4, y: 0}]; for (var i = -4; i < 4; i += 0.01) { gauss_data.push({x: i, y: Math.exp(-0.5 * ((i - mu) / sigma) ** 2) / (sigma * Math.sqrt(2 * Math.PI)) }); } gauss_data.push({x: 4, y: 0}); var gauss_curve = svg .append('g') .append("path") .datum(gauss_data) .attr("fill", "#65AD69") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return x(d.x); }) .y(function(d) { return y(d.y); }) ); var xn_data = [{x: -3, y: 0}]; for (var i = -3; i < 3; i += 0.01) { xn_data.push({x: i, y: Math.exp(-0.5 * ((i - mu) / sigma * Math.sqrt(n)) ** 2) / (sigma * Math.sqrt(2 * Math.PI / n)) }); } xn_data.push({x: 3, y: 0}); var xn_curve = svg .append('g') .append("path") .datum(xn_data) .attr("fill", "#E86456") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return x(d.x); }) .y(function(d) { return y(-d.y - 0.5); }) ); var std_curve; d3.csv("../../../../assets/chi-t.csv", function(error, chi_data) { if (error) throw error; std_curve = svg .append('g') .append("path") .datum(chi_data) .attr("fill", "#EDA137") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return x(-(n - 1) * d["chi_" + (n-1)] - 4.5); }) .y(function(d) { return y(Math.min(4, d.chi_x / n)); }) ); } ); var labels_x = 500; plt_label_path(svg, "#65AD69", labels_x, 0); var span_sample = d3.select("#biased_viz") .append("span") .text("\\(f_X(x) \\sim \\mathcal{N}(0, 1)\\)") .style('color', '#65AD69') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", labels_x + 100 + "px") .style("top", 10 + "px"); plt_label_path(svg, "#E86456", labels_x, 25); var span_mean = d3.select("#biased_viz") .append("span") .text("\\( f_{\\overline{X}_n}(x) \\sim \\mathcal{N}(0, \\frac{1}{n}) \\)") .style('color', '#E86456') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", labels_x + 100 + "px") .style("top", 35 + "px"); plt_label_path(svg, "#EDA137", labels_x, 50); var span_std = d3.select("#biased_viz") .append("span") .text("\\( f_{\\hat{s}_n^2(X)}(x) \\sim \\frac{1}{n} \\chi_{n-1}^2 \\)") .style('color', '#EDA137') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", labels_x + 100 + "px") .style("top", 60 + "px"); var xn_avg_curve = svg .append('g') .append("path") .datum([{x: 0, y: 0}, {x: 0, y: -2}, {x: -0.5, y: -2}]) .attr("fill", "none") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("stroke-dasharray", "3 3") .attr("d", d3.line() .x(function(d) { return x(d.x); }) .y(function(d) { return y(d.y); }) ); var span_mean_avg = d3.select("#biased_viz") .append("span") .text("\\( \\mathbb{E}[\\overline{X}_n] \\)") .style('color', '#696969') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", x(-0.05) + "px") .style("top", y(-2.15) + "px"); var sn_avg_curve = svg .append('g') .append("path") .datum([{x: 1 - 1/n, y: -4}, {x: 1 - 1 / n, y: -6}, {x: 1.5 - 1/n, y: -6}]) .attr("fill", "none") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("stroke-dasharray", "3 3") .attr("d", d3.line() .x(function(d) { return x(d.y); }) .y(function(d) { return y(d.x); }) ); var span_mean_std = d3.select("#biased_viz") .append("span") .text("\\( \\mathbb{E}[\\hat{s}_n^2(X)] \\)") .style('color', '#696969') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", x(-5.3) + "px") .style("top", y(1.6 - 1 / n) + "px"); function sample() { random_samples = []; smpl_dots = []; smpl_copy_dots = []; var average = 0; var sq_curve = svg .append('g') .append("path"); for (var i = 0; i < n; i += 1) { random_samples.push(mu + sigma * randn_bm()); smpl_dots.push(svg.append('g') .selectAll("dot") .data([{x: random_samples[i], y: 5}]) .enter() .append("circle") .attr("cx", function (d) { return x(d.x); } ) .attr("cy", function (d) { return y(d.y); } ) .attr("r", 3) .style("fill", "#65AD69") .attr("stroke", "#000") .attr("stroke-width", 1)); smpl_copy_dots.push(svg.append('g') .selectAll("dot") .data([{x: random_samples[i], y: 0}]) .enter() .append("circle") .attr("cx", function (d) { return x(d.x); } ) .attr("cy", function (d) { return y(d.y); } ) .attr("r", 0) .style("fill", "#65AD69") .attr("stroke", "#000") .attr("stroke-width", 1)); smpl_dots[i].transition() .duration(avg_dur) .attr("cx", function (d) { return x(random_samples[i]); } ) .attr("cy", function (d) { return y(0); } ); average += random_samples[i]; } average /= n; for (var i = 0; i < n; i += 1) { smpl_copy_dots[i] .transition() .delay(avg_dur) .duration(0) .attr("r", 3); smpl_copy_dots[i] .transition() .delay(avg_dur) .duration(avg_dur) .style("fill", "#E86456") .attr("cx", function (d) { return x(average); } ) .attr("cy", function (d) { return y(0); } ); smpl_dots[i] .transition() .delay(3 * avg_dur) .duration(avg_dur) .attr("cx", function (d) { return x(random_samples[i]); } ) .attr("cy", function (d) { return y( (random_samples[i] - average) ** 2); } ); } var xn_dot = svg.append('g') .selectAll("dot") .data([{x: average, y: 0}]) .enter() .append("circle") .attr("cx", function (d) { return x(d.x); } ) .attr("cy", function (d) { return y(d.y); } ) .attr("r", 0) .style("fill", "#E86456") .attr("stroke", "#000") .attr("stroke-width", 1); xn_dot.transition().delay(2 * avg_dur).attr("r", 3); var sq_data = []; for (var i = -4; i <= 4; i += 0.1) { sq_data.push({x: i, y: (i - average) ** 2 }); } sq_curve .datum(sq_data) .attr("fill", "none") .attr("border", 0) .attr("opacity", ".9") .attr("stroke", "black") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return x(d.x); }) .y(function(d) { return y(d.y); }) ); var average_y = 0; for (var i = 0; i < n; i += 1) { average_y += (random_samples[i] - average) ** 2; } average_y /= n; for (var i = 0; i < n; i += 1) { smpl_dots[i] .transition() .delay(4 * avg_dur) .duration(avg_dur) .attr("cx", function (d) { return x(-4); } ) .attr("cy", function (d) { return y( (random_samples[i] - average) ** 2); } ); smpl_dots[i] .transition() .delay(5 * avg_dur) .duration(avg_dur) .style("fill", "#EDA137") .attr("cx", function (d) { return x(-4); } ) .attr("cy", function (d) { return y(average_y); } ); } var totalLength = sq_curve.node().getTotalLength(); sq_curve.attr("stroke-dasharray", totalLength + " " + totalLength) .attr("stroke-dashoffset", totalLength) .transition().duration(4 * avg_dur) .attr("stroke-dashoffset", 0); sq_curve.transition().delay(6 * avg_dur).remove(); var sn_dot = svg.append('g') .selectAll("dot") .data([{x: -4, y: average_y}]) .enter() .append("circle") .attr("cx", function (d) { return x(d.x); } ) .attr("cy", function (d) { return y(d.y); } ) .attr("r", 0) .style("fill", "#EDA137") .attr("stroke", "#000") .attr("stroke-width", 1); sn_dot.transition().delay(6 * avg_dur).attr("r", 3); for (var i = 0; i < n; i += 1) { smpl_dots[i].transition().delay(6 * avg_dur).remove(); smpl_copy_dots[i].transition().delay(2 * avg_dur).remove(); } xn_dots.push(xn_dot); sn_dots.push(sn_dot); } function updateNGauss(new_n) { n = new_n; reset(); xn_data = [{x: -3, y: 0}]; for (var i = -3; i < 3; i += 0.01) { xn_data.push({x: i, y: Math.exp(-0.5 * ((i - mu) / sigma * Math.sqrt(n)) ** 2) / (sigma * Math.sqrt(2 * Math.PI / n)) }); } xn_data.push({x: 3, y: 0}); xn_curve .datum(xn_data) .transition() .duration(1000) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return x(d.x); }) .y(function(d) { return y(-d.y - 0.5); }) ); d3.csv("../../../../assets/chi-t.csv", function(error, chi_data) { if (error) throw error; std_curve .datum(chi_data) .transition() .duration(1000) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return x(-(n - 1) * d["chi_" + (n-1)] - 4.5); }) .y(function(d) { return y(Math.min(4, d.chi_x / n)); }) ); }); sn_avg_curve .datum([{x: 1 - 1/n, y: -4}, {x: 1-1/n, y: -6}, {x: 1.5-1/n, y: -6}]) .transition() .duration(1000) .attr("d", d3.line() .x(function(d) { return x(d.y); }) .y(function(d) { return y(d.x); }) ); span_mean_std .transition() .duration(1000) .style("left", x(-5.3) + "px") .style("top", y(1.6 - 1 / n) + "px"); } var sampleButton = d3.select("#sample-button-2"); sampleButton .on("click", function() { sample(); }); function reset() { for (var i = 0; i < xn_dots.length; i += 1) { xn_dots[i].remove(); sn_dots[i].remove(); } xn_dots = []; sn_dots = []; } var resetButton = d3.select("#reset-button"); resetButton .on("click", function() { reset(); }); var ng_x = d3.scaleLinear() .domain([2, 13]) .range([0, width / 2]) .clamp(true); function roundN(x) { return Math.round(x - 0.5); } function createSlider(svg_, parameter_update, slider_x, loc_x, loc_y, letter, color, init_val, round_fun) { var slider = svg_.append("g") .attr("class", "slider") .attr("transform", "translate(" + loc_x + "," + loc_y + ")"); var drag = d3.drag() .on("start.interrupt", function() { slider.interrupt(); }) .on("start drag", function() { handle.attr("cx", slider_x(round_fun(slider_x.invert(d3.event.x)))); parameter_update(round_fun(slider_x.invert(d3.event.x))); }); slider.append("line") .attr("class", "track") .attr("x1", slider_x.range()[0]) .attr("x2", slider_x.range()[1]) .select(function() { return this.parentNode.appendChild(this.cloneNode(true)); }) .attr("class", "track-inset") .select(function() { return this.parentNode.appendChild(this.cloneNode(true)); }) .attr("class", "track-overlay") .call(drag); slider.insert("g", ".track-overlay") .attr("class", "ticks") .attr("transform", "translate(0," + 18 + ")") .selectAll("text") .data(slider_x.ticks(6)) .enter().append("text") .attr("x", slider_x) .attr("text-anchor", "middle") .attr("font-family", "Arvo") .text(function(d) { return d; }); var handle = slider.insert("circle", ".track-overlay") .attr("class", "handle") .attr("r", 6).attr("cx", slider_x(init_val)); svg_ .append("text") .attr("text-anchor", "middle") .attr("y", loc_y + 3) .attr("x", loc_x - 21) .attr("font-family", "Arvo") .attr("font-size", 17) .text(letter) .style("fill", color); return handle; } createSlider(svg, updateNGauss, ng_x, 190, 350, "n", "#696969", 6, roundN); } biasedness(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Fig. 3. Statistical experiments in estimating $\sigma$ for $X_1, \dots, X_n$ i.i.d. $\sim \mathcal{N}(0, 1)$. We see here that while $\overline{X}_n$ varies around $\mu=0$, expected value of estimator $\hat{s}_n^2(X)$ is lower than $\sigma^2 = 1$.</em></p><p>So far we figured the unbiasedness of $g(X) = \overline{X}_n$. But how can we tell if $\overline{X}_n$ is an UMVU estimator? Can we find an estimator of $\mu$ with variance lower than $\frac{\sigma^2}{n}$?</p><h3 id="efficient-estimator">Efficient estimator</h3><p>Given a set of unbiased estimators, it is not an easy task to determine which one provides the smallest variance. Luckily, we have a theorem which gives us a lower bound for an estimator variance.</p><p>Suppose we have a family of densities $f(\cdot, \vartheta)$, such that following <em>regularity conditions</em> are satisfied:</p><ul><li>Set $M_f=\lbrace x \in \mathcal{X} \mid f(x, \vartheta) &gt; 0 \rbrace$ doesnâ€™t depend on $\vartheta$<li>Partial derivative $\frac{\partial}{\partial \vartheta} \log f(x, \vartheta)$ exists $\forall x \in \mathcal{X}$.<li>The following equalities hold: <sup id="fnref:CR" role="doc-noteref"><a href="#fn:CR" class="footnote" rel="footnote">1</a></sup><ul><li>$\mathbb{E} \big[\frac{\partial}{\partial \vartheta} \log f(X, \vartheta)\big] = 0,$<li>$\mathbb{E} \big[g(X) \frac{\partial}{\partial \vartheta} \log f(X, \vartheta)\big] = \frac{\partial}{\partial \vartheta} \mathbb{E}[g(X)].$</ul><li>$0&lt;\mathbb{E} \big[\big(\frac{\partial}{\partial \vartheta} \log f(X, \vartheta)\big)^2\big]&lt;\infty$</ul><p>Letâ€™s define functions</p>\[U_\vartheta(x) = \left\{\begin{array}{ll} \frac{\partial}{\partial \vartheta} \log f(x, \vartheta), &amp; \text{if } x \in M_f, \\ 0, &amp; \text{otherwise,} \end{array} \right.\]<p>and</p>\[\mathcal{I}(f(\cdot, \vartheta))=\mathbb{E} \big[\big(\frac{\partial}{\partial \vartheta} \log f(X, \vartheta)\big)^2\big].\]<p>Under given regularity conditions we have</p>\[\mathbb{E}[U_\vartheta(X)] = \mathbb{E}\big[\frac{\partial}{\partial \vartheta} \log f(x, \vartheta)\big] = \frac{\partial}{\partial \vartheta} \mathbb{E}[\log f(x, \vartheta)] = 0\]<p>and</p>\[\operatorname{Var}(U_\vartheta(X)) = \mathbb{E}[(U_\vartheta(X))^2]=\mathcal{I}(f(\cdot, \vartheta)).\]<p>Then using Cauchy-Schwartz inequality we get</p>\[\begin{aligned} \big( \frac{\partial}{\partial \vartheta} \mathbb{E}[g(X)] \big)^2 &amp;= \big( \mathbb{E}[g(X) \cdot U_\vartheta(X)] \big)^2 \\ &amp; = \big(\operatorname{Cov}(g(X), U_\vartheta(X)) \big)^2 \\ &amp; \leq \operatorname{Var}(g(X))\cdot \operatorname{Var}(U_\vartheta(X)) \\ &amp;= \mathcal{I}(f(\cdot, \vartheta))\cdot \operatorname{Var}(g(X)). \end{aligned}\]<p>The resulting inequality:</p>\[\operatorname{Var}(g(X)) \geq \frac{\big(\frac{\partial}{\partial \vartheta} \mathbb{E}[g(X)]\big)^2}{\mathcal{I}(f(\cdot, \vartheta))} \quad \forall \vartheta \in \Theta\]<p>gives us <strong>CramÃ©râ€“Rao bound</strong>. Function $\mathcal{I}(f(\cdot, \vartheta))$ is called <strong>Fisher information</strong> for family $\mathcal{P} = \lbrace P_\vartheta \mid \vartheta \in \Theta \rbrace$. If an unbiased estimator $g$ satisfies the upper equation with equality, then it is called <strong>efficient</strong>.</p><p>This theorem gives a lower bound for the variance of an estimator for $\gamma(\vartheta) = \mathbb{E}[g(X)]$ and can be used in principle to obtain UMVU estimators. Whenever the regularity conditions are satisfied for all $g \in \mathcal{E}_\gamma$, then any efficient and unbiased estimator is UMVU.</p><p>Also, for a set of i.i.d. variables $X_1, \dots X_n$, meaning that their joint density distribution is</p>\[f(x,\vartheta) = \prod_{i=1}^n f^i(x,\vartheta),\]<p>we have</p>\[\mathcal{I}(f(\cdot, \vartheta))=n\mathcal{I}(f^1(\cdot, \vartheta)).\]<p>Letâ€™s get back to the example with $X_1, \dots, X_n$ i.i.d. $\sim \mathcal{N}(\mu, 1)$ having the density</p>\[f^1(x, \vartheta) = \frac{1}{\sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2}}.\]<p>Then</p>\[\mathcal{I}(f^1(\cdot, \mu)) = \mathbb{E} \Big[ \big( \frac{\partial}{\partial \mu} \log f^1 (X_1, \mu)\big)^2 \Big] = \mathbb{E}[(X_1 - \mu)^2] = 1.\]<p>In particular, for $X = (X_1, \dots, X_n)$ Fisher information $\mathcal{I}(f(X, \mu)) = n$ and CramÃ©râ€“Rao bound for unbiased estimator:</p>\[\operatorname{Var}(g(X)) \geq \frac{1}{n} \big( \frac{\partial}{\partial \mu} \mathbb{E}[g(X)] \big)^2 = \frac{1}{n}.\]<p>Therefore, $g(x) = \overline{x}_n$ is an UMVU estimator.</p><h3 id="multidimensional-cramÃ©rrao-inequality">Multidimensional CramÃ©râ€“Rao inequality</h3><p>Define function</p>\[G(\vartheta)=\Big( \frac{\partial}{\partial \vartheta_j} \mathbb{E}[g_i(X)] \Big)_{i,j} \in \mathbb{R}^{k \times d}.\]<p>Then with multidimensional Cauchy-Shwartz inequality one can prove that under similar regularity conditions we have:</p>\[\operatorname{Cov}(g(X)) \geq G(\vartheta) \mathcal{I}^{-1}(f(\cdot, \vartheta))G^T(\vartheta) \in \mathbb{R}^{k \times k},\]<p>in the sense of LÃ¶wner ordering<sup id="fnref:MCR" role="doc-noteref"><a href="#fn:MCR" class="footnote" rel="footnote">2</a></sup>, where</p>\[\mathcal{I}(f(\cdot, \vartheta))=\Big( \mathbb{E}\Big[\frac{\partial}{\partial \vartheta_i} \log f(X, \vartheta) \cdot \frac{\partial}{\partial \vartheta_j} \log f(X, \vartheta) \Big] \Big)_{i,j=1}^d \in \mathbb{R}^{d \times d}.\]<p>For an example with $X_1, \dots X_n$ i.i.d. $\sim \mathcal{N}(\mu, \sigma^2)$ with density</p>\[f^1(x,\vartheta)=\frac{1}{\sqrt{2\pi \sigma^2}} \exp \Big(-\frac{(x-\mu)^2}{2\sigma^2}\Big)\]<p>we have</p>\[U_\vartheta = \Big(\frac{\partial}{\partial \mu} \log f^1(X_1,\vartheta), \frac{\partial}{\partial \sigma^2} \log f^1(X_1,\vartheta)\Big)^T = \begin{pmatrix} (X_1-\mu)/\sigma^2 \\ -\frac{1}{2\sigma^2}+\frac{1}{\sigma^4}(X_1-\mu)^2 \end{pmatrix}.\]<p>Fisher information then</p>\[\mathcal{I}(f^1(\cdot, \vartheta))=\mathbb{E}[U_\vartheta U_\vartheta^T]= \begin{pmatrix} \sigma^{-2} &amp; 0 \\ 0 &amp; \frac{1}{2}\sigma^{-4} \end{pmatrix} = \frac{1}{n}\mathcal{I}(f(\cdot, \vartheta)).\]<p>If $g(X)$ is an unbiased estimator, then $G(\vartheta)$ is identity matrix and CramÃ©râ€“Rao bound then</p>\[\begin{aligned} \operatorname{Cov}_\vartheta(g(X)) &amp; \geq G(\vartheta) \ \mathcal{I}^{-1} (f(\cdot, \vartheta)) \ G^T(\vartheta) \\ &amp;= \mathcal{I}^{-1}(f(\cdot, \vartheta)) = \begin{pmatrix} \frac{\sigma^{2}}{n} &amp; 0 \\ 0 &amp; \frac{2\sigma^{4}}{n} \end{pmatrix}. \end{aligned}\]<p>In particular for an unbiased estimator</p>\[\widetilde{g}(X)=\Big(\overline{X}_n, \frac{1}{n-1} \sum_{i=1}^n(X_j-\overline{X}_n)^2 \Big)^T\]<p>the following inequality holds</p>\[\operatorname{Cov}_\vartheta(\widetilde{g}(X)) = \begin{pmatrix} \frac{\sigma^{2}}{n} &amp; 0 \\ 0 &amp; \frac{2\sigma^{4}}{n-1} \end{pmatrix} \geq \mathcal{I}(f(\cdot, \vartheta)),\]<p>therefore $\widetilde{g}$ is not efficient.</p><h3 id="exponential-family">Exponential family</h3><p>In the previous examples, we consider without proof the fulfillment of all regularity conditions of the CramÃ©râ€“Rao inequality. Next, we will discuss a family of distributions for which the CramÃ©râ€“Rao inequality turns into an equality.</p><p>Proposition: let $P_\vartheta$ be distribution with density</p>\[f(x, \vartheta) = c(\vartheta) h(x) \exp(\vartheta T(x)) \quad \forall \vartheta \in \Theta.\]<p>Then equality in CramÃ©râ€“Rao theorem holds for $g(x) = T(x)$.</p><details> <summary>Proof</summary> First let us note that $\int_{\mathcal{X}}f(x)\mu(dx) = 1$ for all $\vartheta \in \Theta$, hence $$ c(\vartheta)=\Big( \int_{\mathcal{X}} h(x)\exp (\vartheta T(x) ) dx \Big)^{-1} $$ and $$ \begin{aligned} 0 &amp; = \frac{\partial}{\partial \vartheta} \int_{\mathcal{X}} c(\vartheta) h(x) \exp ( \vartheta T(x) ) dx \\ &amp; = \int_{\mathcal{X}} (c'(\vartheta)+c(\vartheta)T(x)) h(x) \exp ( \vartheta T(x) ) dx. \end{aligned} $$ Using these two equations we get $$ \begin{aligned} \mathbb{E}[T(X)] &amp; = c(\vartheta) \int_{\mathcal{X}} h(x) T(x) \exp ( \vartheta T(x)) dx \\ &amp; = -c'(\vartheta) \int_{\mathcal{X}}h(x) \exp ( \vartheta T(x) ) dx \\ &amp; = -\frac{c'(\vartheta)}{c(\vartheta)}=(-\log c(\vartheta))'. \end{aligned} $$ Fisher information: $$ \mathcal{I}(f(\cdot, \vartheta)) = \mathbb{E}\Big[\Big( \frac{\partial}{\partial \vartheta} \log f(X, \vartheta) \Big)^2\Big]=\mathbb{E}[(T(X)+(\log c(\vartheta))')^2]=\operatorname{Var}(T(X)). $$ Also $$ \begin{aligned} \frac{\partial}{\partial \vartheta} \mathbb{E}[T(X)] &amp; =\int_{\mathcal{X}} c'(\vartheta) h(x) T(x) \exp ( \vartheta T(x) ) dx + \int_{\mathcal{X}} c(\vartheta) h(x) T^2(x) \exp ( \vartheta T(x) ) dx \\ &amp; = \frac{c'(\vartheta)}{c(\vartheta)} \int_{\mathcal{X}} c(\vartheta) h(x) T(x) \exp ( \vartheta T(x) ) dx + \mathbb{E}[(T(X))^2] \\ &amp; = \mathbb{E}[(T(X))^2] - (\mathbb{E}[T(X)])^2. \end{aligned} $$ Therefore, $$ \frac{\Big(\frac{\partial}{\partial\vartheta}\mathbb{E}[T(X)] \Big)^2}{\mathcal{I}(f(\cdot, \vartheta))}= \operatorname{Var}(T(X)). $$ </details><p>Formally, family $\mathcal{P} = \lbrace P_\vartheta \mid \vartheta \in \Theta \rbrace $ is called an <strong>exponential family</strong> if there exist mappings $c, Q_1, \dots Q_k: \Theta \rightarrow \mathbb{R}$ and $h, T_1, \dots T_k: \mathcal{X} \rightarrow \mathbb{R}$ such that</p>\[f(x,\vartheta) = c(\vartheta) h(x) \exp \Big( \sum_{j=1}^k Q_j(\vartheta) T_j(x) \Big).\]<p>$\mathcal{P}$ is called <strong>$k$-parametric exponential family</strong> if functions $1, Q_1, \dots Q_k$ and $1, T_1, \dots T_k$ are linearly independent. Then we have equality to CramÃ©râ€“Rao bound for $g = (T_1, \dots T_k)^T$.</p><p>Here are some examples:</p><ul><li><p>If $X \sim \operatorname{Bin}(n, \vartheta)$, then</p>\[\begin{aligned} f(x, \vartheta) &amp;= \binom n x \vartheta^x (1-\vartheta)^{n-x} \\ &amp;= (1-\vartheta)^n \binom n x \exp \Big(x \log \frac{\vartheta}{1-\vartheta} \Big). \end{aligned}\]<p>Here $c(\vartheta) = (1-\vartheta)^n$, $h(x) = \binom n x$, $T_1(x) = x$ and $Q_1(\vartheta) = \log \frac{\vartheta}{1-\vartheta}$.</p><li><p>If $X \sim \mathcal{N}(\mu, \sigma^2)$, then $\vartheta = (\mu, \sigma^2)^T$ and</p>\[\begin{aligned} f(x, \vartheta) &amp;= \frac{1}{\sqrt{2\pi\sigma^2}} \exp\Big( \frac{(x-\mu)^2}{2\sigma^2} \Big) \\ &amp;= \frac{1}{\sqrt{2\pi\sigma^2}} \exp \Big( -\frac{\mu^2}{2\sigma^2} \Big) \exp\Big( -\frac{x^2}{2\sigma^2} + \frac{\mu x}{\sigma^2} \Big), \end{aligned}\]<p>where $c(\vartheta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \big( -\frac{\mu^2}{2\sigma^2} \big) $, $Q_1(\vartheta) = -\frac{1}{2\sigma^2}$, $Q_2(\vartheta) = \frac{\mu}{\sigma^2}$, $T_1(x)=x^2$ and $T_2(x)=x$.</p><li><p>If $X \sim \operatorname{Poisson}(\lambda)$, then</p></ul>\[f(x, \lambda) = \frac{\lambda^x e^{-\lambda}}{x!} = e^{-\lambda} \frac{1}{x!} \exp \big(x \log \lambda \big).\]<p>Denoting $Q(\vartheta) = (Q_1(\vartheta), \dots, Q_k(\vartheta))^T$ we get transformed parametric space $ \Theta^* = Q(\Theta) $, which we call <strong>natural parametric space</strong>. In examples above</p><ul><li>$X \sim \operatorname{Bin}(n, \vartheta)$: $\Theta^* = \lbrace \log \frac{\vartheta}{1-\vartheta} \mid \vartheta \in (0, 1) \rbrace = \mathbb{R}$.<li>$X \sim \mathcal{N}(\mu, \sigma^2)$: $\Theta^* = \big\lbrace \big( \frac{\mu}{\sigma^2}, -\frac{1}{\sigma^2} \big) \mid \mu \in \mathbb{R}, \sigma^2 \in \mathbb{R}^+ \big\rbrace = \mathbb{R} \times \mathbb{R}^-.$<li>$X \sim \operatorname{Poisson}(\lambda)$: $\Theta^* = \lbrace \log \lambda \mid \lambda \in \mathbb{R}^+ \rbrace = \mathbb{R}$.</ul><p>It must be noted that for an exponential family $\mathcal{P}$ estimator $T(X) = (T_1(X), \dots T_k(X))$ is UMVU for $\mathbb{E}[T(X)]$. For example, if $X_1, \dots X_n$ i.i.d. $\sim \mathcal{N}(\mu, \sigma^2)$ with joint density</p>\[f(x,\vartheta) = c(\vartheta) \exp \Big( -\frac{n}{2\sigma^2}\Big( \frac{1}{n} \sum_{i=1}^n x_i^2 \Big) + \frac{n\mu}{\sigma^2}\Big( \frac{1}{n}x_i \Big) \Big),\]<p>then estimator</p>\[T(X) = \Big( \frac{1}{n} \sum_{i=1}^n X_i, \frac{1}{n} \sum_{i=1}^n X_i^2 \Big)\]<p>is efficient for $(\mu, \mu^2 + \sigma^2)^T$.</p><h3 id="common-estimation-methods">Common estimation methods</h3><p>If distribution doesnâ€™t belong to exponential family, then for such case there exist two classical estimation methods:</p><ul><li><p><strong>Method of moments</strong>. Let $X_1, \dots X_n$ i.i.d. $\sim P_\vartheta$ and</p>\[\gamma(\vartheta) = f(m_1, \dots, m_k),\]<p>where $m_j = \mathbb{E}[X_1^j]$. Then <strong>estimation by method of moments</strong> will be</p>\[\hat{\gamma} (X) = f(\hat{m}_1, \dots, \hat{m}_k),\]<p>where $m_j = \frac{1}{n}\sum_{i=1}^nX_i^j$.</p><li><p><strong>Maximum likelihood method</strong>. Say $\gamma(\vartheta) = \vartheta \in \mathbb{R}^k$. Then $\hat{\vartheta}(x)$ is a <strong>maximum likelihood estimator</strong> if</p>\[f(x, \hat{\vartheta}) = \sup_{\vartheta \in \Theta} f(x, \vartheta).\]</ul><p>Again in example $X_1, \dots X_n$ i.i.d. $\sim \mathcal {N}(\mu, \sigma^2)$ an estimator for $\vartheta = (\mu, \sigma^2)^T = (m_1, m_2 - m_1^2)^T$ by method of moments will be</p>\[\hat{\gamma}(\vartheta)=(\hat{m}_1, \hat{m}_2-\hat{m}_1^2)^T=(\overline{x}_n, \hat{s}_n^2)^T.\]<p>Itâ€™s easy to show that this estimator coincides with the estimation obtained by the maximum likelihood method.</p><p>Letâ€™s take another example, $X_1, \dots X_n$ i.i.d. $\sim \mathcal{U}(0, \vartheta)$, where estimated parameter $\vartheta &gt; 0$. One can show that estimator</p>\[g_{ML}(X) = X_{(n)} = \max \lbrace X_1, \dots X_n \rbrace\]<p>is a maximum-likelihood estimator. On the other hand,</p>\[g_{MM}(X) = 2 \overline{X}_n\]<p>is an estimator by method of moments. Also, maximum-likelihood estimator follows scaled Beta-distribution, $g_{ML}(X) \sim \vartheta B(n, 1)$, and therefore it is biased:</p>\[\mathbb{E}[g_{ML}(X)] = \vartheta\frac{n}{n+1}.\]<p>UMVU estimator is $g(X) = X_{(n)} (1 + \frac{1}{n})$, and its variance:</p>\[\operatorname{Var}[g(X)] = \vartheta^2\frac{1}{n(n+2)} &lt; \frac{\vartheta^2}{n}\]<p>However, the CramÃ©r-Rao lower bound is $\frac{\vartheta^2}{n}$. This shows importance of regularity conditions for CramÃ©r-Rao theorem. Here, invariance of $M_f$ is not satisified and CramÃ©r-Rao inequality doesnâ€™t hold.</p><hr /><div class="footnotes" role="doc-endnotes"><ol><li id="fn:CR" role="doc-endnote"><p>Letâ€™s rewrite these equations in equivalent forms:</p>\[\int_\mathcal{X} \frac{\partial}{\partial \vartheta} \log f(x, \vartheta) f(x, \vartheta) d x = \int_\mathcal{X} \frac{\partial}{\partial \vartheta} f(x, \vartheta) d x=\frac{\partial}{\partial \vartheta}\int_\mathcal{X} f(x, \vartheta) d x =0,\] \[\int_\mathcal{X} g(x) \frac{\partial}{\partial \vartheta} f(x, \vartheta) dx = \frac{\partial}{\partial \vartheta} \int_\mathcal{X} g(x) f(x,\vartheta) dx.\]<p>In both cases it means that we can interchange differentiation and integration.Â <a href="#fnref:CR" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:MCR" role="doc-endnote"><p>For symmetric matrices $A$ and $B$ we say that</p>\[A \geq 0 \Longleftrightarrow A \text{ is positive semi-definite},\] \[A \geq B \Longleftrightarrow A - B \geq 0.\]<p><a href="#fnref:MCR" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/statistics/'>Statistics</a>, <a href='/categories/visual-guide/'>Visual Guide</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/statistics/" class="post-tag no-text-decoration" >statistics</a> <a href="/tags/parameter-estimation/" class="post-tag no-text-decoration" >parameter estimation</a> <a href="/tags/frequentist-inference/" class="post-tag no-text-decoration" >frequentist inference</a> <a href="/tags/exponential-family/" class="post-tag no-text-decoration" >exponential family</a> <a href="/tags/cramer-rao-inequality/" class="post-tag no-text-decoration" >cramer-rao inequality</a> <a href="/tags/fisher-information/" class="post-tag no-text-decoration" >fisher information</a> <a href="/tags/maximum-likelihood-estimator/" class="post-tag no-text-decoration" >maximum-likelihood estimator</a> <a href="/tags/method-of-moments/" class="post-tag no-text-decoration" >method of moments</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Visual Guide to Statistics. Part I: Basics of Point Estimation - AstraBlog&url=https://astralord.github.io/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Visual Guide to Statistics. Part I: Basics of Point Estimation - AstraBlog&u=https://astralord.github.io/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Visual Guide to Statistics. Part I: Basics of Point Estimation - AstraBlog&url=https://astralord.github.io/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://astralord.github.io/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/">Building Aligned Intelligence System. Part I: Creating GPT Assistant</a><li><a href="/posts/exploring-parallel-strategies-with-jax/">Exploring Parallel Strategies with Jax</a><li><a href="/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/">Building Aligned Intelligence System. Part II. Improving Large Language Models</a><li><a href="/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/">Visual Guide to Statistics. Part I: Basics of Point Estimation</a><li><a href="/posts/visual-guide-to-statistics-part-ii-bayesian-statistics/">Visual Guide to Statistics. Part II: Bayesian Statistics</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/parameter-estimation/">parameter estimation</a> <a class="post-tag" href="/tags/asymptotic-efficiency/">asymptotic efficiency</a> <a class="post-tag" href="/tags/bartlett-test/">bartlett test</a> <a class="post-tag" href="/tags/bayes-estimator/">bayes estimator</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/central-limit-theorem/">central limit theorem</a> <a class="post-tag" href="/tags/chain-of-thought/">chain-of-thought</a> <a class="post-tag" href="/tags/chatgpt/">chatgpt</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/visual-guide-to-statistics-part-ii-bayesian-statistics/"><div class="card-body"> <span class="timeago small" >Mar 21, 2022<i class="unloaded">2022-03-21T06:11:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Visual Guide to Statistics. Part II: Bayesian Statistics</h3><div class="text-muted small"><p> Part II introduces different approach to parameters estimation called Bayesian statistics. Basic definitions We noted in the previous part that it is extremely unlikely to get a uniformly bes...</p></div></div></a></div><div class="card"> <a href="/posts/visual-guide-to-statistics-part-iii-asymptotic-properties-of-estimators/"><div class="card-body"> <span class="timeago small" >Apr 12, 2022<i class="unloaded">2022-04-12T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Visual Guide to Statistics. Part III: Asymptotic Properties of Estimators</h3><div class="text-muted small"><p> A minimal condition for a good estimator is that it is getting closer to estimated parameter with growing size of sample vector. In this post we will focus on asymptotic properties of estimators...</p></div></div></a></div><div class="card"> <a href="/posts/visual-guide-to-statistics-part-iv-foundations-of-testing/"><div class="card-body"> <span class="timeago small" >May 1, 2022<i class="unloaded">2022-05-01T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Visual Guide to Statistics. Part IV: Foundations of Testing</h3><div class="text-muted small"><p> This is the fourth and the last part of a â€˜Visual Guide to Statisticsâ€™ cycle. All the previous parts and other topics related to statistics could be found here. In this post we will test hypoth...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <span class="btn btn-outline-primary disabled" prompt="Older"><p>-</p></span> <a href="/posts/visual-guide-to-statistics-part-ii-bayesian-statistics/" class="btn btn-outline-primary" prompt="Newer"><p>Visual Guide to Statistics. Part II: Bayesian Statistics</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> Â© 2024 <a href="https://github.com/astralord">Aleksandr Samarin</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/parameter-estimation/">parameter estimation</a> <a class="post-tag" href="/tags/asymptotic-efficiency/">asymptotic efficiency</a> <a class="post-tag" href="/tags/bartlett-test/">bartlett test</a> <a class="post-tag" href="/tags/bayes-estimator/">bayes estimator</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/central-limit-theorem/">central limit theorem</a> <a class="post-tag" href="/tags/chain-of-thought/">chain of thought</a> <a class="post-tag" href="/tags/chatgpt/">chatgpt</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://astralord.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
