<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Power of Diffusion Models" /><meta property="og:locale" content="en" /><meta name="description" content="In 2022, insanely beautiful and original images created with generative neural networks are taking the internet by storm. This post focuses on the theory behind diffusion models that underpin the core ideas of the latest generative AI. Brace yourself, this post is math-heavy and there are a lot of formulas ahead." /><meta property="og:description" content="In 2022, insanely beautiful and original images created with generative neural networks are taking the internet by storm. This post focuses on the theory behind diffusion models that underpin the core ideas of the latest generative AI. Brace yourself, this post is math-heavy and there are a lot of formulas ahead." /><link rel="canonical" href="https://astralord.github.io/posts/power-of-diffusion-models/" /><meta property="og:url" content="https://astralord.github.io/posts/power-of-diffusion-models/" /><meta property="og:site_name" content="AstraBlog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-09-25T06:00:00+03:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Power of Diffusion Models" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-07-03T23:09:13+03:00","datePublished":"2022-09-25T06:00:00+03:00","description":"In 2022, insanely beautiful and original images created with generative neural networks are taking the internet by storm. This post focuses on the theory behind diffusion models that underpin the core ideas of the latest generative AI. Brace yourself, this post is math-heavy and there are a lot of formulas ahead.","headline":"Power of Diffusion Models","mainEntityOfPage":{"@type":"WebPage","@id":"https://astralord.github.io/posts/power-of-diffusion-models/"},"url":"https://astralord.github.io/posts/power-of-diffusion-models/"}</script><title>Power of Diffusion Models | AstraBlog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="AstraBlog"><meta name="application-name" content="AstraBlog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/marvel-icon.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">AstraBlog</a></div><div class="site-subtitle font-italic">A place to learn and share knowledge about AI-related things</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/astralord" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['samarin_ad','mail.ru'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/aleksandr-samarin-b8a35496" aria-label="linkedin" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://t.me/astrlrd" aria-label="telegram" class="order-6" target="_blank" rel="noopener"> <i class="fab fa-telegram"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Power of Diffusion Models</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Power of Diffusion Models</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Aleksandr Samarin </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sun, Sep 25, 2022, 6:00 AM +0300" >Sep 25, 2022<i class="unloaded">2022-09-25T06:00:00+03:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Jul 3, 2023, 11:09 PM +0300" >Jul 3, 2023<i class="unloaded">2023-07-03T23:09:13+03:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5853 words">32 min read</span></div></div><div class="post-content"><blockquote><p>In 2022, insanely beautiful and original images created with generative neural networks are taking the internet by storm. This post focuses on the theory behind diffusion models that underpin the core ideas of the latest generative AI. Brace yourself, this post is math-heavy and there are a lot of formulas ahead.</p></blockquote><p><img data-proofer-ignore data-src="/assets/img/space-opera.png" alt="Space Opera" /> <em>In 2022 ‘Théâtre D’opéra Spatial’, an artwork by Jason M. Allen with help of Midjourney took 1st place in the digital art competition at Colorado State Fair. This event sparked a backslash from artists, claiming that creative jobs are now not safe from machines and in danger of becoming obsolete. Here I chose this picture emerging from noise as a symbol of an upcoming age of art, created by artificial intelligence.</em></p><p>Before we start, I want to mention all the sources which were helpful to write this post:</p><ul><li>Papers:<ul><li><a href="https://arxiv.org/pdf/2006.11239.pdf">Denoising Diffusion Probabilistic Models</a><li><a href="https://arxiv.org/pdf/2102.09672.pdf">Improved Denoising Diffusion Probabilistic Models</a><li><a href="https://arxiv.org/pdf/2010.02502.pdf">Denoising Diffusion Implicit Models</a><li><a href="https://arxiv.org/pdf/2105.05233.pdf">Diffusion Models Beat GANs on Image Synthesis</a><li><a href="https://arxiv.org/pdf/1907.05600.pdf">Generative Modeling by Estimating Gradients of the Data Distribution</a><li><a href="https://arxiv.org/pdf/2207.12598.pdf">Classifier-free diffusion guidance</a><li><a href="https://arxiv.org/pdf/2112.10741.pdf">GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models</a><li><a href="https://arxiv.org/pdf/2204.06125.pdf">Hierarchical Text-Conditional Image Generation with CLIP Latents</a><li><a href="https://arxiv.org/pdf/2205.11487.pdf">Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</a><li><a href="https://arxiv.org/pdf/2112.10752.pdf">High-Resolution Image Synthesis with Latent Diffusion Models</a></ul><li>Posts:<ul><li><a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models">What are Diffusion Models?</a><li><a href="https://yang-song.net/blog/2021/score/">Generative Modeling by Estimating Gradients of the Data Distribution</a><li><a href="https://drive.google.com/file/d/1DYHDbt1tSl9oqm3O333biRYzSCOtdtmn/view">Denoising Diffusion-based Generative Modeling: Foundations and Applications</a><li><a href="https://maciejdomagala.github.io/generative_models/2022/06/06/The-recent-rise-of-diffusion-based-models.html">The recent rise of diffusion-based models</a></ul></ul><h3 id="denoising-diffusion-probabilistic-models-ddpm">Denoising diffusion probabilistic models (DDPM)</h3><p>To define a <strong>diffusion probabilistic model</strong> (usually called a <strong>“diffusion model”</strong> for brevity), we first define a Markov chain, which starts from initial datapoint $\mathbf{x}_0$, then gradually adds noise to the data, creating sequence $\mathbf{x}_0, \mathbf{x}_1, \dots, \mathbf{x}_T$, until signal is destroyed.</p><script src="https://d3js.org/d3.v4.min.js"></script><link href="https://fonts.googleapis.com/css?family=Arvo" rel="stylesheet" /><div id="grph_chain" class="svg-container" align="center"></div><script> function draw_triangle(svg, x, y, rotate=0) { var triangleSize = 25; var triangle = d3.symbol() .type(d3.symbolTriangle) .size(triangleSize); svg.append("path") .attr("d", triangle) .attr("stroke", "black") .attr("fill", "gray") .attr("transform", function(d) { return "translate(" + x + "," + y + ") rotate(" + rotate + ")"; }); } function graph_chain() { var svg = d3.select("#grph_chain") .append("svg") .attr("width", 600) .attr("height", 130); svg.append('circle') .attr('cx', 50) .attr('cy', 50) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.85) .attr('fill', '#348ABD'); svg.append('text') .attr('x', 42) .attr('y', 55) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 55) .attr('y', 60) .text("0") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append("path") .attr("stroke", "black") .datum([{x: 25, y: 110}, {x: 27, y: 100}, {x: 29, y: 90}, {x: 30, y: 90}, {x: 32, y: 90}, {x: 33, y: 100}, {x: 36, y: 90}, {x: 38, y: 80}, {x: 42, y: 70}, {x: 45, y: 100}, {x: 50, y: 100}, {x: 55, y: 110}, {x: 56, y: 100}, {x: 57, y: 90}, {x: 58, y: 85}, {x: 60, y: 80}, {x: 65, y: 100}, {x: 68, y: 90}, {x: 72, y: 85}, {x: 75, y: 110},]) .attr("fill", "#348ABD") .attr("opacity", "0.8") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('line') .attr('x1', 20) .attr('y1', 110) .attr('x2', 80) .attr('y2', 110) .style("stroke-width", 1) .attr('stroke', 'black'); svg.append('text') .attr('x', 35) .attr('y', 125) .text("q(x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 61) .attr('y', 125) .text(")") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 56) .attr('y', 128) .text("0") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('line') .attr('x1', 70) .attr('y1', 50) .attr('x2', 110) .attr('y2', 50) .style("stroke-width", 1) .attr('stroke', 'black'); draw_triangle(svg, 110, 50, 90); svg.append('circle') .attr('cx', 130) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 150) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 170) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('line') .attr('x1', 185) .attr('y1', 50) .attr('x2', 225) .attr('y2', 50) .style("stroke-width", 1) .attr("opacity", 0.95) .attr('stroke', 'black'); draw_triangle(svg, 225, 50, 90); svg.append('circle') .attr('cx', 250) .attr('cy', 50) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.75) .attr('fill', '#5286A5'); svg.append("path") .attr("stroke", "black") .datum([{x: 225, y: 110}, {x: 227, y: 110}, {x: 229, y: 90}, {x: 230, y: 100}, {x: 232, y: 90}, {x: 233, y: 100}, {x: 240, y: 95}, {x: 242, y: 75}, {x: 245, y: 80}, {x: 250, y: 85}, {x: 257, y: 90}, {x: 258, y: 85}, {x: 260, y: 88}, {x: 265, y: 100}, {x: 268, y: 90}, {x: 270, y: 109}, {x: 272, y: 85}, {x: 275, y: 110}]) .attr("fill", "#5286A5") .attr("opacity", "0.8") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('line') .attr('x1', 220) .attr('y1', 110) .attr('x2', 280) .attr('y2', 110) .style("stroke-width", 1) .attr('stroke', 'black'); svg.append('text') .attr('x', 240) .attr('y', 55) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 253) .attr('y', 60) .text("t-1") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 234) .attr('y', 125) .text("q(x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 266) .attr('y', 125) .text(")") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 255) .attr('y', 128) .text("t-1") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('line') .attr('x1', 270) .attr('y1', 50) .attr('x2', 330) .attr('y2', 50) .style("stroke-width", 1) .attr("opacity", 0.95) .attr('stroke', 'black'); draw_triangle(svg, 325, 50, 90); svg.append('circle') .attr('cx', 350) .attr('cy', 50) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.7) .attr('fill', '#628498'); svg.append("path") .attr("stroke", "black") .datum([{x: 325, y: 110}, {x: 327, y: 110}, {x: 329, y: 100}, {x: 330, y: 100}, {x: 332, y: 90}, {x: 333, y: 100}, {x: 340, y: 95}, {x: 342, y: 70}, {x: 350, y: 85}, {x: 357, y: 90}, {x: 358, y: 85}, {x: 360, y: 88}, {x: 365, y: 100}, {x: 368, y: 90}, {x: 370, y: 109}, {x: 375, y: 110}]) .attr("fill", "#628498") .attr("opacity", "0.8") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('line') .attr('x1', 320) .attr('y1', 110) .attr('x2', 380) .attr('y2', 110) .style("stroke-width", 1) .attr('stroke', 'black'); svg.append('text') .attr('x', 342) .attr('y', 55) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 355) .attr('y', 60) .text("t") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 335) .attr('y', 125) .text("q(x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 360) .attr('y', 125) .text(")") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 356) .attr('y', 128) .text("t") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('line') .attr('x1', 370) .attr('y1', 50) .attr('x2', 410) .attr('y2', 50) .style("stroke-width", 1) .attr("opacity", 0.95) .attr('stroke', 'black'); draw_triangle(svg, 405, 50, 90); svg.append('circle') .attr('cx', 430) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 450) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 470) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('line') .attr('x1', 490) .attr('y1', 50) .attr('x2', 530) .attr('y2', 50) .style("stroke-width", 1) .attr("opacity", 0.95) .attr('stroke', 'black'); draw_triangle(svg, 525, 50, 90); svg.append('circle') .attr('cx', 550) .attr('cy', 50) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.5) .attr('fill', '#808080'); svg.append("path") .attr("stroke", "black") .datum([{x: 525, y: 110}, {x: 530, y: 109}, {x: 535, y: 103}, {x: 540, y: 92}, {x: 550, y: 70}, {x: 560, y: 92}, {x: 565, y: 103}, {x: 570, y: 109}, {x: 575, y: 110}]) .attr("fill", "#808080") .attr("opacity", "0.8") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('line') .attr('x1', 520) .attr('y1', 110) .attr('x2', 580) .attr('y2', 110) .style("stroke-width", 1) .attr('stroke', 'black'); svg.append('text') .attr('x', 542) .attr('y', 55) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 555) .attr('y', 60) .text("T") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 535) .attr('y', 125) .text("q(x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 561) .attr('y', 125) .text(")") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 556) .attr('y', 128) .text("T") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 33) .attr('y', 15) .text("Data") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 529) .attr('y', 15) .text("Noise") .style("font-size", "14px") .attr("font-family", "Arvo"); } graph_chain(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Forward diffusion process. Given a data point sampled from a real data distribution $\mathbf{x}_0 \sim q(x_0)$, we produce noisy latents $\mathbf{x}_1 \rightarrow \cdots \rightarrow \mathbf{x}_T$ by adding small amount of Gaussian noise at each timestep $t$. The latent $\mathbf{x}_t$ gradually loses its recognizable features as the step $t$ becomes larger and eventually with $T \rightarrow \infty$, $\mathbf{x}_T$ is nearly an isotropic Gaussian distribution.</em></p><p>The step sizes are controlled by a variance schedule $\beta_t \in (0, 1)$:</p>\[\mathbf{x}_t = \sqrt{1-\beta_t} \mathbf{x}_{t-1} + \sqrt{\beta_t} \epsilon_{t-1}, \quad \epsilon_{t-1} \sim \mathcal{N}(0, \mathbf{I})\]<p>Conditional distribution for the forward process is</p>\[q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) = \mathcal{N}(\sqrt{1-\beta_t} \mathbf{x}_t, \beta_t \mathbf{I}), \quad q(\mathbf{x}_{1:T} \vert \mathbf{x}_0) = \prod_{t=1}^T q(\mathbf{x}_t \vert \mathbf{x}_{t-1}).\]<p>Recall that for two Gaussian random variables $\epsilon_1 \sim \mathcal{N}(0, \sigma^2_1\mathbf{I})$ and $\epsilon_2 \sim \mathcal{N}(0, \sigma^2_2 \mathbf{I})$ we have</p>\[\epsilon_1 + \epsilon_2 \sim \mathcal{N}(0, (\sigma_1^2 + \sigma_2^2)\mathbf{I}).\]<p>Therefore for each latent $\mathbf{x}_t$ at arbitrary step $t$ we can sample it in a closed form.</p><p>Using the notation $\alpha_t := 1 - \beta_t$ and $\overline\alpha_t := \prod_{s=1}^t \alpha_s$ we get</p>\[\begin{aligned} \mathbf{x}_t &amp; = {\sqrt{\alpha_t} \mathbf{x}_{t-1}} + { \sqrt{1-\alpha_t} \epsilon_{t-1}} \\ &amp; = {\sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2} + \sqrt{\alpha_t (1-\alpha_{t-1})} \epsilon_{t-2}} + { \sqrt{1-\alpha_t} \epsilon_{t-1}} \\ &amp; = {\sqrt{\alpha_t \alpha_{t-1}} \mathbf{x}_{t-2}} + \sqrt{1-\alpha_t \alpha_{t-1}} \bar\epsilon_{t-2} \qquad \color{Salmon}{\leftarrow \bar\epsilon_{t-2} \sim \mathcal{N}(0, \mathbf{I})} \\ &amp; = \cdots \\ &amp;= \sqrt{\overline\alpha_t} \mathbf{x}_0 + \sqrt{1-\overline\alpha_t} \epsilon \end{aligned}\]<p>and</p>\[q(\mathbf{x}_t \vert \mathbf{x}_{0}) \sim \mathcal{N}\big(\sqrt{\overline\alpha_t}\mathbf{x}_{0}, \sqrt{1-\overline\alpha_t} \mathbf{I}\big).\]<p>If we were able to go in the opposite direction and sample from reverse process distribution $q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$, we could recreate samples from a true distribution $q(\mathbf{x}_0)$ with only a Gaussian noise input $\mathbf{x}_T$. In general reverse process distribution is intractable, since its calculation would require marginalization over the entire data distribution.</p><p>The core idea of diffusion algorithm is to train a model $p_\theta$ to approximate $q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$ in order to run the reverse diffusion process:</p>\[p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_{t}) = \mathcal{N}(\mu_\theta(\mathbf{x}_t, t), \Sigma_\theta(\mathbf{x}_t, t)),\]<p>where $\mu_\theta$ and $\Sigma_\theta$ are trainable networks. Although, for simplicity we can decide for</p>\[\Sigma_\theta(\mathbf{x}_t, t) = \sigma_t^2 \mathbf{I}.\]<div id="grph_rvrs_chain" class="svg-container" align="center"></div><script> function draw_uroboros(svg, x, y=35) { svg.append("path") .attr("stroke", "black") .datum([{x: x + 33, y: y}, {x: x + 20, y: y - 15}, {x: x, y: y - 20}, {x: x - 20, y: y - 15}, {x: x - 33, y: y}]) .attr("fill", "none") .attr("opacity", "0.8") .style("stroke-dasharray", ("4, 4")) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); draw_triangle(svg, x - 31, y - 2, 220); svg.append("path") .attr("stroke", "black") .datum([{x: x + 33, y: y + 30}, {x: x + 20, y: y + 45}, {x: x, y: y + 50}, {x: x - 20, y: y + 45}, {x: x - 33, y: y + 30}]) .attr("fill", "none") .attr("opacity", "0.8") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); draw_triangle(svg, x + 31, y + 32, 40); } function graph_reverse_chain() { var svg = d3.select("#grph_rvrs_chain") .append("svg") .attr("width", 600) .attr("height", 105); svg.append('circle') .attr('cx', 50) .attr('cy', 50) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.85) .attr('fill', '#348ABD'); svg.append('text') .attr('x', 42) .attr('y', 55) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 55) .attr('y', 60) .text("0") .style("font-size", "11px") .attr("font-family", "Arvo"); draw_uroboros(svg, 100); svg.append('circle') .attr('cx', 130) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 150) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 170) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); draw_uroboros(svg, 200); svg.append('circle') .attr('cx', 250) .attr('cy', 50) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.75) .attr('fill', '#5286A5'); svg.append('text') .attr('x', 240) .attr('y', 55) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 253) .attr('y', 60) .text("t-1") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('circle') .attr('cx', 350) .attr('cy', 50) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.7) .attr('fill', '#628498'); draw_uroboros(svg, 300); svg.append('text') .attr('x', 268) .attr('y', 10) .text("p") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 275) .attr('y', 15) .text("θ") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 280) .attr('y', 10) .text("(x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 293) .attr('y', 15) .text("t-1") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 304) .attr('y', 10) .text("| x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 319) .attr('y', 15) .text("t") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 322) .attr('y', 10) .text(")") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 272) .attr('y', 97) .text("q(x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 293) .attr('y', 102) .text("t") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 298) .attr('y', 97) .text("| x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 312) .attr('y', 102) .text("t-1") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 323) .attr('y', 97) .text(")") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 342) .attr('y', 55) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 355) .attr('y', 60) .text("t") .style("font-size", "11px") .attr("font-family", "Arvo"); draw_uroboros(svg, 400); svg.append('circle') .attr('cx', 430) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 450) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 470) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 550) .attr('cy', 50) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.5) .attr('fill', '#808080'); draw_uroboros(svg, 500); svg.append('text') .attr('x', 542) .attr('y', 55) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 555) .attr('y', 60) .text("T") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 33) .attr('y', 15) .text("Data") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 532) .attr('y', 15) .text("Prior") .style("font-size", "14px") .attr("font-family", "Arvo"); } graph_reverse_chain(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Forward and reverse diffusion processes. Going backwards, we start from isotropic Gaussian noise $p(\mathbf{x}_T) \sim \mathcal{N}(0, \mathbf{I})$ and gradually sample from $p_\theta(\mathbf{x}_{t-1} \vert \mathbf{x}_t)$ for $t=T, \dots, 1$ until we get a data point from approximated distribution.</em></p><p>Note that reverse conditional probability is tractable when conditioned on $\mathbf{x}_0$:</p>\[q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}({\color{#5286A5}{\tilde \mu(\mathbf{x}_t, \mathbf{x}_0)}}, {\color{#C19454}{\tilde \beta_t \mathbf{I}}}).\]<p>Efficient training is therefore possible by minimizing Kullback-Leibler divergence between $p_\theta$ and $q$, or formally, evidence lower bound loss</p>\[\begin{aligned} L_{\operatorname{ELBO}} &amp;= \mathbb{E}_q\bigg[\log\frac{q(\mathbf{x}_{1:T} \vert \mathbf{x}_0)}{p_\theta(\mathbf{x}_{0:T})} \bigg] \\ &amp;= \mathbb{E}_q\bigg[\log\frac{\prod_{t=1}^T q(\mathbf{x}_t|\mathbf{x}_{t-1}) }{p_\theta(\mathbf{x}_T) \prod_{t=1}^T p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} \bigg] \\ &amp;= \mathbb{E}_q\bigg[\sum_{t=1}^T \log \frac{ q(\mathbf{x}_t|\mathbf{x}_{t-1})} {p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} -\log p_\theta(\mathbf{x}_T)\bigg] \\ &amp;= \mathbb{E}_q\bigg[\log \frac{q(\mathbf{x}_1|\mathbf{x}_{0})}{p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_0) q(\mathbf{x}_t|\mathbf{x}_0)}{q(\mathbf{x}_{t-1}|\mathbf{x}_0)p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} -\log p_\theta(\mathbf{x}_T)\bigg] \\ &amp;= \mathbb{E}_q\bigg[\log \frac{q(\mathbf{x}_1|\mathbf{x}_{0})}{p_\theta(\mathbf{x}_{0}|\mathbf{x}_1)} + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)} + \log \frac{q(\mathbf{x}_T|\mathbf{x}_0)}{q(\mathbf{x}_1|\mathbf{x}_0)}-\log p_\theta(\mathbf{x}_T)\bigg] \\ &amp;= \mathbb{E}_q\bigg[-\log p_\theta(\mathbf{x}_0|\mathbf{x}_1) + \sum_{t=2}^T \log \frac{q(\mathbf{x}_{t-1}|\mathbf{x}_{t}, \mathbf{x}_0)}{p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)}+ \log \frac{q(\mathbf{x}_T|\mathbf{x}_0)}{p_\theta(\mathbf{x}_T)}\bigg]. \end{aligned}\]<p>Labeling each term:</p>\[\begin{aligned} L_0 &amp;= \mathbb{E}_q[-\log p_\theta(\mathbf{x}_0|\mathbf{x}_1)], &amp; \\ L_{t} &amp;= D_{\operatorname{KL}}\big(q(\mathbf{x}_{t-1} \vert \mathbf{x}_{t}, \mathbf{x}_0) \big|\big| p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)\big), &amp;t = 1, \dots T-1, \\ L_T &amp;= D_{\operatorname{KL}}\big(q(\mathbf{x}_T \vert \mathbf{x}_0) \big|\big| p_\theta(\mathbf{x}_T)\big)\big], \end{aligned}\]<p>we get total objective</p>\[L_{\operatorname{ELBO}}= \sum_{t=0}^{T} L_t.\]<p>Last term $L_T$ can be ignored, as $q$ doesn’t depend on $\theta$ and $p_\theta(\mathbf{x}_T)$ is isotropic Gaussian. All KL divergences in equation above are comparisons between Gaussians, so they can be calculated with closed form expressions instead of high variance Monte Carlo estimates. One can estimate $\color{#5286A5}{\tilde\mu(\mathbf{x}_t, \mathbf{x}_0)}$ directly with</p>\[L_t = \mathbb{E}_q \Big[ \frac{1}{2\sigma_t^2} \|{\color{#5286A5}{\tilde\mu(\mathbf{x}_t, \mathbf{x}_0)}} - \mu_\theta(\mathbf{x}_t, t) \|^2 \Big] + C,\]<p>where $C$ is some constant independent of $\theta$. However <a href="https://arxiv.org/pdf/2006.11239.pdf">Ho et al.</a> propose a different way - train neural network $\epsilon_\theta(\mathbf{x}_t, t)$ to predict the noise.</p><p>We can start from reformulation of $q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)$. First, note that</p>\[\begin{aligned} \log q(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{x}_0) &amp;\propto - {\frac{(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1})^2}{\beta_t}} = - {\frac{\mathbf{x}_t^2 - 2 \sqrt{\alpha_t} \mathbf{x}_t{\color{#5286A5}{\mathbf{x}_{t-1}}} + {\alpha_t} {\color{#C19454}{\mathbf{x}_{t-1}^2}}}{\beta_t}}, \\ \log q(\mathbf{x}_{t-1}|\mathbf{x}_0) &amp;\propto -{\frac{(\mathbf{x}_{t-1} - \sqrt{\bar\alpha_{t-1}} \mathbf{x}_{0})^2}{1-\bar\alpha_{t-1}}} = - {\frac{ {\color{#C19454} {\mathbf{x}_{t-1}^2} } - 2\sqrt{\bar\alpha_{t-1}}{\color{#5286A5}{\mathbf{x}_{t-1}}} \mathbf{x}_{0} + \bar\alpha_{t-1}\mathbf{x}_{0}^2}{1-\bar\alpha_{t-1}}}. \end{aligned}\]<p>Then, using Bayesian rule we have:</p>\[\begin{aligned} \log q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) &amp; = \log q(\mathbf{x}_t|\mathbf{x}_{t-1}, \mathbf{x}_0) + \log q(\mathbf{x}_{t-1}|\mathbf{x}_0) - \log q(\mathbf{x}_{t}|\mathbf{x}_0) \\ &amp; \propto {-\color{#C19454}{(\frac{\alpha_t}{\beta_t} + \frac{1}{1-\bar{\alpha}_{t-1}}) \mathbf{x}_{t-1}^2}} + {\color{#5286A5}{(\frac{2\sqrt{\alpha_t}}{\beta_t}\mathbf{x}_t + \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}}\mathbf{x}_0 )\mathbf{x}_{t-1}}} + f(\mathbf{x}_t, \mathbf{x}_0), \end{aligned}\]<p>where $f$ is some function independent of $\mathbf{x}_{t-1}$.</p><p>Now following the standard Gaussian density function, the mean and variance can be parameterized as follows (recall that $\alpha_t +\beta_t=1$):</p>\[{\color{#C19454}{\tilde \beta_t}} = \Big(\frac{\alpha_t}{\beta_t} + \frac{1}{1-\bar{\alpha}_{t-1}}\Big)^{-1} = \Big(\frac{\alpha_t-\bar{\alpha}_{t}+\beta_t}{\beta_t (1-\bar{\alpha}_{t-1})}\Big)^{-1} = \beta_t \frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_{t}}\]<p>and</p>\[\begin{aligned} {\color{#5286A5}{\tilde\mu(\mathbf{x}_t, \mathbf{x}_0)}} &amp;= \Big( \frac{\sqrt{\alpha_t}}{\beta_t}\mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}}{1-\bar{\alpha}_{t-1}}\mathbf{x}_0 \Big) \cdot \color{#C19454}{\tilde \beta_t} \\ &amp;= \frac{\sqrt{\alpha_t}(1-\bar\alpha_{t-1})}{1-\bar\alpha_t} \mathbf{x}_t + \frac{\sqrt{\bar\alpha_{t-1}}}{1-\bar\alpha_t}\mathbf{x}_0. \end{aligned}\]<p>Using representation $\mathbf{x}_0 = \frac{1}{\sqrt{\bar\alpha_t}}(\mathbf{x}_t - \sqrt{1-\bar\alpha_t}\epsilon)$ we get</p>\[\begin{aligned} L_t &amp;= \mathbb{E}_q \Big[ \frac{1}{2\sigma_t^2} \|{\color{#5286A5}{\tilde\mu(\mathbf{x}_t, \mathbf{x}_0)}} - \mu_\theta(\mathbf{x}_t, t) \|^2 \Big] \\ &amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \Big[ \frac{1}{2\sigma_t^2} \Big \|{\color{#5286A5}{\frac{1}{\sqrt{\bar\alpha_t}}\Big(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar\alpha_t}}\epsilon\Big)}} - \frac{1}{\sqrt{\bar\alpha_t}}\Big(\mathbf{x}_t - \frac{\beta_t}{\sqrt{1-\bar\alpha_t}}\epsilon_\theta(\mathbf{x}_t, t)\Big) \Big \|^2 \Big] \\ &amp;= \mathbb{E}_{\mathbf{x}_0, \epsilon} \Big[ \frac{\beta_t^2}{2\sigma_t^2 \bar\alpha_t (1-\bar\alpha_t)} \Big \|{\color{#5286A5}{\epsilon}} - \epsilon_\theta(\mathbf{x}_t, t) \Big \|^2 \Big] \end{aligned}\]<p>Empirically, <a href="https://arxiv.org/pdf/2006.11239.pdf">Ho et al.</a> found that training the diffusion model works better with a simplified objective that ignores the weighting term:</p>\[L_t^{\text{simple}} = \mathbb{E}_{\mathbf{x}_0, \epsilon} \big[ \|\epsilon - \epsilon_\theta(\mathbf{x}_t, t) \|^2 \big] = \mathbb{E}_{\mathbf{x}_0, \epsilon} \big[ \|\epsilon - \epsilon_\theta(\sqrt{\bar\alpha_t}\mathbf{x}_0+\sqrt{1-\bar\alpha_t} \epsilon, t) \|^2 \big]\]<p><img data-proofer-ignore data-src="/assets/img/diffusion-u-net.png" alt="Diffusion Model Architecture" /> <em>Diffusion models often use U-Net architectures with ResNet blocks and self-attention layers to represent $\epsilon_\theta(\mathbf{x}_t, t)$. Time features (usually sinusoidal positional embeddings or random Fourier features) are fed to the residual blocks using either simple spatial addition or using adaptive group normalization layers. <a href="https://drive.google.com/file/d/1DYHDbt1tSl9oqm3O333biRYzSCOtdtmn/view">Image source</a>.</em></p><p>To summarize, our training process:</p><ul><li>Sample $\mathbf{x}_0 \sim q(\mathbf{x}_0)$<li>Choose randomly a certain step in diffusion process: $t \sim \mathcal{U}(\lbrace 1,2, \dots T \rbrace)$<li>Apply noising: $\mathbf{x}_t = \sqrt{\bar\alpha_t}\mathbf{x}_0+\sqrt{1-\bar\alpha_t} \epsilon$ with $\epsilon \sim \mathcal{N}(0, \mathbf{I})$<li>Take a gradient step on \(\nabla_\theta \| \epsilon - \epsilon_\theta(\mathbf{x}_t, t) \|^2\)<li>Repeat until converge</ul><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">vmap</span><span class="p">,</span> <span class="n">random</span>

<span class="c1"># hyperparameters
</span><span class="n">img_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
<span class="n">T</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># linear schedule
</span><span class="n">betas</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">linspace</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">0.02</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
<span class="n">alphas</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">betas</span>
<span class="n">alpha_bars</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>

<span class="c1"># initial model weights
</span><span class="n">dummy</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">img_shape</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">init</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">dummy</span><span class="p">)</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">eps</span> <span class="o">-</span> <span class="n">model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">apply_noising</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">noise</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">img</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">a</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>
	    
<span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="n">x_0</span><span class="p">):</span>
    <span class="c1"># choose random steps
</span>    <span class="n">t</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">x_0</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">T</span><span class="p">)</span>
    <span class="c1"># add noise
</span>    <span class="n">eps</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">x_0</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="nf">jit</span><span class="p">(</span><span class="nf">vmap</span><span class="p">(</span><span class="n">apply_noising</span><span class="p">))(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">alpha_bars</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">eps</span><span class="p">)</span>
    <span class="c1"># calculate gradients
</span>    <span class="n">grads</span> <span class="o">=</span> <span class="nf">jit</span><span class="p">(</span><span class="nf">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">))(</span><span class="n">params</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">x_t</span><span class="p">)</span>
    <span class="c1"># update parameters with gradients and your favourite optimizer
</span>    <span class="bp">...</span>
</pre></table></code></div></div><p>Inference process consists of the following steps:</p><ul><li>Sample $\mathbf{x}_T \sim \mathcal{N}(0, \mathbf{I})$<li>For $t = T, \dots, 1$</ul>\[\mathbf{x}_{t-1} = \mu_\theta(\mathbf{x}_t, t) + \sigma_t \epsilon,\]<p>where $\epsilon \sim \mathcal{N}(0, \mathbf{I})$ and</p>\[\mu_\theta(\mathbf{x}_t, t) = \frac{1}{\sqrt{\bar\alpha_t}}\Big(\mathbf{x}_t - \frac{1-\alpha_t}{\sqrt{1-\bar\alpha_t}}\epsilon_\theta(\mathbf{x}_t, t) \Big).\]<ul><li>Return $\mathbf{x}_0$</ul><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">sample_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">mu_t</span> <span class="o">=</span> <span class="n">x_t</span> <span class="o">-</span> <span class="n">eps</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bars</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
    <span class="n">mu_t</span> <span class="o">/=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">alpha_bars</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">mu_t</span> <span class="o">+</span> <span class="n">sigma_t</span> <span class="o">*</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">x_0</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sample</span><span class="p">():</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">img_shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">T</span><span class="p">)):</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="nf">sample_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_t</span>
</pre></table></code></div></div><h3 id="denoising-diffusion-implicit-models-ddim">Denoising diffusion implicit models (DDIM)</h3><p>A critical drawback of these models is that they require many iterations to produce a high quality sample. Reverse diffusion process could have thousands of steps and iterating over all the steps is required to produce a single sample, which is much slower compared to GANs, which only needs one pass through a network. For example, it takes around 20 hours to sample 50k images of size 32 × 32 from a DDPM, but less than a minute to do so from a GAN on a Nvidia 2080 Ti GPU. This becomes more problematic for larger images as sampling 50k images of size 256 × 256 could take nearly 1000 hours on the same GPU.</p><p>One simple acceleration method is to reduce diffusion time steps in training. Another one is strided sampling schedule: take sampling update every $[T/S]$ steps to reduce process from $T$ down to $S$ steps. However, both of them lead to immediate worse performance.</p><p>In DDPMs, the generative process is defined as the reverse of a particular Markovian diffusion process, meaning that each event $t$ depends only on the state attained in the previous event $t-1$. <a href="https://arxiv.org/pdf/2010.02502.pdf">Song et al.</a> generalized DDPMs via a class of non-Markovian diffusion processes that lead to the same training objective.</p><p>We can redefine joint distribution $q(\mathbf{x}_{1 : T} \vert \mathbf{x}_0)$ in a way such that forward process is non-Markovian, while marginals stay the same. Let</p>\[\begin{aligned} \mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0 &amp;= \sqrt{\bar\alpha_{t-1}} \mathbf{x}_0 + \sqrt{1 - \bar\alpha_{t-1}} \epsilon_{t-1} &amp; \color{Salmon}{\epsilon_{t-1} \sim \mathcal{N}(0, \mathbf{I})} \\ &amp; = \sqrt{\bar\alpha_{t-1}} \mathbf{x}_0 + \sqrt{1 - \bar\alpha_{t-1} - \sigma_t^2} \epsilon_{t} + \sigma_t \epsilon &amp; \color{Salmon}{\epsilon \sim \mathcal{N}(0, \mathbf{I})} \\ &amp; = \sqrt{\bar\alpha_{t-1}} \mathbf{x}_0 + \sqrt{1 - \bar\alpha_{t-1} - \sigma_t^2} \frac{\mathbf{x}_t - \sqrt{\bar\alpha_t}\mathbf{x}_0}{\sqrt{1-\bar\alpha_t}} + \sigma_t \epsilon \end{aligned}\]<p>with some deviation process $\sigma_t$. Then we have</p>\[q_\sigma(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar\alpha_{t-1}} \mathbf{x}_0 + \sqrt{1 - \bar\alpha_{t-1} - \sigma_t^2} \frac{\mathbf{x}_t - \sqrt{\bar\alpha_t}\mathbf{x}_0}{\sqrt{1-\bar\alpha_t}}, \sigma^2_t \mathbf{I})\]<p>and</p>\[q_\sigma(\mathbf{x}_{t} \vert \mathbf{x}_0) = \mathcal{N}(\sqrt{\bar\alpha_{t}} \mathbf{x}_0, \sqrt{1 - \bar\alpha_{t}} \mathbf{I}).\]<p>Recall that for Markovian diffusion process we have distribution</p>\[q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) = \mathcal{N}({\color{#5286A5}{\tilde \mu(\mathbf{x}_t, \mathbf{x}_0)}}, {\color{#C19454}{\tilde \beta_t \mathbf{I}}})\]<p>with</p>\[{\color{#C19454}{\tilde \beta_t}} = \frac{1-\bar\alpha_{t-1}}{1-\bar\alpha_t} \beta_t.\]<p>Hence, we can parameterize distribution $q_\sigma(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)$ with hyperparamer $\eta &gt; 0$, such that</p>\[\sigma_t^2 = \eta {\color{#C19454}{\tilde \beta_t}}.\]<p>Different choices of $\eta$ result in different generative processes, all while using the same model $\epsilon_\theta$, so re-training the model is unnecessary. The special case of $\eta = 1$ corresponds to DDPM. Setting $\eta = 0$ makes the sampling process deterministic. Such model is named the <strong>denoising diffusion implicit model (DDIM)</strong>. In general, one can generate samples in autoregressive way by formula</p>\[\mathbf{x}_{t-1} = \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t - \sqrt{1-\bar\alpha_t}\epsilon_\theta(\mathbf{x}_t, t)) + \sqrt{1-\bar\alpha_{t-1} - \sigma_t^2} \epsilon_\theta(\mathbf{x}_t, t) + \sigma_t \epsilon.\]<p>We can accelerate inference process by only sampling a subset of $S$ diffusion steps $\lbrace \tau_1, \dots, \tau_S \rbrace$.</p><div id="ddim_chain" class="svg-container" align="center"></div><script> function ddim_chain() { var svg = d3.select("#ddim_chain") .append("svg") .attr("width", 600) .attr("height", 158); svg.append('circle') .attr('cx', 50) .attr('cy', 70) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.85) .attr('fill', '#348ABD'); svg.append('text') .attr('x', 42) .attr('y', 75) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 55) .attr('y', 80) .text("0") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append("path") .attr("stroke", "black") .datum([{x: 200, y: 55}, {x: 187, y: 40}, {x: 134, y: 20}, {x: 80, y: 40}, {x: 67, y: 55}]) .attr("fill", "none") .attr("opacity", "0.8") .style("stroke-dasharray", ("4, 4")) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); draw_triangle(svg, 69, 52, 230); svg.append("path") .attr("stroke", "black") .datum([{x: 200, y: 85}, {x: 187, y: 100}, {x: 134, y: 120}, {x: 80, y: 100}, {x: 67, y: 85}]) .attr("fill", "none") .attr("opacity", "0.8") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); draw_triangle(svg, 198, 88, 50); svg.append('text') .attr('x', 110) .attr('y', 130) .text("q(x ") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 137) .attr('y', 130) .text("| x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 131) .attr('y', 133) .text("1") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 151) .attr('y', 133) .text("0") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 155) .attr('y', 130) .text(")") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('circle') .attr('cx', 217) .attr('cy', 70) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.75) .attr('fill', '#4388B1'); svg.append('text') .attr('x', 209) .attr('y', 75) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 222) .attr('y', 80) .text("1") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append("path") .attr("stroke", "black") .datum([{x: 536, y: 55}, {x: 520, y: 40}, {x: 385, y: 25}, {x: 250, y: 40}, {x: 233, y: 55}]) .attr("fill", "none") .attr("opacity", "0.8") .style("stroke-dasharray", ("4, 4")) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); draw_triangle(svg, 236, 52, 240); svg.append("path") .attr("stroke", "black") .datum([{x: 217, y: 140}, {x: 50, y: 140}, {x: 50, y: 90}]) .attr("fill", "none") .attr("opacity", "0.8") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .attr("stroke", "black") .datum([{x: 550, y: 90}, {x: 550, y: 140}, {x: 217, y: 140}, {x: 217, y: 90}]) .attr("fill", "none") .attr("opacity", "0.8") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); draw_triangle(svg, 550, 94, 0); svg.append('text') .attr('x', 350) .attr('y', 155) .text("q(x ") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 377) .attr('y', 155) .text("| x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 371) .attr('y', 158) .text("3") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 391) .attr('y', 158) .text("1") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 395) .attr('y', 155) .text(", x )") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 408) .attr('y', 158) .text("0") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('circle') .attr('cx', 383) .attr('cy', 70) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.25) .attr('fill', '#5286A5'); svg.append('text') .attr('x', 375) .attr('y', 75) .text("x") .style("font-size", "21px") .attr("opacity", 0.5) .attr("font-family", "Arvo"); svg.append('text') .attr('x', 388) .attr('y', 80) .text("2") .style("font-size", "11px") .attr("opacity", 0.5) .attr("font-family", "Arvo"); svg.append('circle') .attr('cx', 550) .attr('cy', 70) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.7) .attr('fill', '#628498'); svg.append('text') .attr('x', 542) .attr('y', 75) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 555) .attr('y', 80) .text("3") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 105) .attr('y', 15) .text("p") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 112) .attr('y', 20) .text("θ") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 119) .attr('y', 15) .text("(x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 132) .attr('y', 20) .text("0") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 137) .attr('y', 15) .text("| x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 151) .attr('y', 20) .text("1") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 155) .attr('y', 15) .text(")") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 350) .attr('y', 15) .text("p") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 357) .attr('y', 20) .text("θ") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 364) .attr('y', 15) .text("(x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 377) .attr('y', 20) .text("1") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 381) .attr('y', 15) .text("| x") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 396) .attr('y', 20) .text("2") .style("font-size", "8px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 400) .attr('y', 15) .text(")") .style("font-size", "14px") .attr("font-family", "Arvo"); } ddim_chain(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Accelerated generation with DDIM and $\tau = [1, 3]$.</em></p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">sample_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">x_0_scaled</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_t</span> <span class="o">-</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bars</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">alphas</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
    <span class="n">mu_t</span> <span class="o">=</span> <span class="n">x_0_scaled</span> <span class="o">+</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bars</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">sigma_t</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">mu_t</span> <span class="o">+</span> <span class="n">sigma_t</span> <span class="o">*</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">x_0</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">taus</span><span class="p">):</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">img_shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="n">taus</span><span class="p">):</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="nf">sample_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_t</span>
</pre></table></code></div></div><h3 id="score-based-generative-modelling">Score based generative modelling</h3><p>Diffusion model is an example of discrete Markov chain. We can extend it to continuous stochastic process. Let’s define <strong>Wiener process (Brownian motion)</strong> $\mathbf{w}_t$ - a random process, such that it starts with $0$, its samples are continuous paths and all of its increments are independent and normally distributed, i.e.</p>\[\frac{\mathbf{w}(t) - \mathbf{w}(s)}{\sqrt{t - s}} \sim \mathcal{N}(0, \mathbf{I}), \quad t &gt; s.\]<p>Let also</p>\[\mathbf{x}\big(\frac{t}{T}\big) := \mathbf{x}_t \ \text{ and } \ \beta\big(\frac{t}{T}\big) := \beta_t \cdot T,\]<p>then</p>\[\mathbf{x}\big(\frac{t + 1}{T}\big) = \sqrt{1-\frac{\beta(t/T)}{T}} \mathbf{x}(t/T) + \sqrt{\beta(t/T)} \Big( \mathbf{w}\big(\frac{t+1}{T}\big)-\mathbf{w}\big(\frac{t}{T}\big) \Big).\]<p>Rewriting equation above with $t:=\frac{t}{T}$ and $\Delta t := \frac{1}{T}$, we get</p>\[\begin{aligned} \mathbf{x}(t+\Delta t) &amp;= \sqrt{1-\beta(t)\Delta t} \mathbf{x}(t) + \sqrt{\beta(t)} (\mathbf{w}(t + \Delta t)-\mathbf{w}(t)) \\ &amp; \approx \Big(1 - \frac{\beta(t) \Delta t}{2} \Big) \mathbf{x}(t) + \sqrt{\beta(t)}(\mathbf{w}(t + \Delta t)-\mathbf{w}(t)). &amp; \color{Salmon}{\leftarrow \text{Taylor expansion}} \end{aligned}\]<p>With $\Delta t \rightarrow 0$ this converges to <strong>stochastic differential equation (SDE)</strong>:</p>\[d\mathbf{x} = -\frac{1}{2}\beta(t)\mathbf{x}dt + \sqrt{\beta(t)} d\mathbf{w}.\]<p>The equation of type</p>\[d\mathbf{x} = f(\mathbf{x}, t)dt + g(t)d\mathbf{w}\]<p>has a unique strong solution as long as the coefficients are globally Lipschitz in both state and time (<a href="http://www.stat.ucla.edu/~ywu/research/documents/StochasticDifferentialEquations.pdf">Øksendal (2003)</a>). We hereafter denote by $q_t(\mathbf{x})$ probability density of $\mathbf{x}(t)$.</p><p>By starting from samples of $\mathbf{x}_T \sim q_T(\mathbf{x})$ and reversing the process, we can obtain samples $\mathbf{x}_0 \sim q_0(\mathbf{x})$. It was proved by <a href="https://reader.elsevier.com/reader/sd/pii/0304414982900515?token=87C349DB9BEE275FFC8CA1B9E94F4EB84D25343F2FBCF9886B08402A7CE1C334B1ECBC2A7DB2805CD00A2BD720F9FBFF&amp;originRegion=eu-west-1&amp;originCreation=20220906054001">Anderson (1982)</a> that the reverse of a diffusion process is also a diffusion process, running backwards in time and given by the reverse-time SDE:</p>\[\begin{aligned} d\mathbf{x} = [f(\mathbf{x}, t) - g(t)^2 &amp;\underbrace{\nabla_{\mathbf{x}} \log q_t(\mathbf{x})}]dt + g(t) d\bar{\mathbf{w}}, &amp;\\ &amp;\color{Salmon}{\text{Score Function}} \\ \end{aligned}\]<p>where $\bar{\mathbf{w}}$ is a standard Wiener process when time flows backwards from $T$ to $0$. In our case with</p>\[f(\mathbf{x},t) = -\frac{1}{2}\beta(t)\mathbf{x}(t) \ \text{ and } \ g(t) = \sqrt{\beta(t)}\]<p>we have reverse diffusion process</p>\[d\mathbf{x} = \big[-\frac{1}{2}\beta(t)\mathbf{x} - \beta(t) \nabla_{\mathbf{x}} \log q_t(\mathbf{x})\big] dt + \sqrt{\beta(t)}d\bar{\mathbf{w}}.\]<p>Once the score of each marginal distribution is known for all $t$, we can map data to a noise (prior) distribution with a forward SDE, and reverse this SDE for to sample from $q_0$.</p><div id="cntns_chain" class="svg-container" align="center"></div><script> d3.select("#cntns_chain") .style("position", "relative"); function continuous_chain() { var svg = d3.select("#cntns_chain") .append("svg") .attr("width", 600) .attr("height", 85); svg.append('circle') .attr('cx', 50) .attr('cy', 50) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.85) .attr('fill', '#348ABD'); svg.append('text') .attr('x', 42) .attr('y', 55) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 55) .attr('y', 60) .text("0") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('line') .attr('x1', 70) .attr('y1', 40) .attr('x2', 530) .attr('y2', 40) .style("stroke-width", 1) .attr('stroke', 'black'); svg.append('line') .attr('x1', 70) .attr('y1', 60) .attr('x2', 530) .attr('y2', 60) .style("stroke-width", 1) .attr('stroke', 'black'); draw_triangle(svg, 525, 60, 90); draw_triangle(svg, 75, 40, 270); svg.append('circle') .attr('cx', 550) .attr('cy', 50) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.5) .attr('fill', '#808080'); svg.append('text') .attr('x', 542) .attr('y', 55) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 555) .attr('y', 60) .text("T") .style("font-size", "11px") .attr("font-family", "Arvo"); d3.select("#cntns_chain") .append("span") .text("\\(d\\mathbf{x} = -\\frac{1}{2}\\beta(t)\\mathbf{x} dt + \\sqrt{\\beta(t)}d\\mathbf{w} \\)") .style("font-size", "14px") .style("font-weight", "700") .attr("font-family", "Arvo") .style("position", "absolute") .style("left", "285px") .style("top", "70px"); d3.select("#cntns_chain") .append("span") .text("\\(d\\mathbf{x} = \\big[-\\frac{1}{2}\\beta(t)\\mathbf{x} - \\beta(t) \\nabla_{\\mathbf{x}} \\log q_t(\\mathbf{x})\\big] dt + \\sqrt{\\beta(t)}d\\bar{\\mathbf{w}} \\)") .style("font-size", "14px") .style("font-weight", "700") .attr("font-family", "Arvo") .style("position", "absolute") .style("left", "215px") .style("top", "10px"); svg.append('text') .attr('x', 33) .attr('y', 15) .text("Data") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 532) .attr('y', 15) .text("Prior") .style("font-size", "14px") .attr("font-family", "Arvo"); } continuous_chain(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Forward and reverse generative diffusion SDEs.</em></p><p>In order to estimate $\nabla_{\mathbf{x}} \log q_t(\mathbf{x})$ we can train a time-dependent score-based model $\mathbf{s}_\theta(\mathbf{x}, t)$, such that</p>\[\mathbf{s}_\theta(\mathbf{x}, t) \approx \nabla_{\mathbf{x}} \log q_t(\mathbf{x}).\]<p>The marginal diffused density $q_t(\mathbf{x}(t))$ is not tractable, however,</p>\[q_t(\mathbf{x}(t) \vert \mathbf{x}(0)) \sim \mathcal{N}(\sqrt{\bar{\alpha}(t)} \mathbf{x}(0), (1 - \bar{\alpha}(t)) \mathbf{I})\]<p>with $\bar{\alpha}(t) = e^{\int_0^t \beta(s) ds}$. Therefore we can minimize</p>\[\mathcal{L} = \mathbb{E}_{t \sim \mathcal{U}(0, t)} \mathbb{E}_{\mathbf{x}(0) \sim q_0(\mathbf{x})} \mathbb{E}_{\mathbf{x}(t) \sim q_t(\mathbf{x}(t) \vert \mathbf{x}(0))}[ \| \mathbf{s}_\theta(\mathbf{x}(t), t) - \nabla_{\mathbf{x}(t)} \log q_t(\mathbf{x}(t) \vert \mathbf{x}(0)) \|^2 ].\]<h4 id="connection-to-diffusion-model">Connection to diffusion model</h4><p>Given a Gaussian distribution</p>\[\mathbf{x}(t) = \sqrt{\bar{\alpha}(t)} \mathbf{x}(0) + \sqrt{1 - \bar{\alpha}(t)} \epsilon, \quad \epsilon \sim \mathcal{N}(0, \mathbf{I}),\]<p>we can write the derivative of the logarithm of its density function as</p>\[\begin{aligned} \nabla_{\mathbf{x}(t)} \log q_t(\mathbf{x}(t) \vert \mathbf{x}(0)) &amp;= -\nabla_{\mathbf{x}(t)} \frac{(\mathbf{x}(t) - \sqrt{\bar{\alpha}(t)} \mathbf{x}(0))^2}{2 (1 - \bar{\alpha}(t))} \\ &amp;= -\frac{\mathbf{x}(t) - \sqrt{\bar{\alpha}(t)} \mathbf{x}(0)}{1 - \bar{\alpha}(t)} \\ &amp;= \frac{\epsilon}{\sqrt{1 - \bar{\alpha}(t)}}. \end{aligned}\]<p>Also,</p>\[\mathbf{s}_\theta(\mathbf{x}, t) = -\frac{\epsilon_\theta(\mathbf{x}, t)}{\sqrt{1 - \bar{\alpha}(t)}}.\]<h3 id="guided-diffusion">Guided diffusion</h3><p>Once the model $\epsilon_\theta(\mathbf{x}_t, t)$ is trained, we can use it to run the isotropic Gaussian distribution $\mathbf{x}_T$ back to $\mathbf{x}_0$ and generate limitless image variations. But how can we guide the class-conditional model to generate specific images by feeding additional information about class $y$ during the training process?</p><h4 id="classifier-guidance">Classifier guidance</h4><p>If we have a differentiable discriminative model $f_\phi(y \vert \mathbf{x}_t)$, trained to classify noisy images $\mathbf{x}_t$, we can use its gradients to guide the diffusion sampling process toward the conditioning information $y$ by altering the noise prediction.</p><p>We can write the score function for the joint distribution $q(\mathbf{x}, y)$ as following,</p>\[\begin{aligned} \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t, y) &amp;= \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) + \nabla_{\mathbf{x}_t} \log q(y \vert \mathbf{x}_t) \\ &amp; \approx -\frac{\epsilon_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}} + \nabla_{\mathbf{x}_t} \log f_\phi (y \vert \mathbf{x}_t) \\ &amp;= -\frac{1}{\sqrt{1 - \bar{\alpha}_t}}\big(\epsilon_\theta(\mathbf{x}_t, t) - \sqrt{1 - \bar{\alpha}_t}\nabla_{\mathbf{x}_t} \log f_\phi (y \vert \mathbf{x}_t)\big). \end{aligned}\]<p>At each step of denoising, the classifier checks whether the image is denoised in the right direction and contributes its own gradient of loss function into the overall loss of diffusion model. To control the strength of the classifier guidance, we can add a weight $\omega$, called the <strong>guidance scale</strong>, and here is our new classifier-guided model:</p>\[\tilde{\epsilon}_\theta(\mathbf{x}_t, t) = \epsilon_\theta(\mathbf{x}_t, t) - \omega \sqrt{1 - \bar{\alpha}_t} \nabla_{\mathbf{x}_t} \log f_\phi (y \vert \mathbf{x}_t).\]<p>We can then use the exact same sampling procedure, but with the modified noise predictions $\tilde{\epsilon}_\theta$. This results in approximate sampling from distribution:</p>\[\tilde{q}(\mathbf{x}_t \vert y) \propto q(\mathbf{x}_t) \cdot q(y \vert \mathbf{x}_t)^\omega.\]<p>Basically, we are raising the conditional part of the distribution to a power, which corresponds to tuning the inverse temperature of that distribution. With large $\omega$ we focus onto distribution modes and produce higher fidelity (but less diverse) samples.</p><p><img data-proofer-ignore data-src="/assets/img/guided-gaussian.png" alt="Guided Gaussians" /> <em>Guidance on a toy 2D example of three classes, in which the conditional distribution for each class is an isotropic Gaussian, each mixture component representing data conditioned on a class. The leftmost plot is the non-guided marginal density. Left to right are densities of mixtures of normalized guided conditionals with increasing guidance strength. <a href="https://arxiv.org/pdf/2207.12598.pdf">Image source</a></em></p><p>A downside of classifier guidance is that it requires an additional classifier model and thus complicates the training pipeline. One can’t plug in a standard pre-trained classifier, because this model has to be trained on noisy data $\mathbf{x}_t$. And even having a classifier, which is robust to noise, classifier guidance is inherently limited in its effectiveness. Most of the information in the input $\mathbf{x}_t$ is not relevant to predicting $y$, and as a result, taking the gradient of the classifier w.r.t. its input can yield arbitrary (and even adversarial) directions in input space.</p><h4 id="classifier-free-guidance">Classifier-free guidance</h4><p><a href="https://openreview.net/pdf?id=qw8AKxfYbI">Ho &amp; Salimans</a> proposed an alternative method, <strong>a classifier-free guidance</strong>, which doesn’t require training a separate classifier. Instead, one trains a conditional diffusion model, parameterized by $\epsilon_\theta(\mathbf{x}_t, t \vert y)$ with conditioning dropout: 10-20% of the time, the conditioning information $y$ is removed. In practice, it is replaced with a special input value $y=\emptyset$ representing the absence of conditioning information. This way model knows how to generate images unconditionally as well, i.e.</p>\[\epsilon_\theta(\mathbf{x}_t, t) = \epsilon_\theta(\mathbf{x}_t, t \vert \emptyset).\]<p>How could we use it for sampling? By Bayes rule we have</p>\[\begin{aligned} q(y \vert \mathbf{x}_t) &amp;= \frac{q(\mathbf{x}_t \vert y) q(y)}{q(\mathbf{x}_t)} \\ \Longrightarrow \nabla_{\mathbf{x}_t} \log q(y \vert \mathbf{x}_t) &amp;= \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t \vert y) - \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) \\ &amp; \approx -\frac{\epsilon_\theta(\mathbf{x}_t, t \vert y) - \epsilon_\theta(\mathbf{x}_t, t)}{\sqrt{1 -\bar{\alpha}_t}}. \end{aligned}\]<p>Substituting this into the formula for classifier guidance, we get</p>\[\begin{aligned} \tilde{\epsilon}_\theta(\mathbf{x}_t, t \vert y) &amp;= \epsilon_\theta(\mathbf{x}_t, t) - \omega \sqrt{1 - \bar{\alpha}_t} \nabla_{\mathbf{x}_t} \log q(y \vert \mathbf{x}_t) \\ &amp;= (1-\omega) \epsilon_\theta(\mathbf{x}_t, t) + \omega\epsilon_\theta(\mathbf{x}_t, t \vert y). \end{aligned}\]<p>The classifier-free guided model is a linear interpolation between models with and without labels: for $\omega=0$ we get unconditional model, and for $\omega=1$ we get the standard conditional model. However, as experiments have shown in <a href="https://arxiv.org/pdf/2105.05233.pdf">Dhariwal &amp; Nichol paper</a>, guidance works even better with $\omega &gt; 1$.</p><p>Note on notation: authors of original paper applied classifier guidance to already conditional diffusion model $\epsilon(\mathbf{x}_t, t \vert y)$:</p>\[\begin{aligned} \tilde{\epsilon}_\theta(\mathbf{x}_t, t \vert y) &amp;= \epsilon_\theta(\mathbf{x}_t, t \vert y) - \omega \sqrt{1 - \bar{\alpha}_t} \nabla_{\mathbf{x}_t} \log q(y \vert \mathbf{x}_t) \\ &amp;= (\omega + 1) \epsilon_\theta(\mathbf{x}_t, t \vert y) - \omega\epsilon_\theta(\mathbf{x}_t, t). \end{aligned}\]<p>This is the same as applying guidance to unconditional model with $\omega + 1$ scale, because</p>\[\tilde{q}(\mathbf{x}_t \vert y) \propto q(\mathbf{x}_t \vert y) \cdot q(y \vert \mathbf{x}_t)^\omega \propto q(\mathbf{x}_t) \cdot q(y \vert \mathbf{x}_t)^{\omega+1}.\]<h4 id="clip-guidance">CLIP guidance</h4><p>With CLIP guidance the classifier is replaced with a <a href="https://arxiv.org/pdf/2103.00020.pdf"><strong>CLIP</strong></a> <strong>model</strong> (abbreviation for <strong>C</strong>ontrastive <strong>L</strong>anguage-<strong>I</strong>mage <strong>P</strong>re-training). CLIP was originally a separate auxiliary model to rank the results from generative model, called <a href="https://arxiv.org/pdf/2102.12092.pdf"><strong>DALL·E</strong></a>. DALL·E was the first public system capable of creating images based on a textual description from OpenAI, however it was not a diffusion model and is therefore out of the scope for this post. DALL·E’s name is a portmanteau of the names of animated robot Pixar character WALL-E and the Spanish surrealist artist Salvador Dalí.</p><p>The idea behind CLIP is fairly simple:</p><ul><li>Take two encoders, one for a text snippet and another one for an image<li>Collect a sufficiently large dataset of image-text pairs (e.g. 400 million scraped from the Internet in the paper)<li>Train the model in a contrastive fashion: it must produce high similarity score for an image and a text from the same pair and a low similarity score for mismatched image and text.</ul><p><img data-proofer-ignore data-src="/assets/img/clip-arch.png" alt="CLIP" /> <em>CLIP approach: jointly train an image encoder and a text encoder to predict the correct pairings of a batch of (image, text) training examples. At test time the learned text encoder synthesizes a zero-shot linear classifier by embedding the names or descriptions of the target dataset’s classes. The classes can be adjustable without retraining a model.</em></p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre><td class="rouge-code"><pre><span class="c1"># image_encoder - ResNet or Vision Transformer
# text_encoder - CBOW or Text Transformer
# I[n, h, w, c] - minibatch of aligned images
# T[n, l] - minibatch of aligned texts
# W_i[d_i, d_e] - learned proj of image to embed
# W_t[d_t, d_e] - learned proj of text to embed
# tau - learned temperature parameter
</span>
<span class="c1"># extract feature representations of each modality
</span><span class="n">I_f</span> <span class="o">=</span> <span class="nf">image_encoder</span><span class="p">(</span><span class="n">I</span><span class="p">)</span> <span class="c1">#[n, d_i]
</span><span class="n">T_f</span> <span class="o">=</span> <span class="nf">text_encoder</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="c1">#[n, d_t]
</span>
<span class="c1"># joint multimodal embedding [n, d_e]
</span><span class="n">I_e</span> <span class="o">=</span> <span class="nf">l2_normalize</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">I_f</span><span class="p">,</span> <span class="n">W_i</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">T_e</span> <span class="o">=</span> <span class="nf">l2_normalize</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">T_f</span><span class="p">,</span> <span class="n">W_t</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># scaled pairwise cosine similarities [n, n]
</span><span class="n">logits</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">I_e</span><span class="p">,</span> <span class="n">T_e</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">tau</span><span class="p">)</span>

<span class="c1"># symmetric loss function
</span><span class="n">labels</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">loss_i</span> <span class="o">=</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">loss_t</span> <span class="o">=</span> <span class="nf">cross_entropy_loss</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss_i</span> <span class="o">+</span> <span class="n">loss_t</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
</pre></table></code></div></div><p>Let $f(\mathbf{x})$ and $g(y)$ be image and text encoders respectively. Then CLIP loss for $(i, j)$ pair is</p>\[\begin{aligned} \mathcal{L}_{\operatorname{CLIP}}(i, j) &amp;= \frac{1}{2} \bigg(-\log \frac{\exp(f(\mathbf{x}_i) \cdot g(y_j) / \tau)}{\sum_k \exp(f(\mathbf{x}_i) \cdot g(y_k) / \tau)}-\log \frac{\exp(f(\mathbf{x}_i) \cdot g(y_j) / \tau)}{\sum_k \exp(f(\mathbf{x}_k) \cdot g(y_j) / \tau)} \bigg). \end{aligned}\]<p>Ideally, we get</p>\[f(\mathbf{x}) \cdot g(y) \approx \frac{q(\mathbf{x}, y)}{q(\mathbf{x}) q(y)} = \frac{q(y \vert \mathbf{x})}{ q(y)},\]<p>which can be used to steer generative models instead of pretrained classifier:</p>\[\begin{aligned} \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t, y) &amp;= \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) + \nabla_{\mathbf{x}_t} \log q(y \vert \mathbf{x}_t) \\&amp;= \nabla_{\mathbf{x}_t} \log q(\mathbf{x}_t) + \nabla_{\mathbf{x}_t} (\log q(y \vert \mathbf{x}_t) -\log q(y)) \\ &amp; \approx -\frac{\epsilon_\theta(\mathbf{x}_t, t)}{\sqrt{1 - \bar{\alpha}_t}} + \nabla_{\mathbf{x}_t} \log (f(\mathbf{x}_t) \cdot g(y)). \end{aligned}\]<p>Similar to classifier guidance, CLIP must be trained on noised images $\mathbf{x}_t$ to obtain the correct gradient in the reverse process.</p><h3 id="glide">GLIDE</h3><p><a href="https://arxiv.org/pdf/2112.10741.pdf"><strong>GLIDE</strong></a>, which stands for <strong>G</strong>uided <strong>L</strong>anguage to <strong>I</strong>mage <strong>D</strong>iffusion for Generation and <strong>E</strong>diting, is a model by OpenAI that has beaten DALL·E, arguably presented the most novel and interesting ideas and yet received comparatively little publicity.</p><p>Motivated by the ability of guided diffusion models to generate photorealistic samples and the ability of text-to-image models to handle free-form prompts, authors of the paper applied guided diffusion to the problem of text-conditional image synthesis. They also compared two techniques for guiding diffusion models towards text prompts: CLIP guidance and classifier-free guidance. Using human and automated evaluations, they found that classifier-free guidance yields higher-quality images.</p><p>An architecture of GLIDE consists of three major blocks:</p><ul><li>U-Net based text-conditional diffusion model at 64 × 64 resolution (2.3B parameters)<li>Transformer based text encoder to condition on natural language descriptions (1.2B parameters)<li>Another text-conditional upsampling diffusion model to increase the resolution to 256 × 256 (1.5B parameters)</ul><p>The U-Net model as usual stacks residual layers with downsampling convolutions, followed by a stack of residual layers with upsampling convolutions, with skip connections connecting the layers with the same spatial size. It also consists of attention layers which are crucial for simultaneous text processing.</p><p>The text encoder was built from 24 residual blocks of width 2048. The input is a sequence of $K$ tokens. The output of the transformer is used in two ways:</p><ul><li>the final token embedding token is used in place of a class embedding $y$,<li>the final layer of token embeddings is added to every attention layer of the model.</ul><p>Model has fewer parameters than DALL·E (5B vs 12B) and was trained on the same dataset, however was favored over it by human evaluators and has beaten it by <a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance">FID score</a>. However, as the authors mentioned, unoptimized GLIDE takes 15 seconds to sample one image on a single A100 GPU. This is much slower than sampling for related GAN methods, which produce images in a single forward pass and are thus more favorable for use in real-time applications.</p><h3 id="dalle-2-unclip">DALL·E 2 (unCLIP)</h3><p>In April 2022, OpenAI released a new model, called <a href="https://arxiv.org/pdf/2204.06125.pdf"><strong>DALL·E 2</strong></a> (or <strong>unCLIP</strong> in the paper), which is a clever combination of CLIP and GLIDE. The CLIP model is trained separately on a data of image-text pairs $(\mathbf{x}, y)$. Let</p>\[\mathbf{z}_i = \operatorname{CLIP}(\mathbf{x}) \quad \text{and} \quad \mathbf{z}_t = \operatorname{CLIP}(y)\]<p>be CLIP image and text embeddings respectively. In the paper Vision Transformer was used as image encoder and Transformer with a causal attention mask as text encoder. Then CLIP is frozen and the unCLIP learns two models in parallel:</p><ul><li>a special prior model $q(\mathbf{z}_i \vert y)$ outputs CLIP image embedding given the text $y$,<li>a diffusion decoder (modified GLIDE) $q(\mathbf{x} \vert \mathbf{z}_i, [y])$ generates the image $\mathbf{x}$ given CLIP image embedding $\mathbf{z}_i$ and optionally the original text $y$.</ul><p>Stacking these two components yields a generative model of images $\mathbf{x}$ given captions $y$:</p>\[q(\mathbf{x} \vert y) = q(\mathbf{x}, \mathbf{z}_i \vert y) = q(\mathbf{x} \vert \mathbf{z}_i, y) \cdot q(\mathbf{z}_i \vert y).\]<p><img data-proofer-ignore data-src="/assets/img/unCLIP.png" alt="unCLIP" /> <em>unCLIP architecture. Below the dotted line the text-to-image process is depicted. Given a text $y$ the CLIP text encoder generates a text embedding $\mathbf{z}_t$. Then a diffusion or autoregressive prior model processes this CLIP text embedding to construct an image embedding $\mathbf{z}_i$. Then a diffusion decoder generates images conditioned on CLIP image embeddings (and text). The decoder essentially inverts image embeddings back into images</em></p><p>The authors tested two model classes for the prior:</p><ul><li>Autoregressive prior quantizes image embedding to a sequence of discrete codes and predict them autoregressively.<li>Diffusion prior models the continuous image embedding by diffusion models conditioned on $y$.</ul><p>Diffusion prior outperforms the autoregressive prior for comparable model size and reduced training compute. The diffusion prior also performs better than the autoregressive prior in pairwise comparisons against GLIDE.</p><p>As opposed to the way of training proposed by <a href="https://arxiv.org/pdf/2006.11239.pdf">Ho et al.</a>, predicting the unnoised image embedding directly instead of predicting the noise was a better fit. Meaning, that instead of</p>\[L_t = \mathbb{E}_{\mathbf{x}_0, \epsilon} \big[ \|\epsilon - \epsilon_\theta(\mathbf{x}_t, t \vert y) \|^2 \big]\]<p>the unCLIP diffusion prior loss is</p>\[L_t = \mathbb{E}_{\mathbf{x}_0, \epsilon} \big[ \| \mathbf{z}_i - f_\theta(\mathbf{z}_i^{(t)}, t \vert y) \|^2 \big],\]<p>where $f_\theta$ stands for the prior model and $\mathbf{z}_i^{(t)}$ is the noised image embedding.</p><p>The bipartite latent representation enables several text-guided image manipulation tasks. For example, one can fix CLIP image embeddings $\mathbf{z}_i$ and run decoder with different decoder latents $\mathbf{x}_T$.</p><p><img data-proofer-ignore data-src="/assets/img/unCLIP-manipulation.png" alt="unCLIP manipulation" /> <em>Variations of an input image by encoding with CLIP and then decoding with a diffusion model. The variations preserve both semantic information like presence of a clock in the painting and the overlapping strokes in the logo, as well as stylistic elements like the surrealism in the painting and the color gradients in the logo, while varying the non-essential details.</em></p><p>Or one can change $\mathbf{z}_i$ towards the difference of the text CLIP embeddings $\mathbf{z}_t$ of two prompts.</p><p><img data-proofer-ignore data-src="/assets/img/unCLIP-manipulation-2.png" alt="unCLIP manipulation 2" /> <em>Text diffs applied to images by interpolating between their CLIP image embeddings and a normalised difference of the CLIP text embeddings produced from the two descriptions. Decoder latent $\mathbf{x}_T$ is kept as a constant.</em></p><h3 id="imagen">Imagen</h3><p>Two months after the publication of DALL·E 2 Google Brain team presented <a href="https://arxiv.org/pdf/2205.11487.pdf"><strong>Imagen</strong></a>. It uses a pre-trained T5-XXL language model instead of CLIP to encode text for image generation. The idea is that this model has vastly more context regarding language processing than a model trained only on the image captions, and so is able to produce more valuable embeddings without the need to additionally fine-tune it. Authors of the paper noted, that scaling text encoder is extremely efficient and more important than scaling diffusion model size.</p><p>Next, the resolution is increased via super-resolution diffusion models. There is another key observation from authors: noise conditioning augmentation weakens information from low-resolution models, thus it is beneficial to use text conditioning as extra information input.</p><p><img data-proofer-ignore data-src="/assets/img/imagen-arch.png" alt="Imagen" /> <em>Visualization of Imagen. Imagen uses a frozen text encoder to encode the input text into text embeddings. A conditional diffusion model maps the text embedding into a 64 × 64 image. Imagen further utilizes text-conditional super-resolution diffusion models to upsample the image, first 64 × 64 → 256 × 256, and then 256 × 256 → 1024 × 1024.</em></p><h4 id="noise-conditioning-augmentation">Noise conditioning augmentation</h4><p>The solution can be viewed as a sequence of diffusion models, which was called <strong>cascaded diffusion models</strong> in <a href="https://arxiv.org/pdf/2106.15282.pdf">Ho et al. (2021)</a>. Noise conditioning augmentation between these models is crucial to the final image quality, which is to apply strong data augmentation to the low-resolution image $\mathbf{z}$ of each super-resolution model $p_\theta(\mathbf{x} \vert \mathbf{z})$. In simple terms, it is equivalent to applying various data augmentation techniques, such as a Gaussian noise/blur, to a low-resolution image before it is fed into the super-resolution models.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">train_step_base</span><span class="p">(</span><span class="n">z_0</span><span class="p">):</span>
    <span class="c1"># diffusion forward process
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">T</span><span class="p">)</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">z</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">z_s</span> <span class="o">=</span> <span class="nf">jit</span><span class="p">(</span><span class="nf">vmap</span><span class="p">(</span><span class="n">apply_noising</span><span class="p">))(</span><span class="n">z</span><span class="p">,</span> <span class="n">alpha_bars</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="n">eps</span><span class="p">)</span>
    <span class="c1"># optimize loss(z_0, model(z_s, s))
</span>    <span class="bp">...</span>
    
<span class="k">def</span> <span class="nf">train_step_sr</span><span class="p">(</span><span class="n">z_0</span><span class="p">,</span> <span class="n">x_0</span><span class="p">):</span>
    <span class="c1"># add gaussian conditioning augmentation to the low-resolution image
</span>    <span class="n">s</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">z_0</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">T</span><span class="p">)</span>
    <span class="n">eps_z</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">z_0</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">z_0</span> <span class="o">=</span> <span class="nf">jit</span><span class="p">(</span><span class="nf">vmap</span><span class="p">(</span><span class="n">apply_noising</span><span class="p">))(</span><span class="n">z_0</span><span class="p">,</span> <span class="n">alpha_bars</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="n">eps_z</span><span class="p">)</span>    
    <span class="c1"># diffusion forward process
</span>    <span class="n">t</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">x_0</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">minval</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="n">T</span><span class="p">)</span>
    <span class="n">eps_x</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">x_0</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="nf">jit</span><span class="p">(</span><span class="nf">vmap</span><span class="p">(</span><span class="n">apply_noising</span><span class="p">))(</span><span class="n">x_0</span><span class="p">,</span> <span class="n">alpha_bars</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">eps_x</span><span class="p">)</span>
    <span class="c1"># optimize loss(x_0, model(x_t, z_0, t, s))
</span>    <span class="bp">...</span>
</pre></table></code></div></div><p>In addition, there are also two similar forms of conditioning augmentation that require small modification to the training process:</p><ul><li>Truncated conditioning augmentation stops the diffusion process early at step $t &gt; 0$ for low resolution.<li>Non-truncated conditioning augmentation runs the full low resolution reverse process until step $0$ but then corrupt it by $\mathbf{z}_t’ \sim q(\mathbf{z}_t \vert \mathbf{z}_0)$ and then feeds the corrupted $\mathbf{z}_t’$ into the super-resolution model.</ul><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">sample_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">condition</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">condition</span><span class="p">)</span>
    <span class="n">mu_t</span> <span class="o">=</span> <span class="n">x_t</span> <span class="o">-</span> <span class="n">eps</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alphas</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_bars</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
    <span class="n">mu_t</span> <span class="o">/=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">alpha_bars</span><span class="p">[</span><span class="n">t</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">mu_t</span> <span class="o">+</span> <span class="n">sigma_t</span> <span class="o">*</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">x_0</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sample_base</span><span class="p">():</span>
    <span class="n">z_t</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">img_shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">T</span><span class="p">)):</span>
        <span class="n">z_t</span> <span class="o">=</span> <span class="nf">sample_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">z_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">not_using_truncated</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">s</span><span class="p">)):</span>
            <span class="n">z_t</span> <span class="o">=</span> <span class="nf">sample_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">z_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="n">eps_z</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">z_t</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">z_t</span> <span class="o">=</span> <span class="nf">apply_noising</span><span class="p">(</span><span class="n">z_t</span><span class="p">,</span> <span class="n">alpha_bars</span><span class="p">[</span><span class="n">s</span><span class="p">],</span> <span class="n">eps_z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z_t</span>

<span class="k">def</span> <span class="nf">sample_sr</span><span class="p">():</span>
    <span class="c1"># sample augmented low-resolution image
</span>    <span class="n">z_0</span> <span class="o">=</span> <span class="nf">sample_base</span><span class="p">()</span>
    <span class="c1"># sample high-resolution image
</span>    <span class="n">x_t</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">img_shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">T</span><span class="p">)):</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="nf">sample_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">z_t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_t</span>
</pre></table></code></div></div><h4 id="dynamic-thresholding">Dynamic thresholding</h4><p>Another major key feature of Imagen is a so-called <strong>dynamic thresholding</strong>. Authors of the model found out that larger classifier-free guidance scale $\omega$ leads to better text alignment, but worse image fidelity producing highly saturated and unnatural images. They hypothesised that large $\omega$ increases train-test mismatch and generated images are saturated due to the very large gradient updates during sampling.</p><p>At each sampling step $t$, the prediction $\mathbf{x}_t$ must be within the same bounds as training data, i.e. within $[−1, 1]$, but authors of Imagen found empirically that high guidance weights cause predictions to exceed these bounds. To counter this problem, they proposed to adjust the pixel values of samples at each sampling step to be within this range. Basically, two approaches could be applied:</p><ul><li>Static thresholding: clip $\mathbf{x}_t$ to $[-1, 1]$.<li>Dynamic thresholding: at each sampling step $t$, compute $s$ as a certain $p$-percentile absolute pixel value; if $s &gt; 1$ clip the prediction to $[-s, s]$ and divide by $s$.</ul><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">sample</span><span class="p">():</span>
    <span class="n">x_t</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">img_shape</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">reversed</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">T</span><span class="p">)):</span>
        <span class="n">x_t</span> <span class="o">=</span> <span class="nf">sample_step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">using_static</span><span class="p">:</span>
            <span class="n">x_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">percentile</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">abs</span><span class="p">(</span><span class="n">x_t</span><span class="p">),</span> <span class="n">p</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="nf">tuple</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_t</span><span class="p">.</span><span class="n">ndim</span><span class="p">)))</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
            <span class="n">x_t</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">x_t</span><span class="p">,</span> <span class="o">-</span><span class="n">s</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span> <span class="o">/</span> <span class="n">s</span>
    <span class="k">return</span> <span class="n">x_t</span>
</pre></table></code></div></div><h3 id="latent-space-diffusion-model--stable-diffusion">Latent-space diffusion model / Stable diffusion</h3><p><a href="https://arxiv.org/pdf/2112.10752.pdf">Rombach &amp; Blattmann, et al. 2022</a> presented <strong>latent diffusion models (LDM)</strong>, which operate in the latent space of pretrained variational autoencoders instead of pixel space, making training cost lower and inference speed faster.</p><p>The diffusion and denoising processes happen on a 2D latent vector $\mathbf{z}$, which is an image $\mathbf{x}$, compressed by encoder. Then an decoder reconstructs the images from the latent vector. The paper explored two types of regularization in autoencoder training to avoid arbitrarily high-variance in the latent spaces:</p><ul><li>KL-regularization: a small KL penalty towards a standard normal distribution over the learned latent, similar to VAE.<li>VQ-regularization: Uses a vector quantization layer within the decoder, like VQVAE but the quantization layer is absorbed by the decoder.</ul><p>The denoising model is a time-conditioned U-Net. Authors also introduced cross-attention layers into the model architecture to handle flexible conditioning information. Each type of conditioning information is paired with a domain-specific encoder $\tau_\theta$ to project the conditioning input $y$ to an intermediate representation, which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing</p>\[\operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \operatorname{softmax} \Big( \frac{\mathbf{QK}^T}{\sqrt{d}} \Big) \cdot \mathbf{V},\]<p>where</p>\[\mathbf{Q} = \mathbf{W}_Q^{(i)} \cdot \varphi_i(\mathbf{z}_t), \ \mathbf{K} =\mathbf{W}_K^{(i)} \cdot \tau_\theta(y), \ \mathbf{V} =\mathbf{W}_V^{(i)} \cdot \tau_\theta(y).\]<p>Here $\varphi_i$ denotes a (flattened) intermediate representation of the UNet implementing $\epsilon_\theta$.</p><div id="ltnt_dffsn" class="svg-container" align="center"></div><script> d3.select("#ltnt_dffsn") .style("position", "relative"); function latent_diffusion() { var svg = d3.select("#ltnt_dffsn") .append("svg") .attr("width", 600) .attr("height", 200); svg.append('circle') .attr('cx', 50) .attr('cy', 150) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.85) .attr('fill', '#348ABD'); svg.append('text') .attr('x', 42) .attr('y', 155) .text("x") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 55) .attr('y', 160) .text("0") .style("font-size", "11px") .attr("font-family", "Arvo"); draw_uroboros(svg, 100, 135); svg.append('text') .attr('x', 70) .attr('y', 112) .text("Decoder") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 70) .attr('y', 196) .text("Encoder") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('circle') .attr('cx', 150) .attr('cy', 150) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.75) .attr('fill', '#A4D8D8'); svg.append('text') .attr('x', 143) .attr('y', 155) .text("z") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 155) .attr('y', 160) .text("0") .style("font-size", "11px") .attr("font-family", "Arvo"); draw_uroboros(svg, 200, 135); svg.append('circle') .attr('cx', 250) .attr('cy', 150) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.75) .attr('fill', '#9EC9C9'); svg.append('text') .attr('x', 243) .attr('y', 155) .text("z") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 255) .attr('y', 160) .text("1") .style("font-size", "11px") .attr("font-family", "Arvo"); draw_uroboros(svg, 300, 135); svg.append('circle') .attr('cx', 330) .attr('cy', 150) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 350) .attr('cy', 150) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 370) .attr('cy', 150) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); draw_uroboros(svg, 400, 135); svg.append('circle') .attr('cx', 450) .attr('cy', 150) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.5) .attr('fill', '#808080'); svg.append('text') .attr('x', 442) .attr('y', 155) .text("z") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 454) .attr('y', 160) .text("T") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('circle') .attr('cx', 550) .attr('cy', 50) .attr('r', 20) .attr('stroke', 'black') .attr("opacity", 0.5) .attr('fill', '#65AD69'); svg.append('text') .attr('x', 545) .attr('y', 55) .text("y") .style("font-size", "21px") .attr("font-family", "Arvo"); svg.append('rect') .attr('x', 181) .attr('y', 79) .attr('width', 40) .attr('height', 20) .attr('stroke', 'black') .attr("rx", 3) .attr("opacity", 0.9) .attr('fill', '#EDA137'); svg.append('rect') .attr('x', 181) .attr('y', 99) .attr('width', 40) .attr('height', 20) .attr('stroke', 'black') .attr("rx", 3) .attr("opacity", 0.9) .attr('fill', '#EDA137'); svg.append('text') .attr('x', 189) .attr('y', 95) .text("K V") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 196) .attr('y', 115) .text("Q") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('rect') .attr('x', 281) .attr('y', 79) .attr('width', 40) .attr('height', 20) .attr('stroke', 'black') .attr("rx", 3) .attr("opacity", 0.9) .attr('fill', '#EDA137'); svg.append('rect') .attr('x', 281) .attr('y', 99) .attr('width', 40) .attr('height', 20) .attr('stroke', 'black') .attr("rx", 3) .attr("opacity", 0.9) .attr('fill', '#EDA137'); svg.append('text') .attr('x', 289) .attr('y', 95) .text("K V") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 296) .attr('y', 115) .text("Q") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('rect') .attr('x', 381) .attr('y', 79) .attr('width', 40) .attr('height', 20) .attr('stroke', 'black') .attr("rx", 3) .attr("opacity", 0.9) .attr('fill', '#EDA137'); svg.append('rect') .attr('x', 381) .attr('y', 99) .attr('width', 40) .attr('height', 20) .attr('stroke', 'black') .attr("rx", 3) .attr("opacity", 0.9) .attr('fill', '#EDA137'); svg.append('text') .attr('x', 389) .attr('y', 95) .text("K V") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 396) .attr('y', 115) .text("Q") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append("path") .attr("stroke", "black") .datum([{x: 530, y: 50}, {x: 420, y: 50}, {x: 401, y: 50}, {x: 401, y: 79}]) .attr("fill", "none") .attr("opacity", "0.8") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .attr("stroke", "black") .datum([{x: 420, y: 50}, {x: 380, y: 50}]) .attr("fill", "none") .attr("opacity", "0.8") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('circle') .attr('cx', 330) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 350) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append('circle') .attr('cx', 370) .attr('cy', 50) .attr('r', 1) .attr('stroke', 'black') .attr("opacity", 1) .attr('fill', 'black'); svg.append("path") .attr("stroke", "black") .datum([{x: 320, y: 50}, {x: 301, y: 50}, {x: 301, y: 79}]) .attr("fill", "none") .attr("opacity", "0.8") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .attr("stroke", "black") .datum([{x: 320, y: 50}, {x: 220, y: 50}, {x: 201, y: 50}, {x: 201, y: 79}]) .attr("fill", "none") .attr("opacity", "0.8") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); d3.select("#ltnt_dffsn") .append("span") .text("\\( \\tau_\\theta \\)") .style("font-size", "14px") .style("font-weight", "700") .attr("font-family", "Arvo") .style("position", "absolute") .style("left", "515px") .style("top", "25px"); } latent_diffusion(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>The architecture of latent diffusion model.</em></p><p>In 2022 in collaboration with <a href="https://stability.ai/">Stability AI</a> and <a href="https://runwayml.com/">Runway</a> a text-to-image LDM called Stable Diffusion was released. Stable Diffusion was trained on 256x256 (then finetuned on 512x512) images from a subset of the <a href="https://laion.ai/blog/laion-aesthetics/">LAION-Aesthetics V2 dataset</a>, using 256 Nvidia A100 GPUs on Amazon Web Service for a total of 150,000 GPU-hours, at a cost of $600,000.</p><p>Similar to Google’s Imagen, this model uses a frozen CLIP ViT-L/14 text encoder to condition the model on text prompts. With its 860M UNet and 123M text encoder, the model is relatively lightweight and runs on a GPU with at least 10GB VRAM.</p><p><img data-proofer-ignore data-src="/assets/img/stable-diffusion-ex.png" alt="Stable-Diffusion-Examples" /> <em>Stable diffusion samples.</em></p><p>Unlike previous models, Stable Diffusion makes its <a href="https://github.com/CompVis/stable-diffusion">source code</a> available, along with pre-trained weights.</p><h3 id="conclusion">Conclusion</h3><p>Diffusion models have shown amazing capabilities as generative models. They are both analytically tractable and flexible: they can be analytically evaluated and cheaply fit arbitrary structures in data. Besides text-conditioned image generation there a lot of interesting characteristics of the models which were not covered in this post, such as image in-/outpainting, style transfer and image editing.</p><p>On the other hand, there is a shortcoming embedded into the diffusion process structure: the sampling relies on a long Markov chain of diffusion steps and it is still slower than GAN. Latent diffusion models have shown that performance gains can be achieved by learning semantically meaningful latent space with complex compressing encoders.</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/generative-ai/'>Generative AI</a>, <a href='/categories/diffusion-models/'>Diffusion Models</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/clip/" class="post-tag no-text-decoration" >clip</a> <a href="/tags/ddim/" class="post-tag no-text-decoration" >ddim</a> <a href="/tags/score-based-model/" class="post-tag no-text-decoration" >score-based model</a> <a href="/tags/guided-diffusion/" class="post-tag no-text-decoration" >guided diffusion</a> <a href="/tags/dall-e-2/" class="post-tag no-text-decoration" >dall·e 2</a> <a href="/tags/imagen/" class="post-tag no-text-decoration" >imagen</a> <a href="/tags/stable-diffusion/" class="post-tag no-text-decoration" >stable diffusion</a> <a href="/tags/latent-diffusion-model/" class="post-tag no-text-decoration" >latent diffusion model</a> <a href="/tags/jax/" class="post-tag no-text-decoration" >jax</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Power of Diffusion Models - AstraBlog&url=https://astralord.github.io/posts/power-of-diffusion-models/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Power of Diffusion Models - AstraBlog&u=https://astralord.github.io/posts/power-of-diffusion-models/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Power of Diffusion Models - AstraBlog&url=https://astralord.github.io/posts/power-of-diffusion-models/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://astralord.github.io/posts/power-of-diffusion-models/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/">Building Aligned Intelligence System. Part I: Creating GPT Assistant</a><li><a href="/posts/exploring-parallel-strategies-with-jax/">Exploring Parallel Strategies with Jax</a><li><a href="/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/">Building Aligned Intelligence System. Part II. Improving Large Language Models</a><li><a href="/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/">Visual Guide to Statistics. Part I: Basics of Point Estimation</a><li><a href="/posts/visual-guide-to-statistics-part-ii-bayesian-statistics/">Visual Guide to Statistics. Part II: Bayesian Statistics</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/parameter-estimation/">parameter estimation</a> <a class="post-tag" href="/tags/asymptotic-efficiency/">asymptotic efficiency</a> <a class="post-tag" href="/tags/bartlett-test/">bartlett test</a> <a class="post-tag" href="/tags/bayes-estimator/">bayes estimator</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/central-limit-theorem/">central limit theorem</a> <a class="post-tag" href="/tags/chain-of-thought/">chain-of-thought</a> <a class="post-tag" href="/tags/chatgpt/">chatgpt</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/exploring-parallel-strategies-with-jax/"><div class="card-body"> <span class="timeago small" >Jan 27<i class="unloaded">2024-01-27T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Exploring Parallel Strategies with Jax</h3><div class="text-muted small"><p> Training large language models either like GPT, LlaMa or Mixtral requires immense computational resources. With model sizes ballooning into the billions or sometimes even trillions of parameters...</p></div></div></a></div><div class="card"> <a href="/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/"><div class="card-body"> <span class="timeago small" >Jul 3, 2023<i class="unloaded">2023-07-03T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Building Aligned Intelligence System. Part I: Creating GPT Assistant</h3><div class="text-muted small"><p> In recent years, the field of natural language processing has witnessed a remarkable breakthrough with the advent of Large Language Models (LLMs). These models have demonstrated unprecedented pe...</p></div></div></a></div><div class="card"> <a href="/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/"><div class="card-body"> <span class="timeago small" >Jul 23, 2023<i class="unloaded">2023-07-23T19:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Building Aligned Intelligence System. Part II. Improving Large Language Models</h3><div class="text-muted small"><p> In this post we will look at different techniques for steering LLMs behaviour to get desired outcomes, starting with some basic general principles such as writing a good prompt and ending ...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/applying-graph-neural-networks-to-kaggle-competition/" class="btn btn-outline-primary" prompt="Older"><p>Applying Graph Neural Networks to Kaggle Competition</p></a> <a href="/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/" class="btn btn-outline-primary" prompt="Newer"><p>Building Aligned Intelligence System. Part I: Creating GPT Assistant</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/astralord">Aleksandr Samarin</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/parameter-estimation/">parameter estimation</a> <a class="post-tag" href="/tags/asymptotic-efficiency/">asymptotic efficiency</a> <a class="post-tag" href="/tags/bartlett-test/">bartlett test</a> <a class="post-tag" href="/tags/bayes-estimator/">bayes estimator</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/central-limit-theorem/">central limit theorem</a> <a class="post-tag" href="/tags/chain-of-thought/">chain of thought</a> <a class="post-tag" href="/tags/chatgpt/">chatgpt</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://astralord.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
