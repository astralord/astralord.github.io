<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Visual Guide to Statistics. Part III: Asymptotic Properties of Estimators" /><meta property="og:locale" content="en" /><meta name="description" content="A minimal condition for a good estimator is that it is getting closer to estimated parameter with growing size of sample vector. In this post we will focus on asymptotic properties of estimators." /><meta property="og:description" content="A minimal condition for a good estimator is that it is getting closer to estimated parameter with growing size of sample vector. In this post we will focus on asymptotic properties of estimators." /><link rel="canonical" href="https://astralord.github.io/posts/visual-guide-to-statistics-part-iii-asymptotic-properties-of-estimators/" /><meta property="og:url" content="https://astralord.github.io/posts/visual-guide-to-statistics-part-iii-asymptotic-properties-of-estimators/" /><meta property="og:site_name" content="AstraBlog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-04-12T06:00:00+03:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Visual Guide to Statistics. Part III: Asymptotic Properties of Estimators" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-07-03T23:09:13+03:00","datePublished":"2022-04-12T06:00:00+03:00","description":"A minimal condition for a good estimator is that it is getting closer to estimated parameter with growing size of sample vector. In this post we will focus on asymptotic properties of estimators.","headline":"Visual Guide to Statistics. Part III: Asymptotic Properties of Estimators","mainEntityOfPage":{"@type":"WebPage","@id":"https://astralord.github.io/posts/visual-guide-to-statistics-part-iii-asymptotic-properties-of-estimators/"},"url":"https://astralord.github.io/posts/visual-guide-to-statistics-part-iii-asymptotic-properties-of-estimators/"}</script><title>Visual Guide to Statistics. Part III: Asymptotic Properties of Estimators | AstraBlog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="AstraBlog"><meta name="application-name" content="AstraBlog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/marvel-icon.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">AstraBlog</a></div><div class="site-subtitle font-italic">A place to learn and share knowledge about AI-related things</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/astralord" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['samarin_ad','mail.ru'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/aleksandr-samarin-b8a35496" aria-label="linkedin" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://t.me/astrlrd" aria-label="telegram" class="order-6" target="_blank" rel="noopener"> <i class="fab fa-telegram"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Visual Guide to Statistics. Part III: Asymptotic Properties of Estimators</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Visual Guide to Statistics. Part III: Asymptotic Properties of Estimators</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Aleksandr Samarin </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Apr 12, 2022, 6:00 AM +0300" >Apr 12, 2022<i class="unloaded">2022-04-12T06:00:00+03:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Mon, Jul 3, 2023, 11:09 PM +0300" >Jul 3, 2023<i class="unloaded">2023-07-03T23:09:13+03:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2344 words">13 min read</span></div></div><div class="post-content"><blockquote><p>A minimal condition for a good estimator is that it is getting closer to estimated parameter with growing size of sample vector. In this post we will focus on asymptotic properties of estimators.</p></blockquote><h3 id="consistency-of-estimators">Consistency of estimators</h3><p>Berore talking about estimators convergence, let’s recall that there exist several different notions of convergence of random variables. Let $(X_n)$ be sequence of real-valued random variables, then we say</p><ul><li>$X_n$ <strong>converges in distribution</strong> towards the random variable $X$ if</ul>\[\lim\limits_{n \to \infty} F_{n}(x) = F(x),\]<p>for every $x \in \mathbb{R}$, at which $F$ is continuous. $F_n(x)$ and $F(x)$ are the cumulative distribution functions for $X_n$ and $X$ respectively. We denote convergence in distribution as $X_n \xrightarrow[]{\mathcal{L}} X$.</p><ul><li>$X_n$ <strong>converges in probability</strong> to random variable $X$ if</ul>\[\lim\limits_{n \to \infty} P(|X_n-X|&gt;\varepsilon)=0 \quad \forall \varepsilon &gt; 0.\]<p>Convergence in probability implies convergence in distribution. In the opposite direction, convergence in distribution implies convergence in probability when the limiting random variable $X$ is a constant. We denote convergence in probability as $X_n \xrightarrow[]{\mathbb{P}} X$.</p><ul><li>$X_n$ <strong>converges almost surely</strong> towards $X$ if</ul>\[P(\omega \in \Omega: \lim\limits_{n \to \infty} X_n(\omega) = X(\omega)) = 1.\]<p>Almost sure convergence implies convergence in probability, and hence implies convergence in distribution. Notation: $X_n \xrightarrow[]{\text{a.s.}} X$.</p><p>The similar logic can be applied to a sequence of $d$-dimensional random variables. Also, recall <a href="https://en.wikipedia.org/wiki/Continuous_mapping_theorem">continuous mapping theorem</a>, which states that for a continuous function $f$ we have</p>\[\begin{aligned} &amp;X_n \xrightarrow[]{\mathcal{L}} X \quad \Rightarrow \quad f(X_n) \xrightarrow[]{\mathcal{L}} f(X), \\ &amp;X_n \xrightarrow[]{\mathbb{P}} X \quad \Rightarrow \quad f(X_n) \xrightarrow[]{\mathbb{P}} f(X), \\ &amp;X_n \xrightarrow[]{\text{a.s.}} X \quad \Rightarrow \quad f(X_n) \xrightarrow[]{\text{a.s.}} f(X). \end{aligned}\]<p>Now let $g_n$ be an estimator of $\gamma(\vartheta)$ with values in metric space. Assume that all experiments are defined on a joint probability space $P_\vartheta$ for all $n$. We say that</p><ul><li>$g_n$ is <strong>(weakly) consistent</strong> if</ul>\[g_n \xrightarrow[]{\mathbb{P}}\gamma(\vartheta) \quad \forall \vartheta \in \Theta.\]<ul><li>$g_n$ is <strong>strongly constistent</strong> if</ul>\[g_n \xrightarrow[]{\text{a.s.}} \gamma(\vartheta) \quad \forall \vartheta \in \Theta.\]<p>Recall the method of moments from <a href="https://astralord.github.io/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/#common-estimation-methods">Part I</a>: $X_1, \dots, X_n$ i.i.d. $\sim P_\vartheta$, $\vartheta \in \Theta \subset \mathbb{R}^k$ and $\gamma: \Theta \rightarrow \Gamma \subset \mathbb{R}^l$. Also</p>\[m_j = \mathbb{E}_\vartheta[X_1^j] = \int x^j P_\vartheta(dx)\]<p>for $j = 1, \dots, k$, and</p>\[\gamma(\vartheta) = f(m_1, \dots, m_k).\]<p>Then choose</p>\[\hat{\gamma}(X) = f(\hat{m}_1, \dots, \hat{m}_k),\]<p>where</p>\[\hat{m}_j = \frac{1}{n} \sum_{i=1}^{n}X_k^j.\]<p>By Law of Large Numbers $\hat{m}_j \rightarrow m_j$ a.s. Since $f$ is continuous, we obtain</p>\[\hat{\gamma}(X) \xrightarrow[]{\text{a.s.}} \gamma(\vartheta).\]<p>Hence, $\hat{\gamma}(X)$ is a strongly consistent estimator.</p><h3 id="central-limit-theorem">Central Limit Theorem</h3><p>Let $(X_n)$ be a sequence of $d$-dimensional random variables. <a href="https://en.wikipedia.org/wiki/L%C3%A9vy%27s_continuity_theorem#:~:text=In%20probability%20theory%2C%20L%C3%A9vy's%20continuity,convergence%20of%20their%20characteristic%20functions.">Lévy’s continuity theorem</a> states that</p>\[X_n \xrightarrow[]{\mathcal{L}} X \quad \Longleftrightarrow \quad \mathbb{E}[\exp(iu^TX_n)] \rightarrow \mathbb{E}[\exp(iu^TX)] \quad \forall u \in \mathbb{R}^d.\]<p>If we write $u=ty$ for $t \in \mathbb{R}$, $y \in \mathbb{R}^d$, then we can say that $X_n \xrightarrow[]{\mathcal{L}} X$ if and only if</p>\[y^TX_n \xrightarrow[]{\mathcal{L}} y^TX \quad \forall y \in \mathbb{R}^d.\]<p>This statement is called <strong>Cramér–Wold theorem</strong>.</p><p>If $X_1, \dots, X_n$ are i.i.d. with $\mathbb{E}[X_j]=\mu \in \mathbb{R}^d$ and $\operatorname{Cov}(X_j)=\Sigma \in \mathbb{R}^{d \times d}$ (positive-definite, $\Sigma &gt; 0$), then for random vector</p>\[X^{(n)} = \frac{1}{n}\sum_{j=1}^n X_j \in \mathbb{R}^d\]<p>we know from one-dimensional Central Limit Theorem (CLT) that</p>\[\sqrt{n}(y^TX^{(n)} -y^T\mu) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, y^T\Sigma y) \quad \forall y \in \mathbb{R}^d.\]<p>Applying Cramér–Wold theorem we get</p>\[\sqrt{n}(X^{(n)}-\mu) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, \Sigma).\]<p>This statement is known as <strong>Multidimensional Central Limit Theorem</strong>.</p><style> .svg-container { display: inline-block; position: relative; width: 100%; padding-bottom: 100%; vertical-align: top; overflow: hidden; } .svg-content-responsive { display: inline-block; position: absolute; top: 10px; left: 0; } .ticks { font: 10px arvo; } .track, .track-inset, .track-overlay { stroke-linecap: round; } .track { stroke: #000; stroke-opacity: 0.8; stroke-width: 7px; } .track-inset { stroke: #ddd; stroke-width: 5px; } .track-overlay { pointer-events: stroke; stroke-width: 50px; stroke: transparent; } .handle { fill: #fff; stroke: #000; stroke-opacity: 0.8; stroke-width: 1px; } #sample-button { top: 15px; left: 15px; background: #65AD69; padding-right: 26px; border-radius: 3px; border: none; color: white; margin: 0; padding: 0 1px; width: 60px; height: 25px; font-family: Arvo; font-size: 11px; } #sample-button:hover { background-color: #696969; } #sample-button-2 { top: 15px; left: 15px; background: #65AD69; padding-right: 26px; border-radius: 3px; border: none; color: white; margin: 0; padding: 0 1px; width: 80px; height: 25px; font-family: Arvo; font-size: 11px; } #sample-button-2:hover { background-color: #696969; } #sample-button-3 { top: 15px; left: 15px; background: #65AD69; padding-right: 26px; border-radius: 3px; border: none; color: white; margin: 0; padding: 0 1px; width: 60px; height: 25px; font-family: Arvo; font-size: 11px; } #sample-button-3:hover { background-color: #696969; } #reset-button { top: 15px; left: 15px; background: #E86456; padding-right: 26px; border-radius: 3px; border: none; color: white; margin: 0; padding: 0 1px; width: 60px; height: 25px; font-family: Arvo; font-size: 11px; } #reset-button:hover { background-color: #696969; } #reset-button-2 { top: 15px; left: 15px; background: #E86456; padding-right: 26px; border-radius: 3px; border: none; color: white; margin: 0; padding: 0 1px; width: 60px; height: 25px; font-family: Arvo; font-size: 11px; } #reset-button-2:hover { background-color: #696969; }</style><script src="https://d3js.org/d3.v4.min.js"></script> <script src="https://d3js.org/d3-contour.v1.min.js"></script><link href="https://fonts.googleapis.com/css?family=Arvo" rel="stylesheet" /><p><button id="sample-button">Sample</button> <button id="sample-button-2">Sample 100x</button> <button id="reset-button">Reset</button></p><div id="mclt"></div><script> d3.select("#mclt") .style("position", "relative"); function createSlider(svg_, parameter_update, x, loc_x, loc_y, letter, color, init_val, round_fun) { var slider = svg_.append("g") .attr("class", "slider") .attr("transform", "translate(" + loc_x + "," + loc_y + ")"); var drag = d3.drag() .on("start.interrupt", function() { slider.interrupt(); }) .on("start drag", function() { handle.attr("cx", x(round_fun(x.invert(d3.event.x)))); parameter_update(round_fun(x.invert(d3.event.x))); }); slider.append("line") .attr("class", "track") .attr("x1", x.range()[0]) .attr("x2", x.range()[1]) .select(function() { return this.parentNode.appendChild(this.cloneNode(true)); }) .attr("class", "track-inset") .select(function() { return this.parentNode.appendChild(this.cloneNode(true)); }) .attr("class", "track-overlay") .call(drag); slider.insert("g", ".track-overlay") .attr("class", "ticks") .attr("transform", "translate(0," + 18 + ")") .selectAll("text") .data(x.ticks(6)) .enter().append("text") .attr("x", x) .attr("text-anchor", "middle") .attr("font-family", "Arvo") .text(function(d) { return d; }); var handle = slider.insert("circle", ".track-overlay") .attr("class", "handle") .attr("r", 6).attr("cx", x(init_val)); svg_ .append("text") .attr("text-anchor", "middle") .attr("y", loc_y + 3) .attr("x", loc_x - 21) .attr("font-family", "Arvo") .attr("font-size", 17) .text(letter) .style("fill", color); return handle; } function gamma_rand(k) { x = 0; for (var i = 0; i < k; i += 1) { x -= Math.log(Math.random()); } return x; } function dirichlet(ks) { xs = []; x0 = 0; for (var i = 0; i < ks.length; i += 1) { xs.push(gamma_rand(ks[i])); x0 += xs[i]; } return [xs[0] / x0, xs[1] / x0]; } function erf(x) { if (Math.abs(x) > 3) { return x / Math.abs(x); } var m = 1.00; var s = 1.00; var sum = x * 1.0; for(var i = 1; i < 50; i++){ m *= i; s *= -1; sum += (s * Math.pow(x, 2.0 * i + 1.0)) / (m * (2.0 * i + 1.0)); } return 1.1283791671 * sum; } function Phi(x) { return 0.5 * (1 + erf(x / 1.41421356237)); } function randn_bm() { var u = 0, v = 0; while(u === 0) u = Math.random(); while(v === 0) v = Math.random(); return Math.sqrt( -2.0 * Math.log( u ) ) * Math.cos( 2.0 * Math.PI * v ); } function biv_gauss(rho) { var z1 = randn_bm(); var z2 = rho * z1 + Math.sqrt(1 - rho * rho) * randn_bm(); return [z1, z2]; } function biv_uni(r) { var rho = 2 * Math.sin(r * Math.PI / 6); var z = biv_gauss(rho); return [erf(z[0]), erf(z[1])]; } function phi(x, mu, sigma) { var y = (x - mu) / sigma; y *= y; y = Math.exp(-y / 2); y /= (sigma * 1.41421356237 * Math.PI); return y; } function mclt() { var n = 7, rho = 0; const margin = {top: 20, right: 0, bottom: 5, left: 70}, width = 750 - margin.left - margin.right, height = 400 - margin.top - margin.bottom, fig_height = 200, fig_width = 250, fig_margin = 150, fig_trans = fig_width + fig_margin; const avg_dur = 1000; var svg = d3.select("div#mclt") .append("svg") .attr("width", width + margin.left + margin.right) .attr("height", height + margin.top + margin.bottom) .append("g") .attr("transform", "translate(" + margin.left + "," + margin.top + ")"); var x = d3.scaleLinear() .range([0, fig_width]) .domain([-1, 1]); var xAxis = svg.append("g") .attr("transform", "translate(0," + fig_height + ")") .call(d3.axisBottom(x).ticks(4)); xAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var xAvg = d3.scaleLinear() .range([fig_trans, 2 * fig_width + fig_margin]) .domain([-3, 3]); var xAvgAxis = svg.append("g") .attr("transform", "translate("+ 0 + "," + fig_height + ")") .call(d3.axisBottom(xAvg).ticks(5)); xAvgAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var y = d3.scaleLinear() .range([fig_height, 0]) .domain([-1, 1]); var yAxis = svg.append("g") .call(d3.axisLeft(y).ticks(4)); yAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var yAvg = d3.scaleLinear() .range([fig_height, 0]) .domain([-3, 3]); var yAvgAxis = svg.append("g") .attr("transform", "translate("+ fig_trans + ",0)") .call(d3.axisLeft(yAvg).ticks(5)); yAvgAxis.selectAll(".tick text") .attr("font-family", "Arvo"); const axs_mrgn = 0.25; const uni_data = [{x: -1, y: -1.25}, {x: -1, y: -1.5}, {x: 1, y: -1.5}, {x: 1, y: -1.25}]; var uni_x_curve = svg .append('g') .append("path") .datum(uni_data) .attr("fill", "#65AD69") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .x(function(d) { return x(d.x); }) .y(function(d) { return y(d.y); }) ); var uni_y_curve = svg .append('g') .append("path") .datum(uni_data) .attr("fill", "#65AD69") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .x(function(d) { return x(d.y); }) .y(function(d) { return y(d.x); }) ); var gauss_data = []; var mu = 0, sigma = 1 / Math.sqrt(3); var scale = 3 * axs_mrgn * (sigma * 1.41421356237 * Math.PI); for (var i = -3; i <= 3; i += 0.01) { gauss_data.push({x: i, y: -3.75 - scale * phi(i, mu, sigma)}); } var gauss_x_curve = svg .append('g') .append("path") .datum(gauss_data) .attr("fill", "#E86456") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .x(function(d) { return xAvg(d.x); }) .y(function(d) { return yAvg(d.y); }) ); var gauss_y_curve = svg .append('g') .append("path") .datum(gauss_data) .attr("fill", "#E86456") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .x(function(d) { return xAvg(d.y); }) .y(function(d) { return yAvg(d.x); }) ); var avg_dots = []; function sampleUniform() { var uni_data = [], uni_dots = []; var avg_x = 0, avg_y = 0; var sqrt_n = Math.sqrt(n); for (var i = 0; i < n; i += 1) { var uni_point = biv_uni(rho); uni_data.push({x: uni_point[0], y: uni_point[1]}); avg_x += uni_data[i].x; avg_y += uni_data[i].y; } avg_x /= n; avg_y /= n; for (var i = 0; i < n; i += 1) { uni_dots.push(svg.append('g') .selectAll("dot") .data([uni_data[i]]) .enter() .append("circle") .attr("cx", function (d) { return x(d.x); } ) .attr("cy", function (d) { return y(d.y); } ) .attr("r", 0) .style("fill", "#65AD69") .attr("stroke", "#000") .attr("stroke-width", 1)); uni_dots[i] .transition() .duration(avg_dur) .attr("r", 3); uni_dots[i] .transition() .duration(avg_dur) .delay(avg_dur) .style("fill", "#E86456") .attr("cx", function (d) { return x(avg_x); } ) .attr("cy", function (d) { return y(avg_y); } ); if (i > 0) { uni_dots[i].transition().delay(2 * avg_dur) .attr("opacity", 0); uni_dots[i].transition().delay(2 * avg_dur) .remove(); } } avg_dots.push(uni_dots[0]); avg_dots[avg_dots.length - 1] .transition() .duration(avg_dur) .delay(2 * avg_dur) .attr("cx", function (d) { return xAvg(sqrt_n * avg_x); } ) .attr("cy", function (d) { return yAvg(sqrt_n * avg_y); } ); } function reset() { for (var i = 0; i < avg_dots.length; i += 1) { avg_dots[i].remove(); } avg_dots = []; } var sampleButton = d3.select("#sample-button") .on("click", function() { sampleUniform(); }); var sampleButton = d3.select("#sample-button-2") .on("click", function() { for (var i = 0; i < 100; i += 1) { sampleUniform(); } }); var resetButton = d3.select("#reset-button") .on("click", function() { reset(); }); var rho_x = d3.scaleLinear() .domain([-1, 1]) .range([0, width / 4]) .clamp(true); function trivialRound(x) { return x; } function updateRho(r) { rho = r; reset(); } createSlider(svg, updateRho, rho_x, 50, 0.95 * height, "", "#65AD69", rho, trivialRound); var n_x = d3.scaleLinear() .domain([1, 12]) .range([0, width / 4]) .clamp(true); function roundN(x) { return Math.round(x - 0.5); } function updateN(num) { n = num; reset(); } createSlider(svg, updateN, n_x, 3 * fig_width / 2 + 70, 0.95 * height, "n", "#696969", n, roundN); d3.select("#mclt") .append("div") .text("\\(\\rho \\)") .style('color', '#696969') .style("font-size", "17px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", 95 + "px") .style("top", 0.95 * height + 3 + "px"); d3.select("#mclt") .append("div") .text("\\(X \\sim \\mathcal{U}(-1, 1), \\quad \\Sigma = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\)") .style('color', '#65AD69') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", fig_width / 2 - 35 + "px") .style("top", 0.78 * height + "px"); d3.select("#mclt") .append("div") .text("\\(\\sqrt{n} (X^{(n)}-\\mu) \\sim \\mathcal{N}(0, \\Sigma) \\)") .style('color', '#E86456') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", 3 * fig_width / 2 + 130 + "px") .style("top", 0.8 * height + "px"); } mclt(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Fig. 1. Visualization of multidimensional CLT for two-dimensional case. On the left-hand side there is random vector of two uniformly distributed random variables: $X_1, X_2 \sim \mathcal{U}(-1, 1)$ with mean $\mu=(0, 0)^T$ and correlation $\rho$. On the right-hand side is $\sqrt{n} X^{(n)}$ which for large $n$ has approximately normal distribution with zero mean and the same covariance as $X$.</em></p><h3 id="delta-method">Delta-method</h3><p>Let $(X_n)$ and $(Y_n)$ be sequences of $d$-dimensional random variables, such that</p>\[X_n \xrightarrow[]{\mathcal{L}} X \quad \text{and} \quad Y_n \xrightarrow[]{\mathbb{P}} c\]<p>for some constant vector $c$. Then we can apply the continuous mapping theorem, recognizing the functions $f(x, y)=x+y$ and $f(x, y)=xy$ are continuous, and conclude that</p><ul><li>$X_n+Y_n \xrightarrow[]{\mathcal{L}} X + c,$<li>$Y_n^TX_n \xrightarrow[]{\mathcal{L}} c^TX.$</ul><p>This statement is called <strong>Slutsky’s lemma</strong> and it can be extremely useful in estimating approximate distribution of estimators. For example, let $X_1, \dots X_n$ i.i.d. $\sim \operatorname{Bin}(1, p)$. Estimator of $p$ $g_n(X) = \overline{X}_n$ is unbiased and we know from CLT that</p>\[\sqrt{\overline{X}_n(1-\overline{X}_n)} \xrightarrow[]{\mathbb{P}} \sqrt{p(1-p)}.\]<p>By Slutsky’s lemma,</p>\[\frac{\sqrt{n}(\overline{X}_n-p)}{\sqrt{\overline{X}_n(1-\overline{X}_n)}} \xrightarrow[]{\mathcal{L}} \mathcal{N}(0,1)\]<p>and for large $n$ we have</p>\[P_p(|\overline{X}_n-p|&lt;\varepsilon) \approx 2 \Phi\Bigg(\varepsilon\sqrt{\frac{n}{\overline{X}_n(1-\overline{X}_n)}}\Bigg) -1 \quad \forall p \in (0, 1),\]<p>where $\Phi$ is cumulative distribution function for $\mathcal{N}(0,1)$.</p><p>Slutsky’s lemma also leads to important asymptotic property of estimator $g_n$, called <strong>Delta-method</strong>. Let $(X_n)$ be sequence of $d$-dimensional random variables, such that</p>\[\frac{X_n-\mu}{c_n} \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, \Sigma),\]<p>where $c_n \rightarrow 0$, $\mu \in \mathbb{R}^d$ и $\Sigma \geq 0 \in \mathbb{R}^{d \times d}$. Let also $g:\mathbb{R}^d \rightarrow \mathbb{R}^m$ be continuously differentiable in $\mu$ with Jacobian matrix $D \in \mathbb{R}^{m \times d}$. Then:</p>\[\frac{g(X_n)-g(\mu)}{c_n} \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, D\Sigma D^T).\] <details> <summary>Proof</summary> By Slutsky's Lemma $$X_n-\mu = \frac{X_n-\mu}{c_n}c_n \xrightarrow[]{\mathcal{L}} 0.$$ Convergence in distribution to a constant implies convergence in probability: $X_n \xrightarrow[]{\mathbb{P}} \mu$. Then $$\frac{g(X_n)-g(\mu)}{c_n}=g'(\mu)\frac{X_n-\mu}{c_n}+(g'(\xi_n)-g'(\mu))\frac{X_n-\mu}{c_n},$$ for some intermediate point $\xi_n$, such that $\|\xi_n-\mu \| \leq \|X_n-\mu \|$. From $X_n \xrightarrow[]{\mathbb{P}} \mu$ we have $\xi_n \xrightarrow[]{\mathbb{P}} \mu$ and $g'(\xi_n) \xrightarrow[]{\mathbb{P}} g'(\mu)$ (because $g$ is continuously differentiable). Applying again Slutsky's Lemma: $$ g'(\mu) \frac{X_n-\mu}{c_n} \xrightarrow[]{\mathcal{L}} g'(\mu) \cdot \mathcal{N}(0, \Sigma) $$ finishes the proof. </details><ul><li>Recall example with method of moments, but now with additional conditions on $\mathbb{E}[X_1^{2k}] &lt; \infty$ for all $\vartheta \in \Theta$ and $\gamma$ being continuously differentiable with Jacobian matrix $D$. We know from CLT that</ul>\[\sqrt{n}((\hat{m}_1, \dots, \hat{m}_k)^T - (m_1, \dots, m_k)^T) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, \Sigma),\]<p>where</p>\[\Sigma = (\Sigma)_{i,j=1}^k = (m_{i+j} - m_i m_j)_{i,j=1}^k.\]<p>Then</p>\[\sqrt{n}(\gamma(\hat{m}_1, \dots, \hat{m}_k) - \gamma(m_1, \dots, m_k)) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, D \Sigma D^T).\]<ul><li>Take another example: let $X_1, \dots X_n$ be i.i.d. with</ul>\[\mathbb{E}_\vartheta[X_i] = \mu \quad \text{and} \quad \operatorname{Var}_\vartheta(X_i) = \sigma^2.\]<p>From CLT we have</p>\[\sqrt{n}(\overline{X}_n - \mu) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, \sigma^2).\]<p>Choose $\overline{X}_n^2$ as an estimator for $\mu^2$. Applying Delta-method we get</p>\[\sqrt{n}(\overline{X}_n^2-\mu^2) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, 4\mu^2\sigma^2).\]<ul><li>Let</ul>\[(X_i, Y_i)^T \sim \mathcal{N} \begin{pmatrix} \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}, \begin{pmatrix} \sigma^2 &amp; \rho \sigma \tau \\ \rho \sigma \tau &amp; \tau^2 \end{pmatrix} \end{pmatrix}, \quad i = 1, \dots, n,\]<p>be i.i.d with parameter $\vartheta = (\mu_1, \mu_2, \sigma^2, \tau^2, \rho)^T$. The estimator</p>\[\hat{\rho}_n = \frac{SQ_{xy}}{\sqrt{SQ_{xx} SQ_{yy}}},\]<p>where</p>\[SQ_{xy} = \frac{1}{n} \sum_{i=1}^{n}(X_i-\overline{X}_n)(Y_i - \overline{Y}_n),\]<p>$SQ_{xx}, SQ_{yy}$ - likewise, is called <strong>the Pearson correlation coefficient</strong>. Without loss of generality, assume $\mu_1=\mu_2=0$, $\sigma=\tau=1$, because $\hat{\rho}_n$ is invariant under affine transformation.</p><p>Prove first that $S_n = (SQ_{xx}, SQ_{yy}, SQ_{xy})^T$ satisifies</p>\[\sqrt{n}(S_n - m) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, V),\]<p>where $m=(1, 1, \rho)^T$ and</p>\[V = 2 \begin{pmatrix} 1 &amp; \rho^2 &amp; \rho \\ \rho^2 &amp; 1 &amp; \rho \\ \rho &amp; \rho &amp; (1 + \rho^2)/2 \end{pmatrix}.\] <details> <summary> Sketch of the proof </summary> We use Slutsky's Lemma and CLT to show that $$\sqrt{n}(\overline{X}_n \overline{Y}_n) \xrightarrow[]{\mathbb{P}} 0, \quad \sqrt{n}(\overline{X}_n)^2 \xrightarrow[]{\mathbb{P}} 0, \quad \sqrt{n}(\overline{Y}_n)^2 \xrightarrow[]{\mathbb{P}} 0. $$ Then it is simple to conclude $$\sqrt{n}(S_n - m) - \sqrt{n}\Big(\frac{1}{n}\sum_{i=1}^{n}Z_i - m \Big) \xrightarrow[]{\mathbb{P}} 0,$$ with $Z_i = (X_i^2, Y_i^2, X_iY_i)^T$. Then prove that $$\operatorname{Cov}(Z_i) = \mathbb{E}[Z_i Z_i^T]-\mathbb{E}[Z_i]\mathbb{E}[Z_i]^T = V. $$ The rest follows from multidimensional CLT. </details><p>Then take $g(S_n)=\hat{\rho}_n$ with $g(x_1, x_2, x_3) = \frac{x_3}{\sqrt{x_1 x_2}}$. Jacobian matrix of $g$ at $m$:</p>\[D = (-\rho/2, -\rho/2, 1).\]<p>In total,</p>\[\sqrt{n}(\hat{\rho}_n - \rho) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, DVD^T) = \mathcal{N}(0, (1-\rho^2)^2).\]<p><button id="sample-button-3">Sample</button> <button id="reset-button-2">Reset</button></p><div id="prsn_plt"></div><script> d3.select("#prsn_plt") .style("position", "relative"); function prsn_plt() { var n = 10, rho = 0, rho_n = 0; var sqxx = 0, sqyy = 0, sqxy = 0; const margin = {top: 20, right: 0, bottom: 5, left: 70}, width = 750 - margin.left - margin.right, height = 400 - margin.top - margin.bottom, fig_height = 200, fig_width = 250, fig_margin = 150, fig_trans = fig_width + fig_margin; const avg_dur = 1000; var svg = d3.select("div#prsn_plt") .append("svg") .attr("width", width + margin.left + margin.right) .attr("height", height + margin.top + margin.bottom) .append("g") .attr("transform", "translate(" + margin.left + "," + margin.top + ")"); var x = d3.scaleLinear() .range([0, fig_width]) .domain([-3, 3]); var xAxis = svg.append("g") .attr("transform", "translate(0," + fig_height + ")") .call(d3.axisBottom(x).ticks(5)); xAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var xRho = d3.scaleLinear() .range([fig_trans, 2 * fig_width + fig_margin]) .domain([-3, 3]); var xRhoAxis = svg.append("g") .attr("transform", "translate("+ 0 + "," + fig_height + ")") .call(d3.axisBottom(xRho).ticks(5)); xRhoAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var y = d3.scaleLinear() .range([fig_height, 0]) .domain([-3, 3]); var yAxis = svg.append("g") .call(d3.axisLeft(y).ticks(5)); yAxis.selectAll(".tick text") .attr("font-family", "Arvo"); var yRho = d3.scaleLinear() .range([fig_height, 0]) .domain([0, 0.25]); var yRhoAxis = svg.append("g") .attr("transform", "translate("+ fig_trans + ",0)") .call(d3.axisLeft(yRho).ticks(5)); yRhoAxis.selectAll(".tick text") .attr("font-family", "Arvo"); const axs_mrgn = 0.25; var gauss_data = []; var mu = 0, sigma = 1; var scale = 3 * axs_mrgn * (sigma * 1.41421356237 * Math.PI); for (var i = -3; i <= 3; i += 0.01) { gauss_data.push({x: i, y: -3.75 - scale * phi(i, mu, sigma)}); } var gauss_x_curve = svg .append('g') .append("path") .datum(gauss_data) .attr("fill", "#65AD69") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .x(function(d) { return x(d.x); }) .y(function(d) { return y(d.y); }) ); var gauss_y_curve = svg .append('g') .append("path") .datum(gauss_data) .attr("fill", "#65AD69") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .x(function(d) { return x(d.y); }) .y(function(d) { return y(d.x); }) ); function getGaussRhoData() { var sigma_rho = Math.max(1 - rho ** 2, 1e-6); var gauss_rho_data = [{x: -3, y: 0}]; for (var i = -3; i <= 3; i += 0.01) { gauss_rho_data.push({x: i, y: phi(i, 0, sigma_rho)}); } gauss_rho_data.push({x: 3, y: 0}); return gauss_rho_data; } var gauss_rho_data = getGaussRhoData(); var gauss_rho_curve = svg .append('g') .append("path") .datum(gauss_rho_data) .attr("fill", "#E86456") .attr("border", 0) .attr("opacity", ".8") .attr("stroke", "#000") .attr("stroke-width", 1) .attr("stroke-linejoin", "round") .attr("d", d3.line() .x(function(d) { return xRho(d.x); }) .y(function(d) { return yRho(d.y); }) ); function updateRhoCurve() { var gauss_rho_data = getGaussRhoData(); yRho.domain([0, Math.max(0.25, d3.max(gauss_rho_data, function(d) { return d.y })) ]); yRhoAxis .transition() .duration(avg_dur) .call(d3.axisLeft(yRho).ticks(5)) .selectAll(".tick text") .attr("font-family", "Arvo"); gauss_rho_curve.datum(gauss_rho_data) .transition() .duration(avg_dur) .attr("d", d3.line() .x(function(d) { return xRho(d.x); }) .y(function(d) { return yRho(d.y); }) ); } function estimate_rho() { var avg_x = 0, avg_y = 0; for (var i = 0; i < n; i += 1) { avg_x += x.invert(gauss_dots[i].attr("cx")); avg_y += y.invert(gauss_dots[i].attr("cy")); } avg_x /= n; avg_y /= n; sqxx = 0, sqyy = 0, sqxy = 0; for (var i = 0; i < n; i += 1) { sqxx += (x.invert(gauss_dots[i].attr("cx")) - avg_x) ** 2; sqyy += (y.invert(gauss_dots[i].attr("cy")) - avg_y) ** 2; sqxy += (x.invert(gauss_dots[i].attr("cx")) - avg_x) * (y.invert(gauss_dots[i].attr("cy")) - avg_y); } return sqxy / Math.sqrt(sqxx * sqyy); } var sqxx_text = svg .append("text") .attr("text-anchor", "start") .attr("y", 45) .attr("x", fig_width + 65) .attr("font-family", "Arvo") .attr("font-size", 12) .style("fill", "#EDA137"); var sqyy_text = svg .append("text") .attr("text-anchor", "start") .attr("y", 70) .attr("x", fig_width + 65) .attr("font-family", "Arvo") .attr("font-size", 12) .style("fill", "#EDA137"); var sqxy_text = svg .append("text") .attr("text-anchor", "start") .attr("y", 95) .attr("x", fig_width + 65) .attr("font-family", "Arvo") .attr("font-size", 12) .style("fill", "#EDA137"); var rho_text = svg .append("text") .attr("text-anchor", "start") .attr("y", 120) .attr("x", fig_width + 60) .attr("font-family", "Arvo") .attr("font-size", 12) .style("fill", "#EDA137"); function updateText() { sqxx_text .transition() .duration(500) .text(" = " + Math.round(100 * sqxx / n) / 100); sqyy_text .transition() .duration(500) .text(" = " + Math.round(100 * sqyy / n) / 100); sqxy_text .transition() .duration(500) .text(" = " + Math.round(100 * sqxy / n) / 100); rho_text .transition() .duration(500) .text(" = " + Math.round(100 * rho_n) / 100); } function update_rho_n() { var sqrt_n = Math.sqrt(n); rho_n = estimate_rho(); rho_dots[rho_dots.length - 1] .transition() .duration(avg_dur) .attr("cx", function (d) { return xRho(sqrt_n * (rho_n - rho)); } ); updateText(); } function drag(d) { d3.select(this) .attr("cx", d.x = d3.event.x) .attr("cy", d.y = d3.event.y); update_rho_n(); } var gauss_data = []; var gauss_dots = []; var rho_dots = []; function sampleGauss() { if (gauss_dots.length > 0) { for (var i = 0; i < gauss_dots.length; i += 1) { gauss_dots[i].remove(); } gauss_dots = []; } gauss_data = []; var sqrt_n = Math.sqrt(n); for (var i = 0; i < n; i += 1) { var rnd_pnt = biv_gauss(rho); gauss_data.push({x: rnd_pnt[0], y: rnd_pnt[1]}); } for (var i = 0; i < n; i += 1) { gauss_dots.push(svg.append('g') .selectAll("dot") .data([{x: x(gauss_data[i].x), y: y(gauss_data[i].y)}]) .enter() .append("circle") .attr("cx", function (d) { return d.x; } ) .attr("cy", function (d) { return d.y; } ) .attr("r", 0) .style("fill", "#65AD69") .attr("stroke", "#000") .attr("stroke-width", 1) .on("mouseover", function(d) { d3.select(this) .style("cursor", "pointer");}) .on("mousemove", function (d) {}) .call(d3.drag().on("drag", drag)) ); gauss_dots[i] .transition() .duration(avg_dur) .attr("r", 3); } rho_n = estimate_rho(); var rho_dot = svg.append('g') .selectAll("dot") .data([{x: sqrt_n * (rho_n - rho), y: 0}]) .enter() .append("circle") .attr("cx", function (d) { return xRho(d.x); } ) .attr("cy", function (d) { return yRho(d.y); } ) .attr("r", 0) .style("fill", "#E86456") .attr("stroke", "#000") .attr("stroke-width", 1); rho_dot.transition().duration(avg_dur).attr("r", 3); if (rho_dots.length > 0) { rho_dots[rho_dots.length - 1] .transition() .attr("opacity", 0.5) .attr("r", 3); } rho_dots.push(rho_dot); updateText(); } function reset() { for (var i = 0; i < rho_dots.length; i += 1) { rho_dots[i].remove(); } for (var i = 0; i < gauss_dots.length; i += 1) { gauss_dots[i].remove(); } rho_dots = []; gauss_dots = []; sqxx = sqxy = sqyy = rho_n = 0; updateText(); } d3.select("#sample-button-3") .on("click", function() { sampleGauss(); }); d3.select("#reset-button-2") .on("click", function() { reset(); }); var rho_x = d3.scaleLinear() .domain([-1, 1]) .range([0, width / 4]) .clamp(true); function trivialRound(x) { return x; } function updateRho(r) { rho = r; updateRhoCurve(); reset(); } createSlider(svg, updateRho, rho_x, 50, 0.95 * height, "", "#65AD69", rho, trivialRound); var n_x = d3.scaleLinear() .domain([2, 100]) .range([0, width / 4]) .clamp(true); function roundN(x) { return Math.round(x - 0.5); } function updateN(num) { n = num; reset(); } createSlider(svg, updateN, n_x, 3 * fig_width / 2 + 70, 0.95 * height, "n", "#696969", n, roundN); d3.select("#prsn_plt") .append("div") .text("\\(\\rho \\)") .style('color', '#696969') .style("font-size", "17px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", 95 + "px") .style("top", 0.95 * height + 3 + "px"); d3.select("#prsn_plt") .append("div") .text("\\(X \\sim \\mathcal{N}(0, \\Sigma), \\quad \\Sigma = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\)") .style('color', '#65AD69') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", fig_width / 2 - 35 + "px") .style("top", 0.78 * height + "px"); d3.select("#prsn_plt") .append("div") .text("\\(\\sqrt{n} (\\hat{\\rho}_n-\\rho) \\sim \\mathcal{N}(0, (1-\\rho^2)^2) \\)") .style('color', '#E86456') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", 3 * fig_width / 2 + 120 + "px") .style("top", 0.8 * height + "px"); d3.select("#prsn_plt") .append("div") .text("\\(SQ_{xx} \\)") .style('color', '#EDA137') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", fig_width + 100 + "px") .style("top", 50 + "px"); d3.select("#prsn_plt") .append("div") .text("\\(SQ_{yy} \\)") .style('color', '#EDA137') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", fig_width + 100 + "px") .style("top", 75 + "px"); d3.select("#prsn_plt") .append("div") .text("\\(SQ_{xy} \\)") .style('color', '#EDA137') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", fig_width + 100 + "px") .style("top", 100 + "px"); d3.select("#prsn_plt") .append("div") .text("\\( \\hat{\\rho}_n \\)") .style('color', '#EDA137') .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .attr("font-weight", 700) .style("position", "absolute") .style("left", fig_width + 100 + "px") .style("top", 125 + "px"); } prsn_plt(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Fig. 2. Visualization of asymptotic normality for Pearson correlation coefficient. Drag sample dots to observe how it affects $SQ$ coefficients and $\hat{\rho}_n$.</em></p><h3 id="asympotic-efficiency">Asympotic efficiency</h3><p>Let $g_n \subset \mathbb{R}^l$ be a sequence of estimators with</p>\[\mu_n(\vartheta)=\mathbb{E}_\vartheta[g_n] \in \mathbb{R}^l \quad \text{and} \quad \Sigma_n(\vartheta)=\operatorname{Cov}(\vartheta) \in \mathbb{R}^{l \times l},\]<p>such that $\lVert \Sigma_n(\vartheta) \rVert \rightarrow 0$. Then</p><ul><li>$g_n$ is called <strong>asymptotically unbiased</strong> for $\gamma(\vartheta)$ if</ul>\[\mu_n(\vartheta) \rightarrow \gamma(\vartheta),\]<ul><li>$g_n$ is called <strong>asymptotically normal</strong> if</ul>\[\Sigma_n^{-\frac{1}{2}}(\vartheta)(g_n-\mu_n(\vartheta)) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, \mathbb{I}_l),\]<p>where $\mathbb{I}_l$ is identity matrix.</p><p>Let $f_n: \mathcal{X} \rightarrow \mathbb{R}^l$ be asymptotically unbiased and asymptotically normal sequence of estimators. Under regularity conditions from <a href="https://astralord.github.io/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/#efficient-estimator">Cramér–Rao theorem</a> we call $g_n$ <strong>asymptotically efficient</strong>, if</p>\[\lim\limits_{n \rightarrow \infty} \Sigma_n(\vartheta) \mathcal{I}(f_n(\cdot, \vartheta))=\mathbb{I}_l \quad \forall \vartheta \in \Theta,\]<p>where $\mathcal{I}(f_n(\cdot, \vartheta))$ is Fisher information.</p><p>The intuition behind definition above is the following: if $g_n$ is unbiased, then by Cramér–Rao theorem $\operatorname{Cov}_\vartheta(g_n) \geq \mathcal{I}^{-1}(f_n(\cdot, \vartheta))$. Due to asymptotic normality:</p>\[\Sigma_n^{-\frac{1}{2}}(\vartheta)(g_n-\mu_n(\vartheta)) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, \mathbb{I}_l)\]<p>we have approximately</p>\[\operatorname{Cov}_\vartheta(g_n) \approx \Sigma_n(\vartheta) \approx \mathcal{I}^{-1}(f_n(\cdot, \vartheta))\]<p>and $g_n$ is asymptotically unbiased and asymptotically efficient.</p><p>Recall example from <a href="https://astralord.github.io/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/#multidimensional-cram%C3%A9rrao-inequality">Part I</a>: for $X_1, \dots X_n$ i.i.d. $\sim \mathcal{N}(\mu, \sigma^2)$ estimator</p>\[g_n(X) = \begin{pmatrix} \overline{X}_n \\ \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \overline{X}_n)^2 \end{pmatrix}\]<p>satisfies the equality</p>\[\operatorname{Cov}_\vartheta(g_n) = \begin{pmatrix} \sigma^2/n &amp; 0 \\ 0 &amp; 2\sigma^4 / (n - 1) \end{pmatrix} = \Sigma_n(\vartheta).\]<p>But Fisher information is</p>\[\mathcal{I}^{-1}(f_n(\cdot, \vartheta)) = \begin{pmatrix} \sigma^2/n &amp; 0 \\ 0 &amp; 2\sigma^4 / n \end{pmatrix}\]<p>and $g_n$ is not efficient, but asymptotically efficient.</p><h3 id="asymptotic-properties-of-maximum-likelihood-estimators">Asymptotic properties of maximum-likelihood estimators</h3><p>In <a href="https://astralord.github.io/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/#common-estimation-methods">Part I</a> we briefly mentioned maximum-likelihood estimators as one of the most common estimation methods in statistic. It is worth knowing what their asymptotic properties are. Let’s rewrite the definition here: let $X_1, \dots X_n$ be i.i.d. $\sim P_\vartheta$, $\vartheta \in \Theta$ with densities $f(\cdot, \vartheta)$. We call</p>\[\ell(\cdot, \vartheta) = \log f(\cdot, \vartheta)\]<p><strong>the log-likelihood function</strong> and set</p>\[\begin{aligned}\hat{\theta}_n(X) &amp;= \arg \sup_{\vartheta \in \Theta} f(X, \vartheta) \\&amp;= \arg \sup_{\vartheta \in \Theta} \ell (X, \vartheta) \\&amp;= \arg \sup_{\vartheta \in \Theta} \frac{1}{n} \sum_{i=1}^{n} \ell (X_i, \vartheta) \end{aligned}\]<p>as <strong>the maximum-likelihood estimator</strong> for $\vartheta$.</p><p>Now, say the following conditions are satisfied:</p><ol><li>$\Theta \subset \mathbb{R}^k$ is compact space<li>$L(\eta, \vartheta) = \mathbb{E}[\ell(X_i, \eta)]$ and $L_n(\eta) = \frac{1}{n}\sum_{i=1}^n\ell(X_i, \eta)$ are a.s. continuous functions over $\eta$.<li> \[\sup_{\eta \in \Theta} | L_n(\eta)-L(\eta, \vartheta)|\xrightarrow{\mathcal{L}}0.\]</ol><p>Then maximum-likelihood estimator $\hat{\theta}_n$ is consistent.</p><p>Proof: for any $\eta \in \Theta$:</p>\[L(\eta, \vartheta) = \int \ell(x, \eta) f(x,\vartheta) dx = \int \ell(x,\vartheta)f(x,\vartheta)dx - KL(\vartheta | \eta),\]<p>where $KL$ is <strong>Kullback-Leibler divergence</strong>:</p>\[KL(\vartheta | \eta) = \int_{\mathcal{X}} \log\Big(\frac{f(x,\vartheta)}{f(x,\eta)}\Big) f(x,\vartheta)dx.\]<p>It can be shown that</p>\[\begin{aligned} KL(\vartheta | \eta) &amp; = \int_{\mathcal{X}} -\log\Big(\frac{f(x,\eta)}{f(x,\vartheta)}\Big) f(x,\vartheta)dx \\ \color{\Salmon}{\text{Jensen inequality} \rightarrow } &amp; \geq -\log\int_{\mathcal{X}} \frac{f(x,\eta)}{f(x,\vartheta)} f(x,\vartheta)dx \\ &amp; = 0, \end{aligned}\]<p>and it turns into equality only when $f(x,\vartheta) = f(x,\eta)$ for almost every $x$. Therefore we conclude that $L(\eta, \vartheta)$ reaches maximum at $\eta = \vartheta$.</p><p>Using the fact that function $m_f = \arg\max_{\eta \in \Theta} f(\eta)$ is continuous if $m_f$ is unique, we finish the proof from</p>\[\vartheta = \arg \max L(\eta, \vartheta)\quad \text{and} \quad \hat{\theta}_n=\arg \max L_n(\eta)\]<p>and condition 3.</p><h3 id="asymptotic-efficiency-of-maximum-likelihood-estimators">Asymptotic efficiency of maximum-likelihood estimators</h3><p>If the following conditions are satisfied:</p><ul><li>$\Theta \subset \mathbb{R}^k$ is compact and $\vartheta \subset \operatorname{int}(\Theta)$.<li>$\ell(x, \eta)$ is continuous $\forall \eta \in \Theta$ and twice continuously differentiable over $\vartheta$ for almost every $x \in \mathcal{X}$.<li>$\ell(x, \eta)$ is Lipschitz: there exist functions $H_0, H_2 \in L^1(P_\vartheta)$ and $H_1 \in L^2(P_\vartheta)$, such that:</ul>\[\sup_{\eta \in \Theta} \|\ell(x, \eta)\| \leq H_0(x), \quad \sup_{\eta \in \Theta} \|\dot{\ell}(x, \eta)\| \leq H_1(x), \quad \sup_{\eta \in \Theta} \|\ddot{\ell}(x, \eta)\| \leq H_2(x) \quad \forall x \in \mathcal{X}.\]<ul><li>Fisher information</ul>\[\mathcal{I}(f(\cdot, \vartheta))=\mathbb{E}_\vartheta[\dot{\ell}(X,\vartheta)\dot{\ell}(X,\vartheta)^T]\]<p>is positive definite (and therefore invertible),</p><p>then $\hat{\theta}_n$ is asymptotically normal:</p>\[\sqrt{n}(\hat{\theta}_n-\vartheta) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, \mathcal{I}(f(\cdot, \vartheta))^{-1}).\]<p>We will prove it in 4 steps:</p><p><span style="color:salmon"><em>Step 1.</em></span> Prove the constistency of $\hat{\theta}_n$. For this we need to verify that all conditions from theorem about consistency of maximum-likelihood estimator are satisfied:</p><ol><li>Satisfied by the assumption.<li>$L_n(\eta)$ is a.s. continuous. Using 2-3 conditions and dominated convergence we get \(|L(\eta_1, \vartheta) - L(\eta_2, \vartheta)| \leq \int_{\mathcal{X}} |\ell(x, \eta_1) - \ell(x,\eta_2)| f(x,\vartheta) \mu(dx) \rightarrow 0,\) for $\eta_1 \rightarrow \eta_2$.<li>By Law of Large Numbers: \(\begin{aligned} \limsup_{n \rightarrow \infty} \sup_{\| \eta_1 - \eta_2 \| &lt; \delta} | L_n(\eta_1) - L_n(\eta_2)| &amp; \leq \limsup_{n \rightarrow \infty} \frac{1}{n} \sum_{i=1}^{n} \sup_{\| \eta_1 - \eta_2 \| &lt; \delta} |\ell(X_i, \eta_1) - \ell(X_i, \eta_2) |\\ &amp; = \mathbb{E}_\vartheta[\sup_{\| \eta_1 - \eta_2 \| &lt; \delta}|\ell(X,\eta_1) - \ell(X, \eta_2)|] \end{aligned}\)</ol><p>Because $\Theta$ is compact, function $\ell(X, \eta)$ is a.s. uniformly continuous in $\eta$. As a consequence, the last statement converges to zero for $\delta \rightarrow 0$ (using again dominated convergence).</p><p><span style="color:salmon"><em>Step 2.</em></span> Let</p>\[\dot{L}_n(\vartheta) := \frac{1}{n} \sum_{i=1}^{n} \dot{\ell}(X_i, \vartheta).\]<p>Prove that $\sqrt{n}\dot{L}_n(\vartheta) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, \mathcal{I}(f(\cdot, \vartheta))).$</p><p>Let $A_n$ be $k$-dimensional rectangle with vertices in $\hat{\theta}_n$ and $\vartheta$. Because $\hat{\theta}_n \xrightarrow{\mathcal{L}} \vartheta$ and $\vartheta \in \operatorname{int}(\Theta)$, we have</p>\[P_\vartheta(A_n \subset \operatorname{int}(\Theta)) \rightarrow 1.\]<p>Also</p>\[\dot{L}_n(\hat{\theta}_n) = \frac{1}{n} \sum_{i=1}^{n} \dot{\ell}(X_i, \hat{\theta}_n) = 0\]<p>by definition of $\hat{\theta}_n$, and</p>\[\begin{aligned} \mathbb{E}[\dot{\ell}(X_i, \vartheta)] &amp; = \int_{\mathcal{X}} \dot{\ell}(x, \vartheta) f(x, \vartheta) dx \\ &amp; = \int_{\mathcal{X}} \dot{f}(x, \vartheta) dx \\ &amp; =\frac{\partial}{\partial \vartheta}\int_{\mathcal{X}} f(x, \vartheta) dx = 0. \end{aligned}\]<p>By definition</p>\[\operatorname{Cov}(\dot{\ell}(X_i, \vartheta)) = \mathcal{I}(f(\cdot, \vartheta)).\]<p>Then by CLT:</p>\[\sqrt{n} \dot{L}_n(\vartheta) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, \mathcal{I}(f(\cdot, \vartheta))).\]<p><span style="color:salmon"><em>Step 3.</em></span> By Mean Theorem:</p>\[-\dot{L}_n(\vartheta) = \dot{L}_n(\hat{\theta}_n) - \dot{L}_n(\vartheta) = \ddot{L}_n(\widetilde{\theta}_n)(\hat{\theta}_n - \vartheta)\]<p>for some $\widetilde{\theta}_n \in A_n$. Prove that $\ddot{L}_n(\widetilde{\theta}_n) \xrightarrow{\mathcal{L}} -\mathcal{I}(f(\cdot, \vartheta))$.</p><p>We use the equation</p>\[\ddot{\ell}(x, \vartheta) = \frac{\ddot{f}(x, \vartheta)}{f(x,\vartheta)} - \dot{\ell}(x, \vartheta)\dot{\ell}(x, \vartheta)^T.\]<p>to show that</p>\[\mathbb{E}_\vartheta[\ddot{\ell}(X, \vartheta)] + \mathcal{I}(f(\cdot, \vartheta)) = \mathbb{E}_\vartheta\Big[ \frac{\ddot{f}(X, \vartheta)}{f(X,\vartheta)} \Big] = 0,\]<p>From Law of Large Numbers it follows that</p>\[\ddot{L}_n(\vartheta) \xrightarrow{\mathcal{L}} - \mathcal{I}(f(\cdot, \vartheta)).\]<p>Finally, we use the equality</p>\[\lim\limits_{\delta \rightarrow 0} \lim\limits_{n \rightarrow \infty} P_\vartheta(\| \widetilde{\theta}_n - \vartheta \| &lt; \delta) = 1\]<p>and continuity of $\ddot{\ell}$ over $\vartheta$ to finish the proof.</p><p><span style="color:salmon"><em>Step 4.</em></span> Now we conclude that</p>\[\lim\limits_{n \rightarrow \infty} P_\vartheta(\ddot{L}_n(\widetilde{\theta}_n) \text{ is invertible}) = 1.\]<p>and applying Slutsky’s lemma we get</p>\[\begin{aligned} \sqrt{n}(\hat{\theta}_n - \vartheta) &amp; = -\ddot{L}_n(\widetilde{\theta}_n)^{-1} \sqrt{n} \dot{L}_n(\vartheta) \\ &amp; \rightarrow \mathcal{I}(f(\cdot, \vartheta))^{-1} \mathcal{N}(0, \mathcal{I}(f(\cdot, \vartheta))) \\&amp;= \mathcal{N}(0, \mathcal{I}(f(\cdot, \vartheta))^{-1}). \color{Salmon}{\square} \end{aligned}\]<p>Take an example: let $X_1, \dots X_n$ be i.i.d. $\sim \operatorname{Exp}(\lambda)$ with joint density</p>\[f_n(X, \lambda) = \lambda^n \exp \Big(-\lambda \sum_{i=1}^n X_i \Big) \quad \forall x \in \mathbb{R}^+.\]<p>To find maximum-likelihood estimator one must maximize log-density</p>\[\ell_n(X, \lambda) = n \log(\lambda) - \lambda \sum_{i=1}^n X_i \quad \forall x \in \mathbb{R}^+\]<p>with respect to $\lambda$. Taking the derivative and equating it to zero we get</p>\[\frac{n}{\lambda} = \sum_{i=1}^{n} X_i,\]<p>and estimator is</p>\[\hat{\lambda}_n = \frac{1}{\overline{X}_n}.\]<p>Next, using the fact that</p>\[\mathbb{E}[X] = \lambda^{-1} \quad \text{and} \quad \operatorname{Var}(X) = \lambda^{-2},\]<p>and $\dot{\ell}_1(X, \lambda) = -(X - \lambda^{-1})$, we calculate Fisher information:</p>\[\mathcal{I}(f(\cdot, \lambda)) = \mathbb{E}\Big[\Big(X - \frac{1}{\lambda}\Big)^2\Big]=\frac{1}{\lambda^2}.\]<p>By theorem of asymptotic efficiency of maximum-likelihood estimators we get</p>\[\sqrt{n}(\hat{\lambda}_n - \lambda) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, \lambda^2),\]<p>On the other hand by CLT</p>\[\sqrt{n}\Big(\overline{X}_n - \frac{1}{\lambda}\Big) \xrightarrow[]{\mathcal{L}} \mathcal{N}\Big(0, \frac{1}{\lambda^2}\Big).\]<p>Using Delta-method for $g(x) = x^{-1}$ we get the same result:</p>\[\sqrt{n}(\overline{X}_n^{-1} - \lambda) \xrightarrow[]{\mathcal{L}} \mathcal{N}(0, \lambda^2).\]</div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/statistics/'>Statistics</a>, <a href='/categories/visual-guide/'>Visual Guide</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/statistics/" class="post-tag no-text-decoration" >statistics</a> <a href="/tags/consistent-estimator/" class="post-tag no-text-decoration" >consistent estimator</a> <a href="/tags/central-limit-theorem/" class="post-tag no-text-decoration" >central limit theorem</a> <a href="/tags/slutsky-lemma/" class="post-tag no-text-decoration" >slutsky lemma</a> <a href="/tags/delta-method/" class="post-tag no-text-decoration" >delta method</a> <a href="/tags/asymptotic-efficiency/" class="post-tag no-text-decoration" >asymptotic efficiency</a> <a href="/tags/maximum-likelihood-estimator/" class="post-tag no-text-decoration" >maximum likelihood estimator</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Visual Guide to Statistics. Part III: Asymptotic Properties of Estimators - AstraBlog&url=https://astralord.github.io/posts/visual-guide-to-statistics-part-iii-asymptotic-properties-of-estimators/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Visual Guide to Statistics. Part III: Asymptotic Properties of Estimators - AstraBlog&u=https://astralord.github.io/posts/visual-guide-to-statistics-part-iii-asymptotic-properties-of-estimators/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Visual Guide to Statistics. Part III: Asymptotic Properties of Estimators - AstraBlog&url=https://astralord.github.io/posts/visual-guide-to-statistics-part-iii-asymptotic-properties-of-estimators/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://astralord.github.io/posts/visual-guide-to-statistics-part-iii-asymptotic-properties-of-estimators/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/">Building Aligned Intelligence System. Part I: Creating GPT Assistant</a><li><a href="/posts/exploring-parallel-strategies-with-jax/">Exploring Parallel Strategies with Jax</a><li><a href="/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/">Building Aligned Intelligence System. Part II. Improving Large Language Models</a><li><a href="/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/">Visual Guide to Statistics. Part I: Basics of Point Estimation</a><li><a href="/posts/visual-guide-to-statistics-part-ii-bayesian-statistics/">Visual Guide to Statistics. Part II: Bayesian Statistics</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/parameter-estimation/">parameter estimation</a> <a class="post-tag" href="/tags/asymptotic-efficiency/">asymptotic efficiency</a> <a class="post-tag" href="/tags/bartlett-test/">bartlett test</a> <a class="post-tag" href="/tags/bayes-estimator/">bayes estimator</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/central-limit-theorem/">central limit theorem</a> <a class="post-tag" href="/tags/chain-of-thought/">chain-of-thought</a> <a class="post-tag" href="/tags/chatgpt/">chatgpt</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/"><div class="card-body"> <span class="timeago small" >Mar 21, 2022<i class="unloaded">2022-03-21T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Visual Guide to Statistics. Part I: Basics of Point Estimation</h3><div class="text-muted small"><p> This series of posts is a guidance for those who already have knowledge in probability theory and would like to become familiar with mathematical statistics. Basically, these are notes from lect...</p></div></div></a></div><div class="card"> <a href="/posts/visual-guide-to-statistics-part-ii-bayesian-statistics/"><div class="card-body"> <span class="timeago small" >Mar 21, 2022<i class="unloaded">2022-03-21T06:11:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Visual Guide to Statistics. Part II: Bayesian Statistics</h3><div class="text-muted small"><p> Part II introduces different approach to parameters estimation called Bayesian statistics. Basic definitions We noted in the previous part that it is extremely unlikely to get a uniformly bes...</p></div></div></a></div><div class="card"> <a href="/posts/visual-guide-to-statistics-part-iv-foundations-of-testing/"><div class="card-body"> <span class="timeago small" >May 1, 2022<i class="unloaded">2022-05-01T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Visual Guide to Statistics. Part IV: Foundations of Testing</h3><div class="text-muted small"><p> This is the fourth and the last part of a ‘Visual Guide to Statistics’ cycle. All the previous parts and other topics related to statistics could be found here. In this post we will test hypoth...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/visual-guide-to-statistics-part-ii-bayesian-statistics/" class="btn btn-outline-primary" prompt="Older"><p>Visual Guide to Statistics. Part II: Bayesian Statistics</p></a> <a href="/posts/visual-guide-to-statistics-part-iv-foundations-of-testing/" class="btn btn-outline-primary" prompt="Newer"><p>Visual Guide to Statistics. Part IV: Foundations of Testing</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/astralord">Aleksandr Samarin</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/parameter-estimation/">parameter estimation</a> <a class="post-tag" href="/tags/asymptotic-efficiency/">asymptotic efficiency</a> <a class="post-tag" href="/tags/bartlett-test/">bartlett test</a> <a class="post-tag" href="/tags/bayes-estimator/">bayes estimator</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/central-limit-theorem/">central limit theorem</a> <a class="post-tag" href="/tags/chain-of-thought/">chain of thought</a> <a class="post-tag" href="/tags/chatgpt/">chatgpt</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://astralord.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
