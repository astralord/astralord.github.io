<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Building Aligned Intelligence System. Part I: Creating GPT Assistant" /><meta property="og:locale" content="en" /><meta name="description" content="In recent years, the field of natural language processing has witnessed a remarkable breakthrough with the advent of Large Language Models (LLMs). These models have demonstrated unprecedented performance in a wide range of language tasks, from text generation to question answering. However, the mathematical formulation of these models and the techniques used to fine-tune them for specific tasks can be complex and daunting for many researchers and practitioners. In this post, we will delve into the mathematical underpinnings of LLMs, with a focus on ChatGPT and parameter efficient fine-tuning. We will explore the key concepts and techniques involved in these models, and provide a comprehensive guide to help you understand and apply them in your own research and development. So, whether you’re a seasoned NLP expert or just starting out, join us on this journey to unlock the secrets of LLMs and take your NLP skills to the next level. ‘Engaging preamble’ generated by ChatGPT." /><meta property="og:description" content="In recent years, the field of natural language processing has witnessed a remarkable breakthrough with the advent of Large Language Models (LLMs). These models have demonstrated unprecedented performance in a wide range of language tasks, from text generation to question answering. However, the mathematical formulation of these models and the techniques used to fine-tune them for specific tasks can be complex and daunting for many researchers and practitioners. In this post, we will delve into the mathematical underpinnings of LLMs, with a focus on ChatGPT and parameter efficient fine-tuning. We will explore the key concepts and techniques involved in these models, and provide a comprehensive guide to help you understand and apply them in your own research and development. So, whether you’re a seasoned NLP expert or just starting out, join us on this journey to unlock the secrets of LLMs and take your NLP skills to the next level. ‘Engaging preamble’ generated by ChatGPT." /><link rel="canonical" href="https://astralord.github.io/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/" /><meta property="og:url" content="https://astralord.github.io/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/" /><meta property="og:site_name" content="AstraBlog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-07-03T06:00:00+03:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Building Aligned Intelligence System. Part I: Creating GPT Assistant" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-01-31T21:05:21+03:00","datePublished":"2023-07-03T06:00:00+03:00","description":"In recent years, the field of natural language processing has witnessed a remarkable breakthrough with the advent of Large Language Models (LLMs). These models have demonstrated unprecedented performance in a wide range of language tasks, from text generation to question answering. However, the mathematical formulation of these models and the techniques used to fine-tune them for specific tasks can be complex and daunting for many researchers and practitioners. In this post, we will delve into the mathematical underpinnings of LLMs, with a focus on ChatGPT and parameter efficient fine-tuning. We will explore the key concepts and techniques involved in these models, and provide a comprehensive guide to help you understand and apply them in your own research and development. So, whether you’re a seasoned NLP expert or just starting out, join us on this journey to unlock the secrets of LLMs and take your NLP skills to the next level. ‘Engaging preamble’ generated by ChatGPT.","headline":"Building Aligned Intelligence System. Part I: Creating GPT Assistant","mainEntityOfPage":{"@type":"WebPage","@id":"https://astralord.github.io/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/"},"url":"https://astralord.github.io/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/"}</script><title>Building Aligned Intelligence System. Part I: Creating GPT Assistant | AstraBlog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="AstraBlog"><meta name="application-name" content="AstraBlog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/marvel-icon.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">AstraBlog</a></div><div class="site-subtitle font-italic">A place to learn and share knowledge about AI-related things</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/astralord" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['samarin_ad','mail.ru'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/aleksandr-samarin-b8a35496" aria-label="linkedin" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://t.me/astrlrd" aria-label="telegram" class="order-6" target="_blank" rel="noopener"> <i class="fab fa-telegram"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Building Aligned Intelligence System. Part I: Creating GPT Assistant</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Building Aligned Intelligence System. Part I: Creating GPT Assistant</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Aleksandr Samarin </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Mon, Jul 3, 2023, 6:00 AM +0300" >Jul 3, 2023<i class="unloaded">2023-07-03T06:00:00+03:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Wed, Jan 31, 2024, 9:05 PM +0300" >Jan 31<i class="unloaded">2024-01-31T21:05:21+03:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4375 words">24 min read</span></div></div><div class="post-content"><blockquote><p>In recent years, the field of natural language processing has witnessed a remarkable breakthrough with the advent of Large Language Models (LLMs). These models have demonstrated unprecedented performance in a wide range of language tasks, from text generation to question answering. However, the mathematical formulation of these models and the techniques used to fine-tune them for specific tasks can be complex and daunting for many researchers and practitioners. In this post, we will delve into the mathematical underpinnings of LLMs, with a focus on ChatGPT and parameter efficient fine-tuning. We will explore the key concepts and techniques involved in these models, and provide a comprehensive guide to help you understand and apply them in your own research and development. So, whether you’re a seasoned NLP expert or just starting out, join us on this journey to unlock the secrets of LLMs and take your NLP skills to the next level.</p><p><span style="color:Salmon"><strong>‘Engaging preamble’ generated by ChatGPT.</strong></span></p></blockquote><p>There will be two blogposts. The first post will be helpful to those who want to become acquainted with the RLHF pipeline as it focuses on InstructGPT/ChatGPT development. The second one is for those who are interested in applying large language models to specific tasks.</p><p>Here is the partial list of the sources I used for this and the following parts:</p><ul><li>Papers:<ul><li><a href="https://arxiv.org/pdf/1811.07871.pdf">Scalable agent alignment via reward modeling: a research direction (Leike et al., 2018)</a><li><a href="https://arxiv.org/pdf/2005.14165.pdf">Language Models are Few-Shot Learners (Brown et al., 2020)</a><li><a href="https://arxiv.org/pdf/1909.08593.pdf">Fine-Tuning Language Models from Human Preferences (Ziegler et al., 2020)</a><li><a href="https://arxiv.org/pdf/2203.02155.pdf">Training language models to follow instructions with human feedback (Ouyang et al., 2022)</a><li><a href="https://arxiv.org/pdf/2009.01325.pdf">Learning to summarize from human feedback (Stiennon et al., 2022)</a><li><a href="https://arxiv.org/pdf/2204.05862.pdf">Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback (Bai et al., 2022)</a><li><a href="https://arxiv.org/pdf/1706.03741.pdf">Deep Reinforcement Learning from Human Preferences (Christiano et al., 2023)</a><li><a href="https://arxiv.org/pdf/2303.15647.pdf">Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning (Lialin et al., 2023)</a><li><a href="https://arxiv.org/pdf/2201.11903.pdf">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei et al., 2023)</a></ul><li>Posts:<ul><li><a href="https://huyenchip.com/2023/05/02/rlhf.html">RLHF: Reinforcement Learning from Human Feedback (Chip Huyen, 2023)</a><li><a href="https://huggingface.co/blog/rlhf">Illustrating Reinforcement Learning from Human Feedback (RLHF) (Lambert et al., 2022)</a><li><a href="https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/">Prompt Engineering (Lilian Weng, 2023)</a><li><a href="https://leimao.github.io/article/OpenAI-GPT-Models/">OpenAI GPT Models (Lei Mao, 2023)</a><li><a href="https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md">Techniques to improve reliability (OpenAI, 2023)</a></ul></ul><p>GPT assistant building pipeline can be divided into 4 stages, each of which will be described here, starting from an overview of transformer architecture.</p><p><img data-proofer-ignore data-src="/assets/img/rlhf_karpathy.png" alt="GPT assistant pipeline" /> <em>A diagram from <a href="https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2">Andrej Karpathy talk</a> illustrating the four steps of GPT assistant training pipeline: (1) base model pretraining, (2) supervised fine-tuning (SFT), (3) reward model (RM) training, and (4) reinforcement learning via policy optimization on this reward model and human feedback (RLHF).</em></p><h3 id="pretraining-base-model">Pretraining Base model</h3><h4 id="language-modelling">Language modelling</h4><p>In 2018 OpenAI presented <strong>Generative pre-trained transformer (GPT)</strong>, a type of deep learning model, that is designed to generate natural language text. Their work, which they humbly called <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">Improving Language Understanding by Generative Pre-Training</a>, along with famous <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all you need</a> paper, gave birth to the entire family of large language models and changed an entire AI industry in just 5 years.</p><p>The model was trained on a large corpus of text data with a completion task. During such training model is fed by a partial sequence of words, called <strong>prompt</strong> or <strong>context vector</strong> of tokens, and then asked to generate a response to complete that prompt. The model learns to predict the probability of the next token $x_{k+1}$ in a sequence given by previous tokens $x = (x_1, \dots x_k)$ sampled from dataset $\mathcal{D}$. Strictly speaking, we search for parameters of neural network $\phi$, such that they minimize the cross-entropy loss</p>\[\mathcal{L}(\phi) = -\mathbb{E}_{x \sim \mathcal{D}}[\log \pi_\phi(x_{k+1} \mid x_1, \dots, x_{k})],\]<p>where $\pi_\phi$ is a language model output.</p><p>Though GPT-1 was pre-trained as an autoregressive language model, transformer’s fine-tuning and inference didn’t have to be autoregressive and language model could be further fine-tuned for specialized natural language processing tasks.</p><p><img data-proofer-ignore data-src="/assets/img/gpt-1-downstream.png" alt="GPT-1 downstream" /> <em>Input transformations for fine-tuning GPT on different tasks.</em></p><p>The major conclusion from this paper was that it is no longer necessary to develop specific neural network architectures for specific natural language processing tasks. Transfer learning from GPT language model pre-trained with large corpus of text data was already sufficient.</p><h4 id="transformer-architecture">Transformer architecture</h4><p>The process of text generation with GPT is the following. First, embedding layer takes sequence of tokens $x$ and outputs</p>\[h_0 = x \mathbf{W_e} + \mathbf{W_p},\]<p>where $\mathbf{W_e}$ and $\mathbf{W_p}$ are token and position embedding matrices respectively. Then these embedding vectors are processed by so-called <strong>transformer</strong>, consisting of multiple transformer blocks (more about it later):</p>\[h_n = \operatorname{Transformer-block}(h_{n-1}) \quad n = 1, \dots, N.\]<p>And finally we get the token output distribution by taking $h_N$ and run it through unembedding layer and softmax function:</p>\[\pi_\phi(\cdot \mid x) = \operatorname{softmax}(h_N \mathbf{W_e}^T).\] <script src="https://d3js.org/d3.v4.min.js"></script><link href="https://fonts.googleapis.com/css?family=Arvo" rel="stylesheet" /><div id="gpt_arch_simple" class="svg-container" align="center"></div><script> function line(svg, x1, y1, x2, y2, opacity=1.0, width=2, stroke='black') { svg.append('line') .attr('x1', x1) .attr('y1', y1) .attr('x2', x2) .attr('y2', y2) .style("stroke-width", width) .attr("opacity", opacity) .attr('stroke', stroke); } function triangle(svg, x, y, rotate=0, opacity=1, stroke="black") { const triangleSize = 25; var triangle_symb = d3.symbol() .type(d3.symbolTriangle) .size(triangleSize); svg.append("path") .attr("d", triangle_symb) .attr("stroke", stroke) .attr("fill", "#404040") .attr("opacity", opacity) .attr("transform", function(d) { return "translate(" + x + "," + y + ") rotate(" + rotate + ")"; }); } function up_arrow(svg, x1, y1, y2, opacity=1) { line(svg, x1, y1 + 7, x1, y2, opacity=opacity, width=2, stroke="black"); triangle(svg, x1, y1 + 5, 0, opacity=opacity, stroke="black"); } function bckg_block(svg, x, y, height=280) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 198) .attr('height', height) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#F3F3F4'); } function emb_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1) .attr('fill', '#F7E1E1'); svg.append('text') .attr('x', x + 20) .attr('y', y + 20) .text("Embeddings") .style("font-size", "14px") .attr("font-family", "Arvo"); } function softmax_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#D1E6D1'); svg.append('text') .attr('x', x + 33) .attr('y', y + 20) .text("Softmax") .style("font-size", "14px") .attr("font-family", "Arvo"); } function linear_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#DCDFEE'); svg.append('text') .attr('x', x + 40) .attr('y', y + 20) .text("Linear") .style("font-size", "14px") .attr("font-family", "Arvo"); } function transformer_block(svg, x, y) { bckg_block(svg, x, y, height=30); svg.append('text') .attr('x', x + 34) .attr('y', y + 20) .text("Transformer block") .style("font-size", "14px") .attr("font-family", "Arvo"); } function gpt_arch_simple() { var svg = d3.select("#gpt_arch_simple") .append("svg") .attr("width", 300) .attr("height", 375); softmax_block(svg, 90, 20); line(svg, 150, 50, 150, 60); linear_block(svg, 90, 60); up_arrow(svg, 150, 90, 110); transformer_block(svg, 50, 110); up_arrow(svg, 150, 140, 160); transformer_block(svg, 50, 160); svg.append('line') .attr('x1', 150) .attr('y1', 195) .attr('x2', 150) .attr('y2', 240) .style("stroke-width", 2) .attr("stroke-dasharray", ("2, 10")) .attr('stroke', 'black'); up_arrow(svg, 150, 240, 260); transformer_block(svg, 50, 260); svg.append('text') .attr('x', 50) .attr('y', 225) .text("N ✕") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 230) .attr('y', 315) .text("Positional") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 230) .attr('y', 330) .text("Encoding") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('line') .attr('x1', 220) .attr('y1', 320) .attr('x2', 150) .attr('y2', 320) .style("stroke-width", 2) .attr('stroke', 'black'); up_arrow(svg, 150, 290, 340); svg.append('circle').attr('cx', 150).attr('cy', 320).attr('r', 8) .attr('stroke', 'black') .attr('fill', '#FFFFFF'); svg.append('line') .attr('x1', 150) .attr('y1', 315) .attr('x2', 150) .attr('y2', 325) .style("stroke-width", 2) .attr('stroke', 'black'); svg.append('line') .attr('x1', 145) .attr('y1', 320) .attr('x2', 155) .attr('y2', 320) .style("stroke-width", 2) .attr('stroke', 'black'); emb_block(svg, 90, 340); } gpt_arch_simple(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Simplified view of GPT architecture. Recently other techniques to encode token positions have appeared, such as <a href="https://arxiv.org/pdf/2104.09864.pdf">Rotary Position Embeddings (RoPE)</a> and <a href="https://arxiv.org/pdf/2108.12409.pdf">Attention with Linear Biases (ALiBi)</a>. They are out of the scope of this post</em></p><p>The key innovation of GPT is its use of a transformer architecture, which allows the model to process long sequences of text efficiently. This makes GPT particularly well-suited for tasks that require generating long, coherent pieces of text, such as writing articles or answering questions. The core of the transformer is a <strong>scaled dot product attention</strong> operation, which takes as input a set of queries $\mathbf{Q}$, keys $\mathbf{K}$ and values $\mathbf{V}$ and outputs</p>\[\operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \operatorname{softmax} \Big( \frac{\mathbf{QK}^T}{\sqrt{d}} \Big) \cdot \mathbf{V},\]<p>where $d$ is a hidden dimensionality for queries and keys. The goal is to have an attention mechanism with which any element in a sequence can attend to any other while still being efficient to compute.</p><p>Transformer architecture can be one of two types: <strong>encoder</strong> or <strong>decoder</strong>. The only difference between the two is whether mask is applied to attention layer. This modification in decoder architecture is crucial for next-token-prediction task, because it prevents positions from attending to subsequent positions attention layer modified by mask. Combined with the fact that the output embeddings are offset by one position, masking ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$. It can be implemented inside of scaled dot-product attention by masking out (setting to $-\infty$) all values in the $\mathbf{QK}^T$ matrix which correspond to illegal connections.</p><p>The scaled dot product attention allows a network to attend over a sequence. However, often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it. The attention mechanism can be extended to multiple heads, i.e. multiple different query-key-value triplets on the same features. Specifically, given $\mathbf{Q}$, $\mathbf{K}$, and $\mathbf{V}$ matrices, we transform those into $k$ sub-queries, sub-keys, and sub-values, respectively, which we run through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix $\mathbf{W}^O$. Mathematically, we can express this operation as:</p>\[\operatorname{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V})=[\operatorname{head}_1; \dots; \operatorname{head}_k] \cdot \mathbf{W}^O,\]<p>where</p>\[\operatorname{head}_i = \operatorname{Attention}(\mathbf{QW}_i^Q, \mathbf{KW}_i^W, \mathbf{VW}_i^V), \quad i = 1, \dots, k.\]<p>We’ll refer to this as <strong>multi-head attention layer</strong> with the learnable parameters $\mathbf{W}^Q_{1 \dots k}, \mathbf{W}^K_{1 \dots k}, \mathbf{W}^V_{1 \dots k}$ and $\mathbf{W}^O$ (also called <strong>multi-head self-attention</strong> for $\mathbf{Q} = \mathbf{K} = \mathbf{V}$). Such mechanism allows the model to jointly attend to information from different representation subspaces at different positions. The output of multi-head attention is added to the original input using a residual connection, and we apply a consecutive layer normalization on the sum.</p><p>Transformer is basically a stack of $N$ identical blocks with multi-head attention. In addition to attention sub-layers, each block contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a nonlinear function in between, e.g. ReLU<sup id="fnref:GELU" role="doc-noteref"><a href="#fn:GELU" class="footnote" rel="footnote">1</a></sup>:</p>\[\operatorname{FFN}(x) = \max(0, x\mathbf{W}_1)\mathbf{W}_2\]<p>After feed forward block residual connection with normalization layer is added again.</p><div id="gpt_arch" class="svg-container" align="center"></div><script> function linear_block_2(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 80) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#E9EEEB'); svg.append('text') .attr('x', x + 20) .attr('y', y + 20) .text("Linear") .style("font-size", "14px") .attr("font-family", "Arvo"); } function multi_linear_block(svg, x, y) { svg.append('rect') .attr('x', x + 20) .attr('y', y - 20) .attr('width', 60) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 0.5) .attr('fill', '#E9EEEB'); svg.append('rect') .attr('x', x + 10) .attr('y', y - 10) .attr('width', 60) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 0.5) .attr('fill', '#E9EEEB'); svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 60) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#E9EEEB'); svg.append('text') .attr('x', x + 10) .attr('y', y + 20) .text("Linear") .style("font-size", "14px") .attr("font-family", "Arvo"); up_arrow(svg, x + 30, y + 30, y + 70); up_arrow(svg, x + 30, y - 50, y); up_arrow(svg, x + 40, y - 60, y - 10, 0.5); up_arrow(svg, x + 50, y - 70, y - 20, 0.5); } function addnorm_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#F3F3C6'); svg.append('text') .attr('x', x + 20) .attr('y', y + 20) .text("Add & Norm") .style("font-size", "14px") .attr("font-family", "Arvo"); } function ff_block(svg, x, y, frozen=false) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#C9E7F5'); svg.append('text') .attr('x', x + 15) .attr('y', y + 20) .text("Feed Forward") .style("font-size", "14px") .attr("font-family", "Arvo"); if (frozen) { svg.append('text') .attr('x', x + 108) .attr('y', y + 12) .text("❄") .style("font-size", "14px") .attr("font-family", "Arvo"); } } function mha_block(svg, x, y, frozen=false) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 70) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#FBE2C0'); svg.append('text') .attr('x', x + 35) .attr('y', y + 20) .text("Masked") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 20) .attr('y', y + 40) .text("Multi-Head") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 30) .attr('y', y + 60) .text("Attention") .style("font-size", "14px") .attr("font-family", "Arvo"); if (frozen) { svg.append('text') .attr('x', x + 108) .attr('y', y + 12) .text("❄") .style("font-size", "14px") .attr("font-family", "Arvo"); } } function sdp_block(svg, x, y) { svg.append('rect') .attr('x', x + 20) .attr('y', y - 20) .attr('width', 220) .attr('height', 50) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 0.5) .attr('fill', '#C3BFDC'); svg.append('rect') .attr('x', x + 10) .attr('y', y - 10) .attr('width', 220) .attr('height', 50) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 0.5) .attr('fill', '#C3BFDC'); svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 220) .attr('height', 50) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#C3BFDC'); svg.append('text') .attr('x', x + 45) .attr('y', y + 20) .text("Scaled Dot-Product") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 75) .attr('y', y + 40) .text("Attention") .style("font-size", "14px") .attr("font-family", "Arvo"); up_arrow(svg, x + 110, y - 50, y); up_arrow(svg, x + 120, y - 50, y - 10, 0.5); up_arrow(svg, x + 130, y - 50, y - 20, 0.5); } function concat_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 80) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#FDF8C6'); svg.append('text') .attr('x', x + 17) .attr('y', y + 20) .text("Concat") .style("font-size", "14px") .attr("font-family", "Arvo"); } function scale_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 80) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#FDF8C6'); svg.append('text') .attr('x', x + 24) .attr('y', y + 20) .text("Scale") .style("font-size", "14px") .attr("font-family", "Arvo"); } function mm_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#C3BFDC'); svg.append('text') .attr('x', x + 33) .attr('y', y + 20) .text("MatMul") .style("font-size", "14px") .attr("font-family", "Arvo"); } function mask_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 80) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#F2CDDE'); svg.append('text') .attr('x', x + 21) .attr('y', y + 20) .text("Mask") .style("font-size", "14px") .attr("font-family", "Arvo"); } function gpt_arch() { var svg = d3.select("#gpt_arch") .append("svg") .attr("width", 725) .attr("height", 465); bckg_block(svg, 1, 110); svg.append("path") .attr("stroke", "black") .datum([{x: 220, y: 360}, {x: 500, y: 360}, {x: 500, y: 70}, {x: 220, y: 70}, {x: 220, y: 270}, {x: 170, y: 300}, {x: 220, y: 330}, {x: 220, y: 360}]) .attr("fill", "none") .attr("stroke-width", 3) .attr("opacity", 0.4) .attr("stroke", "gray") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .attr("stroke", "black") .datum([{x: 520, y: 340}, {x: 720, y: 340}, {x: 720, y: 30}, {x: 520, y: 30}, {x: 520, y: 190}, {x: 490, y: 220}, {x: 520, y: 250}, {x: 520, y: 340}]) .attr("fill", "none") .attr("stroke-width", 3) .attr("opacity", 0.4) .attr("stroke", "gray") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); softmax_block(svg, 40, 20); line(svg, 100, 50, 100, 60); linear_block(svg, 40, 60); up_arrow(svg, 100, 90, 120); addnorm_block(svg, 40, 120); line(svg, 100, 150, 100, 160); ff_block(svg, 40, 160); up_arrow(svg, 100, 190, 230); svg.append("path") .attr("stroke", "black") .datum([{x: 100, y: 215}, {x: 35, y: 215}, {x: 10, y: 215}, {x: 10, y: 185}, {x: 10, y: 165}, {x: 10, y: 135}, {x: 35, y: 135}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 35, 135, 90); addnorm_block(svg, 40, 230); line(svg, 100, 260, 100, 270); mha_block(svg, 40, 270); up_arrow(svg, 100, 340, 410); svg.append("path") .attr("stroke", "black") .datum([{x: 65, y: 340}, {x: 65, y: 360}, {x: 135, y: 360}, {x: 135, y: 340}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 65, 345, 0); triangle(svg, 135, 345, 0); svg.append("path") .attr("stroke", "black") .datum([{x: 100, y: 375}, {x: 35, y: 375}, {x: 10, y: 375}, {x: 10, y: 345}, {x: 10, y: 275}, {x: 10, y: 245}, {x: 35, y: 245}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 35, 245, 90); svg.append('text') .attr('x', 160) .attr('y', 405) .text("Positional") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 160) .attr('y', 420) .text("Encoding") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('line') .attr('x1', 150) .attr('y1', 410) .attr('x2', 100) .attr('y2', 410) .style("stroke-width", 2) .attr('stroke', 'black'); line(svg, 100, 415, 100, 430); svg.append('circle').attr('cx', 100).attr('cy', 410).attr('r', 8) .attr('stroke', 'black') .attr('fill', '#FFFFFF'); svg.append('line') .attr('x1', 100) .attr('y1', 405) .attr('x2', 100) .attr('y2', 415) .style("stroke-width", 2) .attr('stroke', 'black'); svg.append('line') .attr('x1', 95) .attr('y1', 410) .attr('x2', 105) .attr('y2', 410) .style("stroke-width", 2) .attr('stroke', 'black'); emb_block(svg, 40, 430); svg.append('text') .attr('x', 0) .attr('y', 100) .text("N ✕") .style("font-size", "14px") .attr("font-family", "Arvo"); up_arrow(svg, 350, 50, 80); linear_block_2(svg, 310, 80); up_arrow(svg, 350, 110, 130); concat_block(svg, 310, 130); multi_linear_block(svg, 240, 310); multi_linear_block(svg, 320, 310); multi_linear_block(svg, 400, 310); sdp_block(svg, 240, 210); svg.append('text') .attr('x', 265) .attr('y', 400) .text("V") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 345) .attr('y', 400) .text("K") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 425) .attr('y', 400) .text("Q") .style("font-size", "14px") .attr("font-family", "Arvo"); mm_block(svg, 580, 50); up_arrow(svg, 600, 80, 100); softmax_block(svg, 540, 100); up_arrow(svg, 600, 130, 150); mask_block(svg, 560, 150); up_arrow(svg, 600, 180, 200); scale_block(svg, 560, 200); up_arrow(svg, 600, 230, 250); mm_block(svg, 540, 250); up_arrow(svg, 575, 280, 310); up_arrow(svg, 625, 280, 310); up_arrow(svg, 675, 80, 310); svg.append('text') .attr('x', 570) .attr('y', 325) .text("Q") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 620) .attr('y', 325) .text("K") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 670) .attr('y', 325) .text("V") .style("font-size", "14px") .attr("font-family", "Arvo"); } gpt_arch(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Generative pre-trained transformer decoder architecture. In some recent implementations one can find layer normalization being applied to input stream of the submodules (right before multi-head attention and feed forward layers, not after).</em></p><p>There are many guides on the internet for implementing a transformer. To get more detailed explanations one can follow <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">this tutorial on PyTorch</a> or <a href="https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial6/Transformers_and_MHAttention.html">this one on Jax</a>.</p><p>It turned out that the transformer architecture is highly scalable, meaning that it is able to handle large amounts of data and perform well on tasks of varying complexity. It has been shown to achieve SoTA performance on a variety of NLP benchmarks, demonstrating its ability to scale to handle complex tasks and large datasets. GPT-1 was just a starting point and later OpenAI presented much bigger models, such as <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a> and <a href="https://arxiv.org/pdf/2005.14165.pdf">GPT-3</a>. Here is a table for comparison:</p><div class="table-wrapper"><table><thead><tr><th>Model<th>Train data size<th>Transformer blocks<th>Context size<th>Total parameters<tbody><tr><td>GPT-1<td>20M tokens<td>12<td>512 tokens<td>117M<tr><td>GPT-2<td>9B tokens<td>48<td>1024 tokens<td>1.5B<tr><td>GPT-3<td>300B tokens<td>96<td>2048 tokens<td>175B</table></div><p>There is no official information yet on how large <a href="https://arxiv.org/pdf/2303.08774.pdf">GPT-4</a> is, but there’ve been some rumors that it consists of multiple expert models with 220B parameters.</p><p>Language modelling is by far the most resource-intensive phase in InstructGPT training. <a href="https://openai.com/research/instruction-following">According to OpenAI</a> the rest of their pipeline used less than 2% of the compute and data relative to model pretraining. One way of thinking about this process is that at the end of this phase base LLM already has all the required capabilities, but they are difficult to elicit and the rest of the process is aimed to “unlock” them.</p><h3 id="supervised-fine-tuning-sft-for-dialogue">Supervised fine-tuning (SFT) for dialogue</h3><p>Suppose we have a pretrained large language model. But this model is far from being a useful assistant. It doesn’t answer questions, it only wants to complete internet documents. Chip Huyen provided a good example <a href="https://huyenchip.com/2023/05/02/rlhf.html#phase_2_sft">in her blogpost</a>: if you give the pretrained model a question, say, <code class="language-plaintext highlighter-rouge">How to make pizza</code>, any of the following could be valid completion.</p><ul><li>Adding more context to the question: <code class="language-plaintext highlighter-rouge">for a family of six</code><li>Adding follow-up questions: <code class="language-plaintext highlighter-rouge">? What ingredients do I need? How much time would it take?</code><li>Actually giving the answer</ul><p>The third option is preferred if you’re looking for an answer. In order to achieve it, base model can be tricked into performing tasks with prompt engineering. For example, we can give model an instruction and expect it to follow, e.g.:</p><div class="language-plaintext highlighter-rouge"><div class="code-header" text-data="plaintext"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>Classify the text into neutral, negative or positive. 
Text: I think the vacation is okay.
Sentiment:
</pre></table></code></div></div><p>Output:</p><div class="language-plaintext highlighter-rouge"><div class="code-header" text-data="plaintext"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>Neutral
</pre></table></code></div></div><p>This is an example of <strong>zero-shot prompting</strong>. One can also give a few demonstration examples of how the dialogue should look like and expect the model to continue in the same vein. This is called <strong>few-shot prompting</strong>.</p><p><img data-proofer-ignore data-src="/assets/img/prompt_engineering.png" alt="Prompt engineering" /> <em>Few-shot prompt engineering example from <a href="https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2">Andrej Karpathy talk</a></em></p><p>Standard few-shot prompting works well for many tasks but is still not a perfect technique. A more efficient way is to train the model to answer questions directly. This is what SFT does: its goal is to optimize the pretrained model to generate the responses that users are looking for. At this stage labelers provide demonstrations of the desired behaviour on the input prompt distribution. Then a pretrained model is fine-tuned on this data learning to appropriately respond to prompts of different use cases (e.g. question answering, summarization, translation).</p><div id="sft_learning" class="svg-container" align="center"></div><script> function moon_sample(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 130) .attr('height', 70) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 0) .attr("opacity", 1.0) .attr('fill', '#C9E7F5'); svg.append('text') .attr('x', x + 55) .attr('y', y + 20) .text('🌖') .style("font-size", "17px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 20) .attr('y', y + 40) .text('Explain the moon') .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 5) .attr('y', y + 60) .text('landing to a 6 year old') .style("font-size", "11px") .attr("font-family", "Arvo"); } function label_dmnstr(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 130) .attr('height', 70) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 0) .attr("opacity", 1.0) .attr('fill', '#DCDFEE'); svg.append('text') .attr('x', x + 50) .attr('y', y + 20) .text('👤🖋️') .style("font-size", "17px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 17) .attr('y', y + 40) .text('Some people went') .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 35) .attr('y', y + 60) .text('to the moon...') .style("font-size", "11px") .attr("font-family", "Arvo"); } function graph_net(svg, x, y, r=3.5) { svg.append('circle').attr('cx', x).attr('cy', y + 5).attr('r', r); svg.append('circle').attr('cx', x).attr('cy', y + 25).attr('r', r); svg.append('circle').attr('cx', x + 30).attr('cy', y).attr('r', r); svg.append('circle').attr('cx', x + 30).attr('cy', y + 15).attr('r', r); svg.append('circle').attr('cx', x + 30).attr('cy', y + 30).attr('r', r); svg.append('circle').attr('cx', x + 60).attr('cy', y + 5).attr('r', r); svg.append('circle').attr('cx', x + 60).attr('cy', y + 25).attr('r', r); line(svg, x, y + 5, x + 30, y, 1, 1); line(svg, x, y + 25, x + 30, y, 1, 1); line(svg, x, y + 5, x + 30, y + 15, 1, 1); line(svg, x, y + 25, x + 30, y + 15, 1, 1); line(svg, x, y + 5, x + 30, y + 30, 1, 1); line(svg, x, y + 25, x + 30, y + 30, 1, 1); line(svg, x + 30, y, x + 60, y + 5, 1, 1); line(svg, x + 30, y + 15, x + 60, y + 5, 1, 1); line(svg, x + 30, y + 30, x + 60, y + 5, 1, 1); line(svg, x + 30, y, x + 60, y + 25, 1, 1); line(svg, x + 30, y + 15, x + 60, y + 25, 1, 1); line(svg, x + 30, y + 30, x + 60, y + 25, 1, 1); } function net_block(svg, x, y, text) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 130) .attr('height', 70) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 0) .attr("opacity", 1.0) .attr('fill', '#DCDFEE'); svg.append('text') .attr('x', x + 55) .attr('y', y + 16) .text(text) .style("font-size", "11px") .attr("font-family", "Arvo"); graph_net(svg, x + 35, y + 30); } function sft_learning() { var svg = d3.select("#sft_learning") .append("svg") .attr("width", 620) .attr("height", 145); moon_sample(svg, 20, 5); line(svg, 160, 40, 220, 40, 1, 1); triangle(svg, 220, 40, 90); svg.append('text') .attr('x', 20) .attr('y', 100) .text('A prompt is sampled') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 20) .attr('y', 120) .text('from prompt dataset') .style("font-size", "14px") .attr("font-family", "Arvo"); label_dmnstr(svg, 235, 5); line(svg, 375, 40, 435, 40, 1, 1); triangle(svg, 435, 40, 90); svg.append('text') .attr('x', 220) .attr('y', 100) .text('A labeler demonstrates') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 240) .attr('y', 120) .text('the desired output') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 265) .attr('y', 140) .text('behaviour') .style("font-size", "14px") .attr("font-family", "Arvo"); net_block(svg, 450, 5, 'SFT'); svg.append('text') .attr('x', 450) .attr('y', 100) .text('This data is used to') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 430) .attr('y', 120) .text('fine-tune GPT model with') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 450) .attr('y', 140) .text('supervised learning') .style("font-size", "14px") .attr("font-family", "Arvo"); } sft_learning(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Supervised fine-tuning stage of <a href="https://arxiv.org/pdf/2203.02155.pdf">InstructGPT pipeline</a>.</em></p><p>Basically, SFT model is an initial language model for RLHF. For InstructGPT training OpenAI fine-tuned three different versions of GPT-3 (1.3B, 6B and 175B) on labeler-written demonstration prompts. OpenAI called supervised finetuning <em>behavior cloning</em>: we demonstrate how the model should behave, and the model clones this behavior. Anthropic, for example, used a different technique: they trained their SFT model by distilling an original language model on context clues for their “helpful, honest, and harmless” criteria.</p><h3 id="reward-model-rm-training">Reward Model (RM) training</h3><p>The problem with SFT is that model learns what kind of responses are plausible for a given context, but it receives no information on how good or bad a response is. At the same time while it is easy for humans to understand which sentences are better than others, it is difficult to formulate and automate reasons for their choice.</p><p>This is where human feedback (HF) comes into play. At this stage datasets of comparisons between model outputs is collected: annotators indicate which output of SFT model they prefer for a given input. At first glance, it may seem that labelers must apply a scalar score directly to each output in order to create data for reward model, but this is difficult to do in practice. The differing values of humans cause these scores to be uncalibrated and noisy. Instead, rankings are used to compare the outputs of multiple models and create a much better regularized dataset.</p><p>To collect ranking data, human labelers are presented with $K_x$ generated samples conditioned on the same prompt $x$ (in InstructGPT paper, $K_x$ is anywhere between $4$ and $9$). This produces</p>\[\binom{K_x}{2} = \frac{K_x(K_x-1)}{2}\]<p>comparisons for each prompt. Labelers express preferences for one answer, denoted as $y_w \succ y_l \mid x$ with $y_w$ being a preferred answer over $y_l$.</p><p>Then a preference model is trained to predict the human-preferred output. Starting from the SFT model with the final unembedding layer removed, a reward model $r_\theta(x, y)$ is trained to take in a prompt $x$ and response $y$, and output a scalar reward $r$. OpenAI used 6B RM with to evaluate 175B language model as they found out that while larger RM had the potential to achieve lower validation loss, their training was more unstable and using a 175B RM and value function greatly increase the compute requirements of reinforcement learning. Deepmind, on a contrary, trained Chinchilla models with 70B parameters for both reward and language model. An intuition would be that reward models need to have similar capacity to understand the text given to them as a model would need in order to generate said text.</p><div id="rm_learning" class="svg-container" align="center"></div><script> function model_answer(svg, x, y, label, text) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 80) .attr('height', 40) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 0) .attr("opacity", 1.0) .attr('fill', '#DCDFEE'); svg.append('circle').attr('cx', x + 40).attr('cy', y + 11).attr('r', 8).attr('opacity', 0.8); svg.append('text') .attr('x', x + 35) .attr('y', y + 15) .text(label) .style("font-size", "13px") .style('fill', '#DCDFEE') .attr("font-family", "Arvo"); } function rank_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 130) .attr('height', 70) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 0) .attr("opacity", 1.0) .attr('fill', '#DCDFEE'); svg.append('text') .attr('x', x + 55) .attr('y', y + 25) .text('👤') .style("font-size", "20px") .attr("font-family", "Arvo"); svg.append('circle').attr('cx', x + 15).attr('cy', y + 45).attr('r', 8).attr('opacity', 0.8); svg.append('text') .attr('x', x + 10) .attr('y', y + 50) .text('D') .style("font-size", "13px") .style('fill', '#DCDFEE') .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 28) .attr('y', y + 50) .text('>') .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('circle').attr('cx', x + 49).attr('cy', y + 45).attr('r', 8).attr('opacity', 0.8); svg.append('text') .attr('x', x + 44) .attr('y', y + 50) .text('C') .style("font-size", "13px") .style('fill', '#DCDFEE') .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 61) .attr('y', y + 50) .text('>') .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('circle').attr('cx', x + 82).attr('cy', y + 45).attr('r', 8).attr('opacity', 0.8); svg.append('text') .attr('x', x + 77) .attr('y', y + 50) .text('A') .style("font-size", "13px") .style('fill', '#DCDFEE') .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 95) .attr('y', y + 50) .text('=') .style("font-size", "15px") .attr("font-family", "Arvo"); svg.append('circle').attr('cx', x + 115).attr('cy', y + 45).attr('r', 8).attr('opacity', 0.8); svg.append('text') .attr('x', x + 110) .attr('y', y + 50) .text('B') .style("font-size", "13px") .style('fill', '#DCDFEE') .attr("font-family", "Arvo"); } function rm_learning() { var svg = d3.select("#rm_learning") .append("svg") .attr("width", 620) .attr("height", 220); moon_sample(svg, 20, 5); model_answer(svg, 0, 80, 'A'); svg.append('text') .attr('x', 5) .attr('y', 112) .text('Explain gravity...') .style("font-size", "9px") .attr("font-family", "Arvo"); model_answer(svg, 90, 80, 'B'); svg.append('text') .attr('x', 102) .attr('y', 112) .text('Explain war...') .style("font-size", "9px") .attr("font-family", "Arvo"); model_answer(svg, 0, 130, 'C'); svg.append('text') .attr('x', 5) .attr('y', 157) .text('Moon is natural') .style("font-size", "9px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 15) .attr('y', 167) .text('satelite of...') .style("font-size", "9px") .attr("font-family", "Arvo"); model_answer(svg, 90, 130, 'D'); svg.append('text') .attr('x', 97) .attr('y', 157) .text('People went to') .style("font-size", "9px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 107) .attr('y', 167) .text('the moon...') .style("font-size", "9px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 20) .attr('y', 195) .text('A prompt and several') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 0) .attr('y', 215) .text('model outputs are sampled') .style("font-size", "14px") .attr("font-family", "Arvo"); line(svg, 180, 100, 230, 100, 1, 1); triangle(svg, 230, 100, 90); rank_block(svg, 245, 65); svg.append('text') .attr('x', 230) .attr('y', 195) .text('A labeler ranks outputs') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 245) .attr('y', 215) .text('from best to worst') .style("font-size", "14px") .attr("font-family", "Arvo"); line(svg, 385, 100, 435, 100, 1, 1); triangle(svg, 435, 100, 90); net_block(svg, 450, 65, 'RM'); svg.append('text') .attr('x', 450) .attr('y', 195) .text('This data is used to') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 450) .attr('y', 215) .text('train reward model') .style("font-size", "14px") .attr("font-family", "Arvo"); } rm_learning(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Reward modelling stage of <a href="https://arxiv.org/pdf/2203.02155.pdf">InstructGPT pipeline</a>. Boxes A-D are samples from the SFT model that get ranked by labelers</em></p><p>There are a number of approaches used to model preferences, the <a href="https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model">Bradley-Terry model</a> being a popular choice:</p>\[\mathbb{P}(y_i \succ y_j \mid x) = \frac{\exp(r_\theta(x, y_i))}{\exp(r_\theta(x, y_i))+\exp(r_\theta(x, y_j))}.\]<p>Framing the problem as a balanced binary classification we come up to the negative log-likelihood loss:</p>\[\mathcal{L}(\theta) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}}\bigg[\frac{1}{\binom{K_x}{2}}\log \sigma (r_\theta(x, y_w) - r_\theta(x, y_l))\bigg].\]<p>This way model learns to maximize the difference between rewards for chosen and rewards for rejected answers. At the end of training, the reward model is normalized so that</p>\[\mathbb{E}_{(x, y) \sim \mathcal{D}}[r_\theta(x, y)]=0\]<p>for all $x$.</p><p>The success of reward modeling relies heavily on the quality of the reward model. If the reward model only captures most aspects of the objective but not all of it, this can lead the agent to find undesirable degenerate solutions. In other words, the agent’s behavior depends on the reward model in a way that is potentially very fragile.</p><h3 id="reinforcement-learning-with-human-feedback-rlhf">Reinforcement Learning with Human Feedback (RLHF)</h3><p>At final stage output of the reward model used as a scalar reward to optimize a policy. Following <a href="https://arxiv.org/pdf/2009.01325.pdf">Stiennon et al. (2020)</a>, authors of InstructGPT fine-tuned the SFT model on their environment using on-policy algorithm, called <strong>Proximal Policy Optimization (PPO)</strong>, <a href="https://arxiv.org/pdf/1707.06347.pdf">Schulman et al. (2017)</a>. The environment is a bandit environment which presents a random customer prompt to language model and expects a response to the prompt with a sequence of probability distributions over tokens $\pi(y \mid x)$. The action space of this policy is all the tokens corresponding to the vocabulary of the language model and the observation space is the distribution of possible input token sequences.</p><p>Given the prompt and response, environment produces a reward determined by RM and ends the episode.</p><div id="rlhf" class="svg-container" align="center"></div><script> function frog_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 130) .attr('height', 70) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 0) .attr("opacity", 1.0) .attr('fill', '#C9E7F5'); svg.append('text') .attr('x', x + 55) .attr('y', y + 20) .text('🐸') .style("font-size", "17px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 30) .attr('y', y + 40) .text('Write a story') .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 35) .attr('y', y + 60) .text('about frogs') .style("font-size", "11px") .attr("font-family", "Arvo"); } function rlhf() { var svg = d3.select("#rlhf") .append("svg") .attr("width", 620) .attr("height", 195); frog_block(svg, 5, 55); line(svg, 150, 95, 210, 95, 1, 1); triangle(svg, 210, 95, 90); svg.append('text') .attr('x', 16) .attr('y', 150) .text('A new sample is') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 17) .attr('y', 170) .text('generated from') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 30) .attr('y', 190) .text('the dataset') .style("font-size", "14px") .attr("font-family", "Arvo"); net_block(svg, 230, 55, 'PPO'); line(svg, 375, 95, 435, 95, 1, 1); triangle(svg, 435, 95, 90); svg.append('text') .attr('x', 220) .attr('y', 150) .text('The policy generates') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 260) .attr('y', 170) .text('an output') .style("font-size", "14px") .attr("font-family", "Arvo"); net_block(svg, 455, 55, 'RM'); svg.append("path") .attr("stroke", "black") .datum([{x: 520, y: 40}, {x: 520, y: 5}, {x: 485, y: 5}, {x: 330, y: 5}, {x: 295, y: 5}, {x: 295, y: 40}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 295, 40, 180); svg.append('text') .attr('x', 405) .attr('y', 20) .text('r') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 420) .attr('y', 150) .text('The reward model calculates') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 440) .attr('y', 170) .text('a reward for the output') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 450) .attr('y', 190) .text('to update the policy') .style("font-size", "14px") .attr("font-family", "Arvo"); } rlhf(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Policy optimization stage of <a href="https://arxiv.org/pdf/2203.02155.pdf">InstructGPT pipeline</a>.</em></p><p>A per-token KL penalty is added to mitigate overoptimization of the reward model. The intuition is that there are many possible responses for any given prompt, the vast majority of them the RM has never seen before. For many of those unknown $(x, y)$ pairs, the RM might give an extremely high or low score by mistake. Without this constraint, we might bias toward those responses with extremely high scores, even though they might not be good responses. The following modified reward function in RL training is maximized:</p>\[\begin{aligned} \mathcal{J}_{\operatorname{PPO}}(\phi) &amp;=\mathbb{E}_{(x,y)\sim \mathcal{D}_{\pi_\phi^{\operatorname{RL}}}}\big[r_\theta(x,y)\big]-\beta D_{\operatorname{KL}}(\pi_\phi^{\operatorname{RL}} \mid \mid \pi^{\operatorname{SFT}}) \\ &amp;= \mathbb{E}_{(x,y)\sim \mathcal{D}_{\pi_\phi^{\operatorname{RL}}}}\Big[r_\theta(x,y)-\beta \log \frac{\pi_\phi^{\operatorname{RL}}(y \mid x)}{\pi^{\operatorname{SFT}}(y \mid x)}\Big], \end{aligned}\]<p>where $\pi_\phi^{\operatorname{RL}}$ is the learned RL policy, $\pi^{\operatorname{SFT}}$ is the supervised trained model. The KL-divergence coefficient, $\beta$ controls the strength of the KL penalty. InstructGPT authors also experimented with mixing the pretraining gradients into the PPO gradients, in order to fix the performance regressions on public NLP datasets. They called these models “PPO-ptx”:</p>\[\mathcal{J}_{\operatorname{PPO-ptx}}(\phi) = \mathcal{J}_{\operatorname{PPO}}(\phi) + \gamma \mathbb{E}_{x \sim \mathcal{D}_{\operatorname{pretrain}}}[\log\pi_\phi^{\operatorname{RL}}(x)],\]<p>where $D_{\operatorname{pretrain}}$ is the pretraining distribution. The pretraining loss coefficient $\gamma$ controls strength of pretraining gradients.</p><p>At this stage one has to be careful to avoid <strong>reward hacking</strong> - an effect that lets the agent get more reward than intended by exploiting loopholes in the process determining the reward. An agent can exploit some misspecification in the reward function, e.g. when the reward function incorrectly provides high reward to some undesired behavior. One potential source for such exploit is the reward model’s vulnerability to adversarial inputs. The agent might figure out how to specifically craft these adversarially perturbed inputs in order to trick the reward model into providing higher reward than the user intends.</p><p>Finally, RM stage and RL stage can be iterated continuously: collect more comparison data on the current best policy, then use it to train a new RM and then a new policy. In InstructGPT pipeline, most of comparison data comes from supervised policies, with some coming from PPO policies.</p><h4 id="details-of-rl-algorithm">Details of RL algorithm</h4><p>Consider response generation as a sequence of input states and actions. Let’s denote input state at timestep $t$ as</p>\[s_t=(x, y_1, \dots y_{t-1}),\]<p>where $y_s$ is a response token, generated at timestep $s$ by actor $\pi_\phi$. During rollout at each timestep $t$ RL model is penalized by</p>\[R_{t+1} = -\beta \log \frac{\pi_\phi^{\operatorname{RL}}(y_t \mid s_t)}{\pi^{\operatorname{SFT}}(y_t \mid s_t)}\]<p>until it reaches terminal state and receives final reward</p>\[R_T = r_\theta(x, y)-\beta \log \frac{\pi_\phi^{\operatorname{RL}}(y_{T-1} \mid s_t)}{\pi^{\operatorname{SFT}}(y_{T-1} \mid s_t)}.\]<div id="backup_diagram" class="svg-container" align="center"></div><script> d3.select("#backup_diagram").style("position", "relative"); function state_circle(svg, x, y) { svg.append('circle') .attr('cx', x) .attr('cy', y) .attr('r', 8) .attr('stroke', 'black') .attr('fill', '#FFFFFF'); } function action_circle(svg, x, y) { svg.append('circle') .attr('cx', x) .attr('cy', y) .attr('r', 5) .attr('stroke', 'black') .attr('fill', '#000000'); } function backup_diagram() { var svg = d3.select("#backup_diagram") .append("svg") .attr("width", 520) .attr("height", 80); const g = svg.append("g").attr("id", "node"); state_circle(svg, 50, 50); line(svg, 58, 50, 245, 50, 1, 1); triangle(svg, 240, 50, 90); action_circle(svg, 250, 50); line(svg, 255, 50, 442, 50, 1, 1); triangle(svg, 437, 50, 90); state_circle(svg, 450, 50); svg.append('text') .attr('x', 15) .attr('y', 75) .text('I think, therefore I') .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 242) .attr('y', 75) .text('am') .style("font-size", "11px") .style("fill", "#FA8072") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 385) .attr('y', 75) .text('I think, therefore I am') .style("font-size", "11px") .attr("font-family", "Arvo"); d3.select("#backup_diagram") .append("div") .text("\\(s_{t} \\)") .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .style("position", "absolute") .style("left", "160px") .style("top", "15px"); d3.select("#backup_diagram") .append("div") .text("\\(y_{t} \\)") .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .style("position", "absolute") .style("left", "360px") .style("top", "15px"); d3.select("#backup_diagram") .append("div") .text("\\(R_{t+1} \\)") .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .style("position", "absolute") .style("left", "455px") .style("top", "25px"); d3.select("#backup_diagram") .append("div") .text("\\(s_{t+1} \\)") .style("font-size", "13px") .style("font-weight", "700") .attr("font-family", "Arvo") .style("position", "absolute") .style("left", "555px") .style("top", "15px"); } backup_diagram(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Backup diagram illustrating one step of sequence generation in terms of reinforcement learning</em></p><p>In general we are interested in maximizing <strong>return</strong>, which is a total sum of rewards going forward:</p>\[G_t = R_{t+1} + R_{t+2} + \dots + R_T.\]<p>It is also common to use the discounted return:</p>\[G_t = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-t} R_T = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1},\]<p>where the discounting factor $\gamma \in [0, 1]$ trades off later rewards to earlier ones.</p><p>Besides actor model, we must have a critic $V_\theta(s_t)$, which estimates $G_t$ and is trained with mean-squared loss:</p>\[\mathcal{L}_V(\theta) = \mathbb{E}_t[(V_\theta(s_t) - G_t)^2]\]<p>While actor is initialized from SFT model, critic can also be initialized from RM model. With critic network estimating the return under current policy actor network learns to maximize the probability of getting a positive <strong>advantage</strong>, the relative value of selected response token:</p>\[\hat{A}_t = G_t - V_\theta(s_t).\]<p>The equality above<sup id="fnref:AE" role="doc-noteref"><a href="#fn:AE" class="footnote" rel="footnote">2</a></sup> can be used for return calculation, but such estimator would have low bias but high variance. Usually returns and advantages are estimated with a technique called <strong>bootstrapping</strong>, e.g. with <strong>generalized version of advantage estimation (GAE)</strong>, popularized by <a href="https://arxiv.org/pdf/1506.02438.pdf">Schulman et. al (2018)</a>:</p>\[\begin{aligned} \hat{A}_t = \delta_t + (\gamma \lambda) \delta_{t+1} + \dots = \sum_{k=0}^{T-t-1} (\gamma \lambda)^k \delta_{t+k}, \\ \text{where } \delta_t = R_{t+1} + \gamma V_\theta(s_{t+1}) - V_\theta(s_t) \end{aligned}\]<p>and $\lambda \in [0,1]$ is a decay factor, penalizing high variance estimates. I recommend <a href="https://danieltakeshi.github.io/2017/04/02/notes-on-the-generalized-advantage-estimation-paper/">this nice post</a> to grasp the idea of GAE.</p><p>To train actor network vanilla policy gradient loss can be used:</p>\[\mathcal{L}^{\operatorname{PG}}(\phi) = -\mathbb{E}_t[\log \pi_\phi(y_t \mid s_t) \hat{A}_t].\]<p>This way, when $\hat{A}_t$ is positive, meaning that the action agent took resulted in a better than average return, we increase the probability of selecting it again in the future. On the other hand, if an advantage is negative, we reduce the likelihood of selected action.</p><p>However, consider the case of optimizing target policy $\pi_\phi$, when the behaviour policy $\pi_{\phi_{\text{old}}}$ is used for collecting trajectories with $\phi_{\text{old}}$ policy parameters before the update. In an original PPO paper it is stated, that while it is appealing to perform multiple steps of optimization on this loss using the same trajectory, doing so is not well-justified, and empirically it often leads to destructively large policy updates. In other words, we have to impose the constraint which won’t allow our new policy to move too far away from an old one. Let $\rho_t(\phi)$ denote the probability ratio between target and behaviour policies:</p>\[\rho_t(\phi) = \frac{\pi_\phi(y_t \mid x)}{\pi_{\phi_{\operatorname{old}}}(y_t \mid x)},\]<p>so $\rho_t(\phi_{\operatorname{old}}) = 1$. Then we can minimize the surrogate objective function (the superscript CPI refers to <strong>conservative policy iteration</strong>)</p>\[\mathcal{L}^{\text{CPI}}(\phi) = -\mathbb{E}_t[\rho_t(\phi) \hat{A}_t].\]<p>Indeed,</p>\[\begin{aligned} \nabla_\phi\mathcal{L}^{\operatorname{PG}}(\phi) \big \vert_{\phi_{\text{old}}} &amp;= -\mathbb{E}_t\big[\nabla_\phi\log \pi_\phi(y_t \mid x)\big \vert_{\phi_{\text{old}}} \hat{A}_t\big] \\ &amp;=-\mathbb{E}_t\bigg[\frac{\nabla_\phi \pi_\phi(y_t \mid s_t) \big \vert_{\phi_{\text{old}}}}{\pi_{\phi_{\text{old}}}(y_t \mid s_t)} \hat{A}_t\bigg] \\ &amp;= -\mathbb{E}_t\big[\nabla_\phi \rho_t(\phi) \big \vert_{\phi_{\text{old}}} \hat{A}_t\big] \\ &amp;= \nabla_\phi\mathcal{L}^{\text{CPI}}(\phi). \end{aligned}\]<p>Now, we would like to insert the aforementioned constraint into this loss function. The main objective which authors of PPO propose is the following.</p>\[\mathcal{L}^{\text{CLIP}}(\phi) = -\mathbb{E}_t\Big[\min \big(\rho_t(\phi) \hat{A}_t, \text{clip}(\rho_t(\phi), 1-\epsilon, 1+\epsilon)\hat{A}_t\big)\Big],\]<p>where $\epsilon$ is a <strong>clip ratio</strong> hyperparameter. The first term inside $\min$ function, $\rho_t(\phi) \hat{A}_t$ is a normal policy gradient objective. And the second one is its clipped version, which doesn’t allow us to destroy our current policy based on a single estimate, because the value of $\hat{A}_t$ is noisy (as it is based on an output of our network).</p><p>The total objective is a combination of clipped loss and error term on the value estimation</p>\[\mathcal{L}(\phi, \theta) = \mathcal{L}^{\text{CLIP}}(\phi) + c\mathcal{L}_V(\theta).\]<div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="c1"># actor - policy neural network
# params - learnable parameters
# states[B] - batch of input states
# token_ids[B] - generated response token indices
# advantages[B] - batch of estimated advantages
# logp_old[B] - log-probabilities of generated tokens according to behaviour policy
# eps - clip ratio, usually around 0.2
</span>
<span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">actor_loss</span><span class="p">(</span><span class="n">actor</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">,</span> <span class="n">advantages</span><span class="p">,</span> <span class="n">logp_old</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="n">logp_dist</span> <span class="o">=</span> <span class="n">actor</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
    <span class="n">logp</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">lp</span><span class="p">[</span><span class="n">y</span><span class="p">]</span> <span class="k">for</span> <span class="n">lp</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">logp_dist</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">)])</span>
    <span class="n">ratio</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">logp</span> <span class="o">-</span> <span class="n">logp_old</span><span class="p">)</span>
    <span class="n">clip_adv</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">clip</span><span class="p">(</span><span class="n">ratio</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">eps</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">*</span> <span class="n">advantages</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="nf">min</span><span class="p">(</span><span class="n">ratio</span> <span class="o">*</span> <span class="n">advantages</span><span class="p">,</span> <span class="n">clip_adv</span><span class="p">)</span>
    
<span class="c1"># critic - value prediction neural network
# returns[B] - batch of discounted returns
</span>
<span class="nd">@jax.jit</span>
<span class="k">def</span> <span class="nf">critic_loss</span><span class="p">(</span><span class="n">critic</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">returns</span><span class="p">):</span>
    <span class="n">values</span> <span class="o">=</span> <span class="n">critic</span><span class="p">.</span><span class="nf">apply</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">values</span> <span class="o">-</span> <span class="n">returns</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
</pre></table></code></div></div><h4 id="note-on-kl-approximations">Note on KL approximations</h4><p>John Schulman, author of PPO algorithm, proposes different estimators of KL-divergence $D_{\operatorname{KL}}(\pi’ \mid \mid \pi)$ in his <a href="http://joschu.net/blog/kl-approx.html">blogpost</a>. Let $\kappa=\frac{\pi(x)}{\pi’(x)}$, then for $\pi \approx \pi’$ we get empirically:</p><div class="table-wrapper"><table><thead><tr><th> <th>Estimation<th>Bias<th>Variance<tbody><tr><td>$k_1$<td>$-\log \kappa$<td>$0$<td>High<tr><td>$k_2$<td>$\frac{1}{2}(\log \kappa)^2$<td>Low<td>Low<tr><td>$k_3$<td>$(\kappa-1)-\log \kappa$<td>$0$<td>Low</table></div><p>The main advantage of $k_2$ and $k_3$ estimators is the zero probability of getting negative values. Although, with true KL-divergence between $\pi$ and $\pi’$ getting larger we can observe that bias for $k_2$ and variance for $k_3$ are increasing as well.</p><h3 id="gpt-chatbot-limitations">GPT chatbot limitations</h3><p><img data-proofer-ignore data-src="/assets/img/shoggoth.jpg" alt="Shoggoth" /> <em><a href="https://thorehusfeldt.com/2023/03/02/reinforcement-learning-using-human-feedback-is-putting-smileys-on-a-shoggoth/">Is RLHF just putting smileys on a shoggoth?</a></em></p><p>AI alignment may be difficult and ambiguous to assess. While being powerful tools, GPT assistants can still output harmful or factually inaccurate text without any uncertainty. OpenAI <a href="https://openai.com/blog/chatgpt">admits</a> that ChatGPT sometimes gives convincing-sounding answers that are incorrect or even complete nonsense. Fixing this issue is a long-term challenge, as:</p><ul><li>During RLHF training model operates in an inherently human problem domain with no source of truth.<li>Supervised training misleads the model because the ideal answer depends on what the model knows, rather than what the human demonstrator knows</ul><p>The effect of an AI making up facts is called <strong>hallucination</strong>. Avoiding this effect through training the model to be more cautious can cause it to decline questions that it can answer correctly. In such situation we would say that we’ve been over-optimizing for <strong>harmlessness</strong>, while under-optimizing <strong>helpfulness</strong>. Putting restrictions on the system to avoid systemic biases as well as unethical opinions that might exist within the training dataset is a reason why it’s currently hard to have nuanced conversations with ChatGPT about controversial, or sensitive, subjects.</p><p>The problem involves the definition of harmlessness – if simply refusing to answer a question is the ‘least harmful’ behavior, then this is probably both very easy to learn, and hard to improve on. That said, a more interesting ‘least harmful’ behavior would involve the model (helpfully) explaining why the request was harmful, and perhaps even trying to convince the human not to pursue such requests. Anthropic informally refer to such a model as a ‘hostage negotiator’.</p><p>Another problem is model robustness. ChatGPT is sensitive to tweaks to the input phrasing or attempting the same prompt multiple times. For example, given one phrasing of a question, the model can claim to not know the answer, but given a slight rephrase, can answer correctly. Moreover, GPT models are stochastic – there’s no guarantee that LLM will give you the same output for the same input every time.</p><p>That being said, we’re still in the early days of GPT applications. We do not have much experience applying RL techniques to large generative models and many things about LLMs, including RLHF, will evolve. There are a large variety of tweaks and tricks that require experimentation to identify, and that can majorly improve the stability and performance of training. And many methods could be tried to further decrease the models’ propensity to generate toxic, biased, or otherwise harmful outputs.</p><p>OpenAI states that one of the biggest open questions is how to design an alignment process that is transparent, that meaningfully represents the people impacted by the technology, and that synthesizes peoples’ values in a way that achieves broad consensus amongst many groups. And I personally believe that finding an answer to this rather general question will be one of the major goals of the future works on AI.</p><hr /><div class="footnotes" role="doc-endnotes"><ol><li id="fn:GELU" role="doc-endnote"><p>Authors of original GPT-1 paper were using Gaussian Error Linear Unit (GELU) activation in Feed-Forward layers</p>\[\operatorname{GELU}(x) = x \Phi(x).\]<p><a href="#fnref:GELU" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:AE" role="doc-endnote"><p>The hat sign means that $\hat{A}_t$ is an estimator of the true advantage, which is:</p>\[A_t = \mathbb{E}_t[G_t \mid s_t, y_t] - \mathbb{E}_t[G_t \mid s_t].\]<p><a href="#fnref:AE" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/generative-ai/'>Generative AI</a>, <a href='/categories/large-language-models/'>Large Language Models</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/transformer/" class="post-tag no-text-decoration" >transformer</a> <a href="/tags/llm/" class="post-tag no-text-decoration" >llm</a> <a href="/tags/gpt/" class="post-tag no-text-decoration" >gpt</a> <a href="/tags/chatgpt/" class="post-tag no-text-decoration" >chatgpt</a> <a href="/tags/rlhf/" class="post-tag no-text-decoration" >rlhf</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Building Aligned Intelligence System. Part I: Creating GPT Assistant - AstraBlog&url=https://astralord.github.io/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Building Aligned Intelligence System. Part I: Creating GPT Assistant - AstraBlog&u=https://astralord.github.io/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Building Aligned Intelligence System. Part I: Creating GPT Assistant - AstraBlog&url=https://astralord.github.io/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://astralord.github.io/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/">Building Aligned Intelligence System. Part I: Creating GPT Assistant</a><li><a href="/posts/exploring-parallel-strategies-with-jax/">Exploring Parallel Strategies with Jax</a><li><a href="/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/">Building Aligned Intelligence System. Part II. Improving Large Language Models</a><li><a href="/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/">Visual Guide to Statistics. Part I: Basics of Point Estimation</a><li><a href="/posts/visual-guide-to-statistics-part-ii-bayesian-statistics/">Visual Guide to Statistics. Part II: Bayesian Statistics</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/parameter-estimation/">parameter estimation</a> <a class="post-tag" href="/tags/asymptotic-efficiency/">asymptotic efficiency</a> <a class="post-tag" href="/tags/bartlett-test/">bartlett test</a> <a class="post-tag" href="/tags/bayes-estimator/">bayes estimator</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/central-limit-theorem/">central limit theorem</a> <a class="post-tag" href="/tags/chain-of-thought/">chain-of-thought</a> <a class="post-tag" href="/tags/chatgpt/">chatgpt</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/"><div class="card-body"> <span class="timeago small" >Jul 23, 2023<i class="unloaded">2023-07-23T19:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Building Aligned Intelligence System. Part II. Improving Large Language Models</h3><div class="text-muted small"><p> In this post we will look at different techniques for steering LLMs behaviour to get desired outcomes, starting with some basic general principles such as writing a good prompt and ending ...</p></div></div></a></div><div class="card"> <a href="/posts/power-of-diffusion-models/"><div class="card-body"> <span class="timeago small" >Sep 25, 2022<i class="unloaded">2022-09-25T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Power of Diffusion Models</h3><div class="text-muted small"><p> In 2022, insanely beautiful and original images created with generative neural networks are taking the internet by storm. This post focuses on the theory behind diffusion models that underpin th...</p></div></div></a></div><div class="card"> <a href="/posts/exploring-parallel-strategies-with-jax/"><div class="card-body"> <span class="timeago small" >Jan 27<i class="unloaded">2024-01-27T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Exploring Parallel Strategies with Jax</h3><div class="text-muted small"><p> Training large language models either like GPT, LlaMa or Mixtral requires immense computational resources. With model sizes ballooning into the billions or sometimes even trillions of parameters...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/power-of-diffusion-models/" class="btn btn-outline-primary" prompt="Older"><p>Power of Diffusion Models</p></a> <a href="/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/" class="btn btn-outline-primary" prompt="Newer"><p>Building Aligned Intelligence System. Part II. Improving Large Language Models</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/astralord">Aleksandr Samarin</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/parameter-estimation/">parameter estimation</a> <a class="post-tag" href="/tags/asymptotic-efficiency/">asymptotic efficiency</a> <a class="post-tag" href="/tags/bartlett-test/">bartlett test</a> <a class="post-tag" href="/tags/bayes-estimator/">bayes estimator</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/central-limit-theorem/">central limit theorem</a> <a class="post-tag" href="/tags/chain-of-thought/">chain of thought</a> <a class="post-tag" href="/tags/chatgpt/">chatgpt</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://astralord.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
