<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Exploring Parallel Strategies with Jax" /><meta property="og:locale" content="en" /><meta name="description" content="Training large language models either like GPT, LlaMa or Mixtral requires immense computational resources. With model sizes ballooning into the billions or sometimes even trillions of parameters, specialized parallelization techniques are essential to make training feasible. In this post, we’ll explore implementing some of these scaling strategies in Jax - a Python framework designed for high-performance numerical computing with support for accelerators like GPU and TPU." /><meta property="og:description" content="Training large language models either like GPT, LlaMa or Mixtral requires immense computational resources. With model sizes ballooning into the billions or sometimes even trillions of parameters, specialized parallelization techniques are essential to make training feasible. In this post, we’ll explore implementing some of these scaling strategies in Jax - a Python framework designed for high-performance numerical computing with support for accelerators like GPU and TPU." /><link rel="canonical" href="https://astralord.github.io/posts/exploring-parallel-strategies-with-jax/" /><meta property="og:url" content="https://astralord.github.io/posts/exploring-parallel-strategies-with-jax/" /><meta property="og:site_name" content="AstraBlog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2024-01-27T06:00:00+03:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Exploring Parallel Strategies with Jax" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-01-31T14:29:34+03:00","datePublished":"2024-01-27T06:00:00+03:00","description":"Training large language models either like GPT, LlaMa or Mixtral requires immense computational resources. With model sizes ballooning into the billions or sometimes even trillions of parameters, specialized parallelization techniques are essential to make training feasible. In this post, we’ll explore implementing some of these scaling strategies in Jax - a Python framework designed for high-performance numerical computing with support for accelerators like GPU and TPU.","headline":"Exploring Parallel Strategies with Jax","mainEntityOfPage":{"@type":"WebPage","@id":"https://astralord.github.io/posts/exploring-parallel-strategies-with-jax/"},"url":"https://astralord.github.io/posts/exploring-parallel-strategies-with-jax/"}</script><title>Exploring Parallel Strategies with Jax | AstraBlog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="AstraBlog"><meta name="application-name" content="AstraBlog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/marvel-icon.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">AstraBlog</a></div><div class="site-subtitle font-italic">A place to learn and share knowledge about AI-related things</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/astralord" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['samarin_ad','mail.ru'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/aleksandr-samarin-b8a35496" aria-label="linkedin" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://t.me/astrlrd" aria-label="telegram" class="order-6" target="_blank" rel="noopener"> <i class="fab fa-telegram"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Exploring Parallel Strategies with Jax</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Exploring Parallel Strategies with Jax</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Aleksandr Samarin </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sat, Jan 27, 2024, 6:00 AM +0300" >Jan 27<i class="unloaded">2024-01-27T06:00:00+03:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Wed, Jan 31, 2024, 2:29 PM +0300" >Jan 31<i class="unloaded">2024-01-31T14:29:34+03:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="6866 words">38 min read</span></div></div><div class="post-content"><blockquote><p>Training large language models either like GPT, LlaMa or Mixtral requires immense computational resources. With model sizes ballooning into the billions or sometimes even trillions of parameters, specialized parallelization techniques are essential to make training feasible. In this post, we’ll explore implementing some of these scaling strategies in Jax - a Python framework designed for high-performance numerical computing with support for accelerators like GPU and TPU.</p></blockquote><h3 id="tensors-sharding">Tensors sharding</h3><p>Jax is a great fit for implementing parallel LLM training thanks to its high-level APIs for composing parallel functions and its seamless acceleration on GPU/TPU hardware. We’ll walk through code examples for data, tensor, pipeline and expert parallelisms in Jax while training a “toy” FFN model for demonstration. The insights from this exercise will help us understand how state-of-the-art systems actually parallelize and distribute LLM training in practice.</p><h4 id="device-placement">Device placement</h4><p>Let’s discover now how to run particular operations on a device of your choice. Don’t worry if you don’t have multiple GPUs, an arbitrary number of devices can be emulated even with single CPU by setting <code class="language-plaintext highlighter-rouge">--xla_force_host_platform_device_count</code> flag:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">jax</span>
<span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="c1"># Use 8 CPU devices
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">XLA_FLAGS</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">--xla_force_host_platform_device_count=8</span><span class="sh">'</span>
</pre></table></code></div></div><p>With this little trick <code class="language-plaintext highlighter-rouge">jax.devices()</code> now shows us eight “different” devices:</p><div class="language-plaintext highlighter-rouge"><div class="code-header" text-data="plaintext"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>[CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3),
 CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7)]
</pre></table></code></div></div><p>Let’s create a small empty tensor <code class="language-plaintext highlighter-rouge">x</code> and observe its physical location:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="n">batch_size</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">8</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">device</span><span class="p">())</span> <span class="c1"># will output default device, e.g. TFRT_CPU_0
</span></pre></table></code></div></div><p>To put tensor <code class="language-plaintext highlighter-rouge">x</code> on specific device one can simply use <code class="language-plaintext highlighter-rouge">device_put</code> function:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">jax</span><span class="p">.</span><span class="nf">devices</span><span class="p">()[</span><span class="mi">1</span><span class="p">]).</span><span class="nf">device</span><span class="p">()</span> <span class="c1"># TFRT_CPU_1
</span></pre></table></code></div></div><p>What if we want to place different parts of our tensor on different devices? There is a technique called <strong>tensor sharding</strong>: we split <code class="language-plaintext highlighter-rouge">x</code> by multiple sub-tensors and place each on its own device. But firstly, we need to create a <code class="language-plaintext highlighter-rouge">sharding</code> object, which is basically a device placement configuration:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">jax.sharding</span> <span class="kn">import</span> <span class="n">PositionalSharding</span>

<span class="n">sharding</span> <span class="o">=</span> <span class="nc">PositionalSharding</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">devices</span><span class="p">())</span>
</pre></table></code></div></div><p>We can split our tensor <code class="language-plaintext highlighter-rouge">x</code> in multiple ways. For example, we can split it column-wise along embedding dimension:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">G</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">local_device_count</span><span class="p">()</span>
<span class="n">sharded_x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sharding</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">G</span><span class="p">))</span>
</pre></table></code></div></div><p>If we print <code class="language-plaintext highlighter-rouge">sharded_x.devices()</code> it will give us a list of all devices, which is not very informative since it tells us nothing about our tensor sharding. Luckily, we have <code class="language-plaintext highlighter-rouge">visualize_array_sharding</code> function from <code class="language-plaintext highlighter-rouge">jax.debug</code> which gives us a pretty visual idea on how <code class="language-plaintext highlighter-rouge">x</code> is sharded:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">jax.debug</span> <span class="kn">import</span> <span class="n">visualize_array_sharding</span>
<span class="kn">import</span> <span class="n">matplotlib</span> <span class="k">as</span> <span class="n">mpl</span>

<span class="k">def</span> <span class="nf">visualize</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">color_map</span><span class="o">=</span><span class="sh">"</span><span class="s">Set3</span><span class="sh">"</span><span class="p">):</span>
    <span class="nf">visualize_array_sharding</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">color_map</span><span class="o">=</span><span class="n">mpl</span><span class="p">.</span><span class="n">colormaps</span><span class="p">[</span><span class="n">color_map</span><span class="p">])</span>
   
<span class="nf">visualize</span><span class="p">(</span><span class="n">sharded_x</span><span class="p">)</span>
</pre></table></code></div></div><p><img data-proofer-ignore data-src="/assets/img/8_cpus_col.png" alt="Column-wise shard" class="w-50" /> <em>Column-wise sharding of tensor with 8 emulated devices.</em></p><p>There are various other ways to shard our tensor: we can split it along batch dimension or, even more, arrange our devices in 4x2 mesh and mesh both axes:</p><p><img data-proofer-ignore data-src="/assets/img/8_cpus_mesh.png" alt="Different sharding" class="w-25" /> <em>Some other ways to shard tensor.</em></p><p>Another way to place a tensor on devices that we need to look at before moving forward is <strong>tensor replication</strong>. Replicating tensor means that several devices will store their own copies of the whole tensor <code class="language-plaintext highlighter-rouge">x</code>:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">replicated_x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sharding</span><span class="p">.</span><span class="nf">replicate</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="nf">visualize</span><span class="p">(</span><span class="n">replicated_x</span><span class="p">,</span> <span class="n">color_map</span><span class="o">=</span><span class="sh">"</span><span class="s">Pastel2_r</span><span class="sh">"</span><span class="p">)</span>
</pre></table></code></div></div><p><img data-proofer-ignore data-src="/assets/img/8_cpus_repl.png" alt="Replicated weights" class="w-25" /> <em>Device placement for replicated tensor $x$.</em></p><p>One can also combine sharding with replicating:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">combined_x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sharding</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">G</span> <span class="o">//</span> <span class="mi">2</span><span class="p">).</span><span class="nf">replicate</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="nf">visualize</span><span class="p">(</span><span class="n">combined_x</span><span class="p">)</span>
</pre></table></code></div></div><p><img data-proofer-ignore data-src="/assets/img/8_combined.png" alt="Replication with sharding" class="w-25" /> <em>Device placement for sharded and replicated tensor $x$.</em></p><p>We will follow the similar way for visualization</p><script src="https://d3js.org/d3.v4.min.js"></script><link href="https://fonts.googleapis.com/css?family=Arvo" rel="stylesheet" /><div id="lgnd" class="svg-container" align="center"></div><script> colors = ['#C7E9E3', '#ECECEC', '#FFF6B7', '#FDBFB9', '#D9EFB5', '#FFFFDA', '#E6F5E2', '#FEDAB1']; function min(a, b) { return a < b ? a : b; } function cell(svg, x, y, color, opacity=1.0) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 14) .attr('height', 14) .attr('stroke', 'black') .attr('stroke-width', 1) .attr('opacity', opacity) .attr("rx", 3) .attr('fill', color); } function dotted_cell(svg, x, y, color, mark=0) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 14) .attr('height', 14) .attr('stroke', 'black') .attr('stroke-width', 1) .attr("rx", 3) .attr('fill', color); if (mark == 1) { mini_cdot(svg, x + 3.75, y + 12); } else if (mark == 2) { mini_cdot(svg, x + 1.5, y + 12); mini_cdot(svg, x + 6, y + 12); } else if (mark == 3) { mini_cdot(svg, x + 3.75, y + 10.5); mini_cdot(svg, x + 1.5, y + 13.5); mini_cdot(svg, x + 6, y + 13.5); } } function shaded_cell(svg, x, y, color, mark=false) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 14) .attr('height', 14) .attr('stroke', 'black') .attr('stroke-width', 1) .attr("rx", 3) .attr('fill', color); if (mark) { for (var i = 1; i < 5; i += 1) { svg.append("path") .datum([{x: x + 3 * i, y: y + 1}, {x: x + 1, y: y + 3 * i}]) .attr("fill", "none") .attr("stroke-width", 0.5) .attr("stroke", 'black') .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); } for (var i = 1; i < 4; i += 1) { svg.append("path") .datum([{x: x + 3 * i, y: y + 13}, {x: x + 13, y: y + 3 * i}]) .attr("fill", "none") .attr("stroke-width", 0.5) .attr("stroke", 'black') .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); } } } function gate(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 56) .attr('height', 28) .attr('stroke', 'black') .attr('stroke-width', 1) .attr("rx", 3) .attr('fill', colors[4]); svg.append('text') .attr('x', x + 7) .attr('y', y + 18) .text("Gating") .style("font-size", "13px") .attr("font-family", "Arvo"); } function tensor(svg, x, y, w, h, color) { for (var i = 0; i < w; i += 1) { for (var j = 0; j < h; j += 1) { cell(svg, x + i * 16, y + j * 16, color); } } } function dotted_tensor(svg, x, y, w, h, color, mark=1) { for (var i = 0; i < w; i += 1) { for (var j = 0; j < h; j += 1) { dotted_cell(svg, x + i * 16, y + j * 16, color, mark); } } } function rplc_tensor(svg, x, y, w, h) { tensor(svg, x, y, w, h, 'lightgrey'); } function shrd_tensor(svg, x, y, w, h, xsh, ysh) { xsplit = ~~(w / xsh); ysplit = ~~(h / ysh); for (var i = 0; i < ysh; i += 1) { for (var j = 0; j < xsh; j += 1) { tensor(svg, x + j * 16 * xsplit, y + i * 20 * ysplit, xsplit, ysplit, colors[i * xsh + j]); } } } function shrd_tensor_no_split(svg, x, y, w, h, xsh, ysh) { xsplit = ~~(w / xsh); ysplit = ~~(h / ysh); for (var i = 0; i < ysh; i += 1) { for (var j = 0; j < xsh; j += 1) { tensor(svg, x + j * 16 * xsplit, y + i * 16 * ysplit, xsplit, ysplit, colors[i * xsh + j]); } } } function relu(svg, x, y) { svg.append('circle') .attr('cx', x) .attr('cy', y) .attr('r', 10) .attr('stroke', 'black') .attr('stroke-width', 1) .attr('fill', 'none'); svg.append("path") .datum([{x: x - 9, y: y + 1}, {x: x, y: y + 1}, {x: x + 7, y: y - 6}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); } function softmax(svg, x, y) { svg.append('circle') .attr('cx', x) .attr('cy', y) .attr('r', 10) .attr('stroke', 'black') .attr('stroke-width', 1) .attr('fill', 'none'); svg.append("path") .datum([{x: x - 9, y: y + 1}, {x: x - 5, y: y - 1}, {x: x, y: y - 9}, {x: x + 5, y: y - 1}, {x: x + 9, y: y + 1}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); } function aggregate(svg, x, y, h) { svg.append("path") .attr("stroke", "black") .datum([{x: x, y: y}, {x: x + 5, y: y}, {x: x + 5, y: y + h}, {x: x, y: y + h}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('text') .attr('x', x + 5) .attr('y', y + h / 2 + 2) .text("⟶") .style("font-size", "12px") .attr("font-family", "Arvo"); } function split(svg, x, y, h) { svg.append("path") .attr("stroke", "black") .datum([{x: x + 5, y: y}, {x: x, y: y}, {x: x, y: y + h}, {x: x + 5, y: y + h}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); } function right_arrow(svg, x, y) { svg.append('text') .attr('x', x) .attr('y', y) .text("⟶") .style("font-size", "11px") .attr("font-family", "Arvo"); } function circle_arrow(svg, x, y) { svg.append('text') .attr('x', x) .attr('y', y) .text("↻") .style("font-size", "11px") .attr("font-family", "Arvo"); } function grad_right_arrow(svg, x, y) { svg.append('text') .attr('x', x + 2) .attr('y', y - 8) .text("∇") .style("font-size", "9px") .attr("font-family", "Arvo"); right_arrow(svg, x, y); } function cdot(svg, x, y) { svg.append('text') .attr('x', x) .attr('y', y) .text("⋅") .style("font-size", "21px") .attr("font-family", "Arvo"); } function mini_cdot(svg, x, y) { svg.append('text') .attr('x', x) .attr('y', y) .text("⋅") .style("font-size", "16px") .attr("font-family", "Arvo"); } function cdots(svg, x, y) { svg.append('text') .attr('x', x) .attr('y', y) .text("⋅⋅⋅") .style("font-size", "11px") .attr("font-family", "Arvo"); } function sync(svg, x, y) { svg.append('text') .attr('x', x) .attr('y', y) .text("⇅") .style("font-size", "11px") .attr("font-family", "Arvo"); } function long_right_arrow(svg, x_sh, y_sh, length=50) { svg.append("path") .datum([{x: x_sh, y: y_sh}, {x: x_sh + length, y: y_sh}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .datum([{x: x_sh + length - 3, y: y_sh - 1}, {x: x_sh + length, y: y_sh}, {x: x_sh + length - 3, y: y_sh + 1}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); } function long_down_arrow(svg, x_sh, y_sh, length=50) { svg.append("path") .datum([{x: x_sh, y: y_sh}, {x: x_sh, y: y_sh + length}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .datum([{x: x_sh - 1, y: y_sh + length - 3}, {x: x_sh, y: y_sh + length}, {x: x_sh + 1, y: y_sh + length - 3}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); } function upright_arrow(svg, x_sh, y_sh) { svg.append("path") .datum([{x: x_sh, y: y_sh}, {x: x_sh + 10, y: y_sh - 10}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .datum([{x: x_sh + 7, y: y_sh - 8}, {x: x_sh + 10, y: y_sh - 10}, {x: x_sh + 8, y: y_sh - 7}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); } function legend() { var svg = d3.select("#lgnd") .append("svg") .attr("width", 600) .attr("height", 90); svg.append('text') .attr('x', 130) .attr('y', 20) .text("Sharded") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 415) .attr('y', 20) .text("Replicated") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('line') .attr('x1', 300) .attr('y1', 0) .attr('x2', 300) .attr('y2', 90) .style("stroke-width", 3) .attr("opacity", 0.7) .attr('stroke', 'black'); shrd_tensor(svg, 127, 40, 4, 2, 4, 1); rplc_tensor(svg, 370, 40, 4, 2, 4, 1); rplc_tensor(svg, 470, 40, 4, 2, 4, 1); svg.append('rect') .attr('x', 365) .attr('y', 35) .attr('width', 170) .attr('height', 40) .attr('stroke', 'black') .attr("rx", 3) .style('stroke-dasharray', ('2,3')) .attr('fill', 'none'); } legend(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Visualization of tensors locations. On the left side - tensor is split column-wise by 4 subtensors, each located on its designated device. On the right side - tensor is copied on 2 devices.</em></p><h4 id="parallel-processing">Parallel processing</h4><p>Let’s create a simple feed-forward layer (FFN), which is one of the core components in modern LLMs. It consists of two linear layers and an activation function between them. If we omit bias and use ReLU as activation between layers, then FFN can be written as</p>\[\operatorname{FFN}(x) = \max(0, x\mathbf{W}_1)\mathbf{W}_2,\]<p>where $x \in \mathbb{R}^{B \times d}$, $\mathbf{W}_1 \in \mathbb{R}^{d \times h}$ and $\mathbf{W}_2 \in \mathbb{R}^{h \times d}$. We will refer to $B$, $d$ and $h$ as to batch size, embedding and hidden dimensions respectively.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="n">jax</span> <span class="kn">import</span> <span class="n">jit</span><span class="p">,</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">NamedTuple</span>
<span class="kn">from</span> <span class="n">jax._src.typing</span> <span class="kn">import</span> <span class="n">ArrayLike</span>

<span class="k">class</span> <span class="nc">Params</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">w1</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span>
    <span class="n">w2</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Params</span><span class="p">):</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">params</span><span class="p">.</span><span class="n">w1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">z</span> <span class="o">@</span> <span class="n">params</span><span class="p">.</span><span class="n">w2</span>
</pre></table></code></div></div><p>Let us also define additional mock functions for data sampling (both features and labels) and FFN weights initialization:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">init_ffn_weights</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">rng</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
        Create FFN weights with Xavier initialization
    </span><span class="sh">'''</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="n">embed_dim</span> <span class="o">+</span> <span class="n">hidden_dim</span><span class="p">))</span>
    <span class="n">w1_key</span><span class="p">,</span> <span class="n">w2_key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">std</span> <span class="o">*</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">w1_key</span><span class="p">,</span> <span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="n">std</span> <span class="o">*</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">w2_key</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
    <span class="k">return</span> <span class="nc">Params</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">rng</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
        Create random features `x` and dependable random targets `y`
    </span><span class="sh">'''</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">w</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span>
</pre></table></code></div></div><p>Now we can run forward pass through FFN layer in Jax simply like this:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="c1"># set up toy example hyper-parameters
</span><span class="n">B</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">32</span>
<span class="c1"># create random keys
</span><span class="n">data_key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">weight_key</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">sample_data</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">data_key</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="nf">init_ffn_weights</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">weight_key</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</pre></table></code></div></div><p>Here <code class="language-plaintext highlighter-rouge">x</code> is stored on one single device and so does <code class="language-plaintext highlighter-rouge">y_pred</code>. Recall that <code class="language-plaintext highlighter-rouge">x</code> is basically a stack of <code class="language-plaintext highlighter-rouge">B</code> features with size <code class="language-plaintext highlighter-rouge">d</code>. It means that we can split this stack along batch dimension and send each part to its own device and process them in parallel. And we already know how to accomplish that:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">sharded_x</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sharding</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="nf">visualize</span><span class="p">(</span><span class="nf">ffn</span><span class="p">(</span><span class="n">sharded_x</span><span class="p">,</span> <span class="n">params</span><span class="p">))</span>
</pre></table></code></div></div><h3 id="data-parallelism">Data Parallelism</h3><p><strong>Data Parallel (DP)</strong> is a relatively simple strategy, but it allows scaling to large data batches: the training data is partitioned across $G$ distributed workers and fed to the model, which is replicated. The training process is done in parallel: dataloader spits out a batch of the total size $B$, then each worker computes activations, loss values $\ell$ and model gradients with its independent data split of size $S=\frac{B}{G}$. Gradients are then synchronized at the end of each training step before the weights update, so that all workers observe consistent model parameters throughout training.</p><div id="data_prll" class="svg-container" align="center"></div><script> d3.select("#data_prll").style("position", "relative"); function data_parallel() { var svg = d3.select("#data_prll") .append("svg") .attr("width", 700) .attr("height", 455); x_start = 50; y_shift = 100; k = 4; svg.append('text') .attr('x', x_start + 11.5) .attr('y', 20) .text("x") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 73) .attr('y', 43) .text("W₁") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 273) .attr('y', 23) .text("W₂") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 434) .attr('y', 23) .text("ℓ") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 490) .attr('y', 23) .text("∂ℓ/∂W₂") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 582) .attr('y', 43) .text("∂ℓ/∂W₁") .style("font-size", "13px") .attr("font-family", "Arvo"); shrd_tensor(svg, x_start, 30, 2, 5 * k, 1, k); for (var i = 0; i < k; i += 1) { svg.append('text') .attr('x', x_start - 50) .attr('y', 74 + i * y_shift) .text("GPU " + i) .style("font-size", "13px") .attr("font-family", "Arvo"); cdot(svg, x_start + 36, 75 + i * y_shift); rplc_tensor(svg, x_start + 50, 54 + i * y_shift, 4, 2); right_arrow(svg, x_start + 130, 71 + i * y_shift); } svg.append('rect') .attr('x', x_start + 45) .attr('y', 50) .attr('width', 72) .attr('height', 85 * k) .attr('stroke', 'black') .attr("rx", 3) .style('stroke-dasharray', ('2,3')) .attr('fill', 'none'); shrd_tensor(svg, x_start + 156, 30, 4, 5 * k, 1, k); for (var i = 0; i < k; i += 1) { relu(svg, x_start + 237, 68 + i * y_shift); cdot(svg, x_start + 252, 75 + i * y_shift); rplc_tensor(svg, x_start + 267, 38 + i * y_shift, 2, 4); right_arrow(svg, x_start + 316, 71 + i * y_shift); } svg.append('rect') .attr('x', x_start + 262) .attr('y', 30) .attr('width', 40) .attr('height', 95 * k) .attr('stroke', 'black') .attr("rx", 3) .style('stroke-dasharray', ('2,3')) .attr('fill', 'none'); shrd_tensor(svg, x_start + 344, 30, 2, 5 * k, 1, k); for (var i = 0; i < k; i += 1) { cdots(svg, x_start + 387, 72 + i * y_shift); aggregate(svg, x_start + 412, 28 + i * y_shift, 82); cell(svg, x_start + 432, 62 + i * y_shift, colors[i]); grad_right_arrow(svg, x_start + 449, 71 + i * y_shift); cdots(svg, x_start + 472, 72 + i * y_shift); tensor(svg, x_start + 498, 38 + i * y_shift, 2, 4, colors[i]); if (i < k - 1) { sync(svg, x_start + 508, 123 + i * y_shift); } grad_right_arrow(svg, x_start + 547, 71 + i * y_shift); tensor(svg, x_start + 575, 54 + i * y_shift, 4, 2, colors[i]); if (i < k - 1) { sync(svg, x_start + 600, 123 + i * y_shift); } } svg.append("path") .datum([{x: x_start - 50, y: 430}, {x: x_start + 650, y: 430}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .datum([{x: x_start + 647, y: 429}, {x: x_start + 650, y: 430}, {x: x_start + 647, y: 431}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('text') .attr('x', 205) .attr('y', 450) .text("Forward") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 570) .attr('y', 450) .text("Backward") .style("font-size", "14px") .attr("font-family", "Arvo"); } data_parallel(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Data Parallel strategy with 4 devices. Embedding and hidden dimensions $d$ and $h$ are equal to 2 and 4 respectively. Each device runs computations with its own separate shard of size $S$ equal to 5</em></p><p>Let’s build and example of a regression training loop with data parallelism. First, we build a deep neural network, consisting of $L$ FFN layers with residual connections (to prevent outputs degenerating to zeros due to ReLU activation). We also set up loss criteria and dataset and weights initialization functions.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
</pre><td class="rouge-code"><pre><span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">Params</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">+=</span> <span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>
            
<span class="nd">@jit</span> 
<span class="k">def</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y_true</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">y_true</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">Params</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    
<span class="k">def</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span>
        <span class="nf">sample_data</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> 
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">)</span>
    <span class="p">])</span>

<span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">layer_num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">rng</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
        Create weights for a stack of `layer_num` FFN layers
    </span><span class="sh">'''</span>
    <span class="n">layer_keys</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">layer_num</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="nf">init_ffn_weights</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">,</span> <span class="n">layer_keys</span><span class="p">[</span><span class="n">l</span><span class="p">])</span> 
            <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">layer_num</span><span class="p">)]</span>
</pre></table></code></div></div><p>We’ve seen how to perform simple parallel operations manually, e.g. batching a simple FFN forward pass across several devices. JAX also supports automatic device parallelism: we can use <code class="language-plaintext highlighter-rouge">jax.pmap</code> to transform a function written for one device into a function that runs in parallel on multiple devices.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">G</span> <span class="o">*</span> <span class="n">d</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="c1"># dummy sample
# replicate model weights
</span><span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">params</span><span class="p">)</span>
<span class="nf">visualize</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">pmap</span><span class="p">(</span><span class="n">ffn</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">'</span><span class="s">G</span><span class="sh">'</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">))</span>
</pre></table></code></div></div><p>Here’s how <code class="language-plaintext highlighter-rouge">pmap</code> works: <code class="language-plaintext highlighter-rouge">ffn()</code> takes data tensors of shape <code class="language-plaintext highlighter-rouge">[B, ...]</code> and computes the output of FFN layer on that batch. We want to spread the batch dimension across all available devices. To do that, we add a new axis. The arguments to the wrapped <code class="language-plaintext highlighter-rouge">ffn()</code> thus need to have shape <code class="language-plaintext highlighter-rouge">[G, B/G, ...]</code>. So, to call <code class="language-plaintext highlighter-rouge">ffn()</code>, we’ll need to reshape data batches so that what used to be batch is reshaped to <code class="language-plaintext highlighter-rouge">[G, B/G, ...]</code>. That’s what <code class="language-plaintext highlighter-rouge">split()</code> does below.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="n">arr</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">num_sections</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">axis</span><span class="p">:</span> <span class="nb">int</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">split</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">num_sections</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">))</span>
</pre></table></code></div></div><p>Additionally, we’ll need to replicate our model parameters, adding the <code class="language-plaintext highlighter-rouge">G</code> axis. This reshaping is how a pmapped function knows which devices to send which data.</p><p>With all that being said, we still need to send gradient information between the devices. For that, we can use <a href="https://jax.readthedocs.io/en/latest/jax.lax.html#parallel-operators">special collective ops</a> such as the <code class="language-plaintext highlighter-rouge">jax.lax.p*</code> ops <code class="language-plaintext highlighter-rouge">psum</code>, <code class="language-plaintext highlighter-rouge">pmean</code>, <code class="language-plaintext highlighter-rouge">pmax</code>, etc. In order to use the collective ops we must specify the name of the <code class="language-plaintext highlighter-rouge">pmap</code>-ed axis through <code class="language-plaintext highlighter-rouge">axis_name</code> argument, and then refer to it when calling the op. Here is a function <code class="language-plaintext highlighter-rouge">update()</code>, which runs forward and backward calculations, updates model parameters and all of it is done in parallel on different devices:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">functools</span>

<span class="c1"># Remember that the 'G' is just an arbitrary string label used
# to later tell 'jax.lax.pmean' which axis to reduce over. Here, we call it
# 'G', but could have used anything, so long as 'pmean' used the same.
</span><span class="nd">@functools.partial</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="n">pmap</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">'</span><span class="s">G</span><span class="sh">'</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="n">Params</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="c1"># Compute the gradients on the given minibatch (individually on each device)
</span>    <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># Combine the gradient across all devices (by taking their mean)
</span>    <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">pmean</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">'</span><span class="s">G</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Also combine the loss. Unnecessary for the update, but useful for logging
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">pmean</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">axis_name</span><span class="o">=</span><span class="sh">'</span><span class="s">G</span><span class="sh">'</span><span class="p">)</span>

    <span class="c1"># Each device performs its own update, but since we start with the same params
</span>    <span class="c1"># and synchronise gradients, the params stay in sync
</span>    <span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">1e-3</span>
    <span class="n">new_params</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">tree_map</span><span class="p">(</span>
       <span class="k">lambda</span> <span class="n">param</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">param</span> <span class="o">-</span> <span class="n">g</span> <span class="o">*</span> <span class="n">LEARNING_RATE</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">grads</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">new_params</span><span class="p">,</span> <span class="n">loss</span>
</pre></table></code></div></div><p>During the update step, we need to combine the gradients computed by each device – otherwise, the updates performed by each device would be different. That’s why we use <code class="language-plaintext highlighter-rouge">jax.lax.pmean</code> to compute the mean across the <code class="language-plaintext highlighter-rouge">G</code> axis, giving us the average gradient of the batch. That average gradient is what we use to compute the update.</p><p>Combining all together, we can now create a full train cycle with data parallel strategy:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">train_with_data_parallel</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">local_device_count</span><span class="p">()</span>
    <span class="c1"># replicate model weights
</span>    <span class="n">replicated_params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">jax</span><span class="p">.</span><span class="nf">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">param</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">p</span><span class="p">)</span> 
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="nf">for </span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
            <span class="c1"># shard data batch
</span>            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">G</span><span class="p">),</span> <span class="nf">split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">G</span><span class="p">)</span>
            <span class="n">replicated_params</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">update</span><span class="p">(</span><span class="n">replicated_params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="c1"># note that loss is actually an array of shape [G], with identical
</span>            <span class="c1"># entries, because each device returns its copy of the loss
</span>            <span class="c1"># visualize(loss) will show [CPU 0, CPU 1, ..., CPU G]
</span>            <span class="n">avg_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Step </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="n">avg_loss</span> <span class="o">/</span> <span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">replicated_params</span>
</pre></table></code></div></div><p>We only need to set hyper-parameters, define training dataset and initialize weights to finally call the training function.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="c1"># set G = 4 devices
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="sh">"</span><span class="s">XLA_FLAGS</span><span class="sh">"</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">--xla_force_host_platform_device_count=4</span><span class="sh">'</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">B</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">L</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">16</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="nf">create_dataset</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Dataset size:</span><span class="sh">'</span><span class="p">,</span> <span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="c1"># [N, 2, B, d]
</span><span class="n">params</span> <span class="o">=</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">random</span><span class="p">.</span><span class="nc">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">))</span>
<span class="nf">train_with_data_parallel</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>
</pre></table></code></div></div><p>And we can observe the process of our model being trained:</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="n">Dataset</span> <span class="n">size</span><span class="p">:</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">Step</span>   <span class="mi">5</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.233</span>
<span class="n">Step</span>  <span class="mi">10</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.184</span>
<span class="n">Step</span>  <span class="mi">15</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.127</span>
<span class="n">Step</span>  <span class="mi">20</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.117</span>
<span class="n">Step</span>  <span class="mi">25</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.111</span>
<span class="n">Step</span>  <span class="mi">30</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.106</span>
<span class="n">Step</span>  <span class="mi">35</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.103</span>
<span class="n">Step</span>  <span class="mi">40</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.100</span>
<span class="n">Step</span>  <span class="mi">45</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.098</span>
<span class="n">Step</span>  <span class="mi">50</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.097</span>
</pre></table></code></div></div><p>The main problem with DP approach is that during the backward pass all the gradients must be transferred to the all other devices. For example, with $\mathbf{W}_1 \in \mathbb{R}^{d \times h}$ weight matrix in float32 the number of $32 \cdot d \cdot h$ bits have to be sent between each pair of devices. The same amount is needed for $\mathbf{W}_2$. If we work in a multi-node setup with $v$ GBit/s of network card bandwidth, we’ll need</p>\[t = \frac{64 \cdot d \cdot h}{v \cdot 1024^3}\]<p>seconds to send the gradients for FFN layer from one node to another (plus an additional overhead $\delta t$ that is neglected here). Given the substantial amount of data communication required in DP, a fast connection (interconnect) between computing devices is necessary. While DP may work for TPU device networks scaling up to pod levels, modern GPUs predominantly have fast interconnectivity only within a group of 8 devices hosted on the same system. Inter-GPU communication is considerably slower across separate hosts.</p><p>There is an example: imagine that we use data parallelism for LLM pretraining and we need to give a rough estimation of the time required for backward calculations. In GPT-3 the embedding size $d$ is 12⋅1024 with hidden size $h=4d$. <a href="https://news.microsoft.com/source/features/innovation/openai-azure-supercomputer/">Microsoft built a supercomputer exclusively for OpenAI</a> with 10,000 GPUs and 400 GBit/s of network connectivity between nodes. Plugging in these numbers we get</p>\[t = \frac{64 \cdot 12 \cdot 4 \cdot 12 \cdot 1024^2}{400 \cdot 1024^3} = 90\mbox{ms}\]<p>just to transfer FFN gradients. As there are 96 FFN layers in GPT-3, it’ll take about 9 seconds for this part of gradient synchronization. And this is just to send data from one node to another, while there might be dozens, hundreds or even thousands nodes with all-to-all communication cost growing quadratically. Easily we can see that data parallelism does not scale with size of the cluster and cannot be used in isolation for large models.</p><p>The described strategy above is also called <strong>Distributed Data Parallel (DDP)</strong> and it is different from <a href="https://huggingface.co/docs/transformers/perf_train_gpu_many#data-parallelism">HuggingFace definition of data paralelism</a>. HuggingFace version of DP helps to overcome slow intra-node connectivity by minimizing the amount of synchronized data and delegating a lot of data/gradient processing to one leading GPU. This, in turn, results in under-utilization of other devices.</p><p>Another common strategy for amortizing communication cost is <strong>gradient accumulation</strong>. We can run multiple forward and backward propagations and accumulate local gradients on each device in parallel before launching data synchronization and taking optimizer step. Additionally, performance can be improved by synchronizing the computed gradients for some tensors while simultaneously computing gradients for anothers.</p><h3 id="model-parallelism">Model parallelism</h3><p>With the advent of large neural networks that do not fit on one device, the need to parallelize models has increased. This is especially true in the case of LLMs, whose number of parameters can exceed the already considerable amount of input data. To distribute computation and memory we can split the model across multiple devices and across multiple dimensions.</p><h4 id="tensor-parallelism">Tensor Parallelism</h4><p>The idea of sharding, the way it was applied to data tensors, can be used in a similar way with respect to the model weights. We can divide each tensor $\mathbf{W}$ into chunks distributed across multiple devices, so instead of having the whole tensor reside on a single device, each shard of the tensor resides on its own accelerator. Each part gets processed separately in parallel on different devices and after processing the results are synced at the end of the step.</p><p>Such strategy is called <strong>Tensor Parallel (TP)</strong> or horizontal parallelism, as the splitting happens on horizontal level (we will get to the vertical/pipeline parallelism later). A simple and efficient way to parallelize FFN calculations was proposed in <a href="https://arxiv.org/pdf/1909.08053.pdf">Megatron-LM</a> paper. Let’s represent matrices $\mathbf{W}_1$ and $\mathbf{W}_2$ as concatenation of $G$ sub-tensors along rows and columns respectively:</p>\[\mathbf{W}_1 = \begin{pmatrix} \color{#8ED3C7}{\mathbf{W}_1^1} &amp; \color{#D9D9D9}{\mathbf{W}_1^2} &amp; \cdots &amp; \color{#FDBFB9}{\mathbf{W}_1^G} \end{pmatrix}, \quad \mathbf{W}_2 = \begin{pmatrix} \color{#8ED3C7}{\mathbf{W}_2^1} \\ \color{#D9D9D9}{\mathbf{W}_2^2} \\ \vdots \\ \color{#FDBFB9}{\mathbf{W}_2^G} \end{pmatrix}\]<p>with $\mathbf{W}_1^k \in \mathbb{R}^{d \times \frac{h}{G}}$, $\mathbf{W}_2^k \in \mathbb{R}^{\frac{h}{G}\times d}$ for $k = 1, \dots, G$. Then with $x$ replicated over devices we can perform sub-tensors multiplications in parallel:</p>\[x\mathbf{W}_1=\begin{pmatrix} x\color{#8ED3C7}{\mathbf{W}_1^1} &amp; x\color{#D9D9D9}{\mathbf{W}_1^2} &amp;\cdots &amp; x\color{#FDBFB9}{\mathbf{W}_1^G} \end{pmatrix}\]<p>and for $z = \max(x\mathbf{W}_1, 0)$ we have</p>\[z\mathbf{W}_2=z {\color{#8ED3C7}{\mathbf{W}_2^1}} + z {\color{#D9D9D9}{\mathbf{W}_2^2}} + \cdots + z {\color{#FDBFB9}{\mathbf{W}_2^G}}.\]<p>The model weights on different devices do not overlap, and the only communication between devices occurs at the end of the FFN layer when we need to sum all the outputs. We can already see the advantage of TP over DP here: computational costs for each device decreases drastically with growing number of devices. On the other hand, the deficiency of TP is that the input data is replicated, so that the batch size per device is now equal to the total batch size. Hence if we are restricted by GPU memory we have to reduce our $B$. Otherwise, we can increase our $S$ by a factor of $G$ to keep up with the same batch size $B=S \times G$ as in DP strategy.</p><div id="tnsr_prll" class="svg-container" align="center"></div><script> function tensor_parallel() { var svg = d3.select("#tnsr_prll") .append("svg") .attr("width", 650) .attr("height", 540); x_start = 50; y_start = 110; y_shift = 100; k = 4; svg.append('text') .attr('x', x_start + 11.5) .attr('y', y_start - 20) .text("x") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 218) .attr('y', 40) .text("W₁") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 240) .attr('y', 40) .text("=") .style("font-size", "15px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 348) .attr('y', 40) .text("W₂") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 370) .attr('y', 40) .text("=") .style("font-size", "15px") .attr("font-family", "Arvo"); shrd_tensor(svg, x_start + 250, 20, 4, 2, 4, 1); shrd_tensor_no_split(svg, x_start + 380, 5, 2, 4, 1, 4); svg.append('text') .attr('x', 140) .attr('y', 40) .text("Sharded weights:") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('rect') .attr('x', 260) .attr('y', 1) .attr('width', 210) .attr('height', 72) .attr('stroke', 'black') .attr("rx", 3) .attr('fill', 'none'); svg.append('text') .attr('x', x_start + 49) .attr('y', y_start + 5) .text("W₁") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 174) .attr('y', y_start + 5) .text("W₂") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 508) .attr('y', y_start + 5) .text("∂ℓ/∂W₁") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 440) .attr('y', y_start + 5) .text("∂ℓ/∂W₂") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 347) .attr('y', y_start + 175) .text("ℓ") .style("font-size", "13px") .attr("font-family", "Arvo"); for (var i = 0; i < k; i += 1) { svg.append('text') .attr('x', x_start - 50) .attr('y', x_start + 103 + i * y_shift) .text("GPU " + i) .style("font-size", "13px") .attr("font-family", "Arvo"); rplc_tensor(svg, x_start, y_start + i * y_shift, 2, 5); cdot(svg, x_start + 36, y_start + 45 + i * y_shift); tensor(svg, x_start + 50, y_start + 24 + i * y_shift, 1, 2, colors[i]); right_arrow(svg, x_start + 78, y_start + 41 + i * y_shift); } svg.append('rect') .attr('x', x_start - 5) .attr('y', y_start - 10) .attr('width', 40) .attr('height', 100 * k) .attr('stroke', 'black') .attr("rx", 3) .style('stroke-dasharray', ('2,3')) .attr('fill', 'none'); shrd_tensor(svg, x_start + 103, y_start, 1, 20, 1, 4); for (var i = 0; i < k; i += 1) { relu(svg, x_start + 136, y_start + 38 + i * y_shift); cdot(svg, x_start + 151, y_start + 45 + i * y_shift); tensor(svg, x_start + 167, y_start + 32 + i * y_shift, 2, 1, colors[i]); right_arrow(svg, x_start + 211, y_start + 41 + i * y_shift); } shrd_tensor(svg, x_start + 236, y_start, 2, 20, 1, 4); for (var i = 0; i < k - 1; i += 1) { sync(svg, x_start + 246, y_start + 93 + i * y_shift); cdot(svg, x_start + 270, y_start + 45 + (i + 1) * y_shift); } for (var i = 0; i < k; i += 1) { cdots(svg, x_start + 295, y_start + 42 + i * y_shift); } aggregate(svg, x_start + 318, y_start - 2, 382); cell(svg, x_start + 345, y_start + 182, 'lightgrey'); grad_right_arrow(svg, x_start + 366, y_start + 191); split(svg, x_start + 389, y_start - 2, 382); for (var i = 0; i < k; i += 1) { cdots(svg, x_start + 400, y_start + 42 + i * y_shift); grad_right_arrow(svg, x_start + 422, y_start + 41 + i * y_shift); tensor(svg, x_start + 447, y_start + 32 + i * y_shift, 2, 1, colors[i]); grad_right_arrow(svg, x_start + 492, y_start + 41 + i * y_shift); tensor(svg, x_start + 522, y_start + 24 + i * y_shift, 1, 2, colors[i]); } svg.append('text') .attr('x', x_start + 247) .attr('y', y_start - 15) .text("d") .style("font-size", "13px") .attr("font-family", "Arvo"); long_right_arrow(svg, x_start + 240, y_start - 10, length=20); svg.append('text') .attr('x', x_start + 280) .attr('y', y_start + 43) .text("S") .style("font-size", "13px") .attr("font-family", "Arvo"); long_down_arrow(svg, x_start + 275, y_start + 15, length=50); svg.append("path") .datum([{x: x_start - 50, y: 515}, {x: x_start + 550, y: 515}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .datum([{x: x_start + 547, y: 514}, {x: x_start + 550, y: 515}, {x: x_start + 547, y: 516}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('text') .attr('x', 160) .attr('y', 540) .text("Forward") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 500) .attr('y', 540) .text("Backward") .style("font-size", "14px") .attr("font-family", "Arvo"); } tensor_parallel(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Tensor Parallel strategy with batch size $B$ set to 5. Each device stores its own part of model weights. Activations are aggregated after each FFN layer.</em></p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">train_with_tensor_parallel</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">local_device_count</span><span class="p">()</span>
    <span class="n">sharded_params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nc">Params</span><span class="p">(</span><span class="n">w1</span><span class="o">=</span><span class="nf">split</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">w1</span><span class="p">,</span> <span class="n">num_sections</span><span class="o">=</span><span class="n">G</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> 
               <span class="n">w2</span><span class="o">=</span><span class="nf">split</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">w2</span><span class="p">,</span> <span class="n">num_sections</span><span class="o">=</span><span class="n">G</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="nf">for </span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
            <span class="c1"># replicate data batch
</span>            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">x</span><span class="p">]</span> <span class="o">*</span> <span class="n">G</span><span class="p">),</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">y</span><span class="p">]</span> <span class="o">*</span> <span class="n">G</span><span class="p">)</span>
            <span class="n">sharded_params</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">update</span><span class="p">(</span><span class="n">sharded_params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">avg_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Step </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="n">avg_loss</span> <span class="o">/</span> <span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sharded_params</span>
</pre></table></code></div></div><p>Since in TP the size of synchronized weights equals to the batch size $B$ multiplied by embedding size $d$, when we operate with float32, each device sends $32 \cdot d \cdot B$ bits to each other device. Thus, the amount of memory transfer between each pair of devices is $O(dh)$ for DP versus $O(dB)$ for TP. We can conclude, that DP is a preferable strategy for small networks (e.g. model can fit onto one device), while TP works better with larger models and smaller batches.</p><h4 id="hybrid-data-and-model-tensor-parallelism">Hybrid data and model tensor parallelism</h4><p>It is common to mix both data and tensor parallelism for large scale models. With a total of $G=\operatorname{TP}\times \operatorname{DP}$ devices, each device stores $\frac{B}{\operatorname{DP}}$ embedding vectors and $\frac{h}{\operatorname{TP}}$ of both the weights and intermediate activations.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">train_with_hybrid_parallel</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">,</span> <span class="n">DP</span><span class="p">,</span> <span class="n">TP</span><span class="p">):</span>
    <span class="n">sharded_params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="nc">Params</span><span class="p">(</span><span class="n">w1</span><span class="o">=</span><span class="nf">split</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">w1</span><span class="p">,</span> <span class="n">num_sections</span><span class="o">=</span><span class="n">TP</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> 
               <span class="n">w2</span><span class="o">=</span><span class="nf">split</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">w2</span><span class="p">,</span> <span class="n">num_sections</span><span class="o">=</span><span class="n">TP</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span>
    <span class="p">]</span>
    <span class="n">hybrid_params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">jax</span><span class="p">.</span><span class="nf">tree_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">param</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">tile</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="p">(</span><span class="n">DP</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">p</span><span class="p">)</span> 
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">sharded_params</span>
    <span class="p">]</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">avg_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="nf">for </span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
            <span class="c1"># shard and then replicate data batch
</span>            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">DP</span><span class="p">),</span> <span class="nf">split</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">DP</span><span class="p">)</span>
            <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">TP</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">repeat</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">TP</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">hybrid_params</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="nf">update</span><span class="p">(</span><span class="n">hybrid_params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
            <span class="n">avg_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">mean</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
        <span class="nf">if </span><span class="p">(</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">5</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Step </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">:</span><span class="mi">3</span><span class="n">d</span><span class="si">}</span><span class="s">, loss: </span><span class="si">{</span><span class="n">avg_loss</span> <span class="o">/</span> <span class="n">dataset</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hybrid_params</span>
</pre></table></code></div></div><h4 id="pipeline-parallelism">Pipeline Parallelism</h4><p>Suppose our neural network, a stack of $L$ FFN layers, is so deep, that it doesn’t fit on a single device. This scenario is practical because a common way to scale up models is to stack layers of the same pattern. It might feel straightforward for us to split our model by layer and that is what <strong>Pipeline Parallel (PP)</strong> strategy does. It splits up the model weights vertically, so that only a small group of consecutive layers of the model are placed on a single device.</p><div id="pp_prll" class="svg-container" align="center"></div><script> d3.select("#pp_prll").style("position", "relative"); function pipeline_arrow(svg, x, y, y_shift) { svg.append("path") .datum([{x: x, y: y}, {x: x + 5, y: y}, {x: x + 10, y: y}, {x: x + 10, y: y + 0.5 * y_shift}, {x: x + 10, y: y + y_shift}, {x: x + 15, y: y + y_shift}, {x: x + 20, y: y + y_shift}]) .attr("fill", "none") .attr("stroke-width", 0.8) .attr("stroke", 'black') .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .datum([{x: x + 18, y: y + y_shift - 1}, {x: x + 20, y: y + y_shift}, {x: x + 18, y: y + y_shift + 1}]) .attr("fill", "none") .attr("stroke-width", 0.8) .attr("stroke", 'black') .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); } function pipeline_arrow_grad(svg, x, y, y_shift) { svg.append("path") .datum([{x: x, y: y + y_shift}, {x: x + 5, y: y + y_shift}, {x: x + 10, y: y + y_shift}, {x: x + 10, y: y + 0.5 * y_shift}, {x: x + 10, y: y}, {x: x + 15, y: y}, {x: x + 20, y: y}]) .attr("fill", "none") .attr("stroke-width", 0.8) .attr("stroke", 'black') .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .datum([{x: x + 18, y: y - 1}, {x: x + 20, y: y}, {x: x + 18, y: y + 1}]) .attr("fill", "none") .attr("stroke-width", 0.8) .attr("stroke", 'black') .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('text') .attr('x', x) .attr('y', y + y_shift / 2) .text("∇") .style("font-size", "9px") .attr("font-family", "Arvo"); } function stage(svg, x, y, id, width=80) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', width) .attr('height', 40) .attr('stroke', 'black') .attr('stroke-width', 1) .attr("rx", 3) .attr('fill', colors[id]); svg.append('text') .attr('x', x + width / 2 - 22) .attr('y', y + 24) .text("Stage " + id) .style("font-size", "13px") .attr("font-family", "Arvo"); } function pipeline_parallel() { var svg = d3.select("#pp_prll") .append("svg") .attr("width", 700) .attr("height", 360); x_start = 50; y_start = 20; x_shift = 80; y_shift = 80; k = 4; for (var i = 0; i < k; i += 1) { svg.append('text') .attr('x', x_start - 50) .attr('y', y_start + 43 + i * y_shift) .text("GPU " + i) .style("font-size", "13px") .attr("font-family", "Arvo"); stage(svg, x_start + i * x_shift, y_start + 20 + i * y_shift, i, 50); if (i < k - 1) { pipeline_arrow(svg, x_start + i * x_shift + 55, y_start + 40 + i * y_shift, y_shift); svg.append("path") .datum([{x: x_start + i * x_shift + 75, y: y_start + 40 + i * y_shift}, {x: x_start + (k - i) * x_shift + 245, y: y_start + 40 + i * y_shift}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .style('stroke-dasharray', ('2,3')) .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .datum([{x: x_start + (k - i) * x_shift + 245, y: y_start + 39 + i * y_shift}, {x: x_start + (k - i) * x_shift + 248, y: y_start + 40 + i * y_shift}, {x: x_start + (k - i) * x_shift + 245, y: y_start + 41 + i * y_shift}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); pipeline_arrow_grad(svg, x_start + (k - i) * x_shift + 245, y_start + 40 + i * y_shift, y_shift); } else { right_arrow(svg, x_start + i * x_shift + 55, y_start + 41 + i * y_shift); grad_right_arrow(svg, x_start + i * x_shift + 95, y_start + 41 + i * y_shift); } stage(svg, x_start + (k - i) * x_shift + 270, y_start + 20 + i * y_shift, i, 50); } cell(svg, x_start + (k - 1) * x_shift + 74, y_start + 32 + (k - 1) * y_shift, colors[k-1]); svg.append('text') .attr('x', x_start + (k - 1) * x_shift + 76.5) .attr('y', y_start + 27 + (k - 1) * y_shift) .text("ℓ") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append("path") .datum([{x: x_start - 50, y: 335}, {x: x_start + 640, y: 335}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .datum([{x: x_start + 637, y: 334}, {x: x_start + 640, y: 335}, {x: x_start + 637, y: 336}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('text') .attr('x', 205) .attr('y', 355) .text("Forward") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 495) .attr('y', 355) .text("Backward") .style("font-size", "14px") .attr("font-family", "Arvo"); } pipeline_parallel(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Naive Pipeline Parallel strategy. Each stage represent forward/backward pass through its own sequence of $\frac{L}{G}$ FFN layers on each device. It can be seen that running every data batch through multiple workers with sequential dependencies leads to large idle bubbles and severe under-utilization of computation resources.</em></p><p>To implement naive PP strategy in Jax we just have to place layers to their corresponding devices and whenever the data goes in each layer it must be switched to the same device.</p><p>Clearly, the main disadvantage of this type of parallelization is that all but one device is idle at any given moment. In addition, at the end of each stage there is a serious communication overhead for transferring data between devices. To reduce idling problem we have to explore other approaches.</p><h5 id="pipeline-parallel-reduced-to-tensor-sharding">Pipeline Parallel reduced to tensor sharding</h5><p>Since our model only consists of $L$ equal (not shared) layers, when we split it vertically, our pipelining scenario looks like $G$ stages of the same subcomputation except for having different weight values. The similar picture we could’ve seen in TP strategy - every device is doing the same operations but with different operands.</p><p>Let’s reorganize the way we look at our model architecture. If we stack weights from different stages and represent them all together as tensors $\mathbf{W}_1 \in \mathbb{R}^{G \times d \times h}$ and $\mathbf{W}_2 \in \mathbb{R}^{G \times h \times d}$ in each FFN layer, we can shard them across stacked axis, so that these stages can be run in parallel, but with different batches. Although, $k$-th stage still have to wait until output activations from $(k-1)$-th stage are calculated and transferred to the $k$-th device. It means that first $k$ runs for $k$-th stage must be done with emulated input data, e.g. filled with zeros or random values.</p><div id="pp_as_tp" class="svg-container" align="center"></div><script> d3.select("#pp_as_tp").style("position", "relative"); function pipeline_parallel_as_tensor() { var svg = d3.select("#pp_as_tp") .append("svg") .attr("width", 700) .attr("height", 435); x_start = 50; y_start = 20; x_shift = 160; y_shift = 100; k = 4; for (var j = 0; j < 4; j += 1) { for (var i = 0; i < k; i += 1) { if (j == 0) { svg.append('text') .attr('x', x_start - 50) .attr('y', y_start + 43 + i * y_shift) .text("GPU " + i) .style("font-size", "13px") .attr("font-family", "Arvo"); } if (i <= j) { dotted_tensor(svg, x_start + j * x_shift, y_start + i * y_shift, 2, 5, colors[i], j - i); } else { tensor(svg, x_start + j * x_shift, y_start + i * y_shift, 2, 5, 'white'); } right_arrow(svg, x_start + j * x_shift + 35, y_start + 41 + i * y_shift); stage(svg, x_start + j * x_shift + 50, y_start + 20 + i * y_shift, i); if (j < 3) { if (i <= j) { pipeline_arrow(svg, x_start + j * x_shift + 135, y_start + 40 + i * y_shift, y_shift); } } else { cdots(svg, x_start + j * x_shift + 135, y_start + 43 + i * y_shift); } } } svg.append("path") .datum([{x: x_start - 50, y: 410}, {x: x_start + 630, y: 410}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .datum([{x: x_start + 627, y: 409}, {x: x_start + 630, y: 410}, {x: x_start + 627, y: 411}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('text') .attr('x', 330) .attr('y', 435) .text("Forward") .style("font-size", "14px") .attr("font-family", "Arvo"); } pipeline_parallel_as_tensor(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Pipeline Parallel inference with inter-batch parallelism. White cells represent zero-paddings.</em></p><p>The extra iterations are equivalent to the bubbles that describe the idle time due to data dependency, although the waiting devices compute on padded data instead of being idle. One can notice that if we split our batch into multiple micro-batches and enable each stage worker to process one micro-batch simultaneously, idle bubbles become much smaller, compared to naive PP.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">stack_stage_weights</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
        Stack G stages, each containing L/G FFN layers
    </span><span class="sh">'''</span>
    <span class="n">L</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">local_device_count</span><span class="p">()</span>
    <span class="n">stage_layers</span> <span class="o">=</span> <span class="n">L</span> <span class="o">//</span> <span class="n">G</span>
    <span class="n">out_params</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">stage_layers</span><span class="p">):</span>
        <span class="n">w1</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">l</span> <span class="o">+</span> <span class="n">g</span> <span class="o">*</span> <span class="n">stage_layers</span><span class="p">].</span><span class="n">w1</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">G</span><span class="p">)])</span>
        <span class="n">w2</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">stack</span><span class="p">([</span><span class="n">params</span><span class="p">[</span><span class="n">l</span> <span class="o">+</span> <span class="n">g</span> <span class="o">*</span> <span class="n">stage_layers</span><span class="p">].</span><span class="n">w2</span> <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">G</span><span class="p">)])</span>
        <span class="n">out_params</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nc">Params</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">out_params</span>

<span class="k">def</span> <span class="nf">pipeline_inference</span><span class="p">(</span><span class="n">params</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Params</span><span class="p">],</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">M</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
        Split input batch to M micro-batches and run PP forward pass
    </span><span class="sh">'''</span>
    <span class="n">B</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">micro_batch_size</span> <span class="o">=</span> <span class="n">B</span> <span class="o">//</span> <span class="n">M</span>
    <span class="c1"># re-organize weights
</span>    <span class="n">params</span> <span class="o">=</span> <span class="nf">stack_stage_weights</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="c1"># split input data to micro-batches
</span>    <span class="n">x</span> <span class="o">=</span> <span class="nf">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">M</span><span class="p">)</span>
    <span class="c1"># create shifting buffer
</span>    <span class="n">state</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">G</span><span class="p">,</span> <span class="n">micro_batch_size</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">G</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">from_prev_stage</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">jnp</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="mi">0</span><span class="p">),</span> <span class="n">state</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">pmap</span><span class="p">(</span><span class="n">model</span><span class="p">)(</span><span class="n">from_prev_stage</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&gt;=</span> <span class="n">G</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span> <span class="c1"># first micro-batch has passed through the last stage
</span>            <span class="n">y_pred</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y_pred</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span>
</pre></table></code></div></div><p>This is the main idea in <a href="https://arxiv.org/pdf/1811.06965.pdf">GPipe (Huang et al. 2019)</a> paper. During training stage, backward calculations are scheduled in reverse order. Gradients from multiple micro-batches are aggregated and applied synchronously at the end, which guarantees learning consistency and efficiency. Given $M$ evenly split micro-batches, the idle time is $O \big( \frac{G - 1}{M + G - 1} \big)$ amortized over the number of micro-steps.</p><p>To reduce memory footprint <strong>gradient checkpointing</strong> can be applied, meaning that during forward computation, each device only stores output activations at the stage boundaries. During the backward pass on $k$-th device the $k$-th stage forward pass re-computes the rest of activations. While it doubles time, required for forward calculations, it helps to reduce peak activation memory requirement to $O \big(B + \frac{L}{G} \times \frac{B}{M}\big)$. In comparison, memory requirement without PP and gradient checkpointing would be $O(B \times L)$, since computing the gradients requires both the next layer gradients and the cached activations.</p><h3 id="expert-parallelism">Expert Parallelism</h3><p>With <strong>Mixture-of-Experts (MoE)</strong> models, different sub-networks (FFN layers in our case) or so-called “experts” specialize in different parts of the input space. For example, in a language model, some experts may specialize in grammar while others focus on semantic understanding. The key to a mixture of experts is having a gating network $\mathcal{G}$ that assigns different parts of each input to the most relevant experts.</p><p>During training, only the experts assigned to a given input have their parameters updated. This sparse update allows mixture of experts models to scale to thousands or even tens of thousands of experts. Each expert can be updated in parallel by a different set of accelerators without heavy communication overhead.</p><h4 id="mixture-of-expert-routing">Mixture of Expert Routing</h4><p>MoE layer was proposed by <a href="https://arxiv.org/pdf/1701.06538.pdf">Shazeer et al. (2017)</a>. It takes a token representation $x$ and then routes it through gating network $\mathcal{G}$ to determined experts. Say, we have $E$ experts in total, then the output of the MoE layer $y$ is the linearly weighted combination of each expert’s output by the gate value</p>\[y = \sum_{i=1}^E \mathcal{G}(x)_i \cdot \operatorname{FFN}_i(x).\]<p>The simple choice of gating function is to create trainable weight matrix $\mathbf{W}_G$ to produce logits, which are normalized via a softmax distribution over the available experts at that layer:</p>\[\mathcal{G}(x)=\operatorname{softmax}(x\mathbf{W}_{\text{G}}).\]<div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">dense_gating</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">gate_params</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">gate_params</span><span class="p">)</span>
</pre></table></code></div></div><p>However, this choice raises two problems:</p><ul><li>MoE layer with dense control vector $\mathcal{G}(x)$ requires computation of all $E$ experts, even those whose impact to the output may be negligible. It would be more efficient if we didn’t have to compute $\operatorname{FFN}_i(x)$ when $\mathcal{G}(x)_i=0$.<li>Self-reinforcing effect: gating network might favor a few strong experts everytime, leaving the rest of the layers redundant.</ul><p>To overcome these issues we introduce sparsity through $\operatorname{topk}$ function and noise via standard Gaussian variable $\epsilon \sim \mathcal{N}(0, 1)$. The amount of noise per component is controlled by a second trainable weight matrix $\mathbf{W}_{\text{noise}}$. Modified gating function would like like this:</p>\[\mathcal{G}(x)=\operatorname{softmax}(\operatorname{topk}(H(x), k)),\]<p>where</p>\[H(x)_i = (x\mathbf{W}_{\text{G}})_i + \epsilon \cdot \operatorname{softplus}(x \mathbf{W}_{\text{noise}})_i\]<p>and</p>\[\operatorname{topk}(v, k)_i = \begin{cases} v, &amp; \text{if } v_i \text{ is in top } k \text{ elements of } v \\ -\infty, &amp; \text{otherwise.} \end{cases}\]<div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">scatter</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">index</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">src</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
        Scatter function analogous to PyTorch `scatter_`
    </span><span class="sh">'''</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="o">*</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">input</span><span class="p">.</span><span class="n">shape</span><span class="p">),</span> 
                       <span class="n">sparse</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> 
                       <span class="n">indexing</span><span class="o">=</span><span class="sh">'</span><span class="s">ij</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">idx</span><span class="p">[</span><span class="n">dim</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
    <span class="k">return</span> <span class="nb">input</span><span class="p">.</span><span class="n">at</span><span class="p">[</span><span class="nf">tuple</span><span class="p">(</span><span class="n">idx</span><span class="p">)].</span><span class="nf">set</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">index_to_mask</span><span class="p">(</span><span class="n">index</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
        Transform given indices to mask of input shape,
        where mask[index] = True and False otherwise
    </span><span class="sh">'''</span>
    <span class="n">zeros</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">input_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
    <span class="k">return</span> <span class="nf">scatter</span><span class="p">(</span><span class="n">zeros</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="bp">True</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">sparse_gating</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> 
                  <span class="n">gate_params</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span>  
                  <span class="n">topk</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                  <span class="n">noise_weights</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                  <span class="n">rng</span><span class="p">:</span> <span class="n">ArrayLike</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">gate_params</span> 
    <span class="k">if</span> <span class="n">noise_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">rng</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">,</span> <span class="sh">"</span><span class="s">Random seed is required to use noisy gating</span><span class="sh">"</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">rng</span><span class="p">,</span> <span class="n">h</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">eps</span> <span class="o">*</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softplus</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">noise_weights</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">+=</span> <span class="n">noise</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">top_k_ids</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">top_k</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">topk</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="nf">index_to_mask</span><span class="p">(</span><span class="n">top_k_ids</span><span class="p">,</span> <span class="n">h</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="o">-</span><span class="n">jnp</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
</pre></table></code></div></div><p>To help load-balancing and to avoid collapse to using a small number of experts an auxiliary importance loss was proposed. Let’s define an importance of an expert $i$ relative to the batch $\mathcal{B}$ as batchwise sum of the gate values for that expert: $\sum_{x \in \mathcal{B}} \mathcal{G}(x)_i$. Importance loss minimizes the squared <a href="https://en.wikipedia.org/wiki/Coefficient_of_variation">coefficient of variation</a> of importance over experts:</p>\[\ell_{\text{aux}}(\mathcal{B}) = \operatorname{CV} \big( \sum_{x \in \mathcal{B}} \mathcal{G}(x) \big)^2.\]<p>Such constraint encourages all experts to have equal importance values.</p><p>Another important detail is that since each expert network receives only a portion of the training samples, we should try to use as large a batch size as possible in MoE. To improve the throughput MoE can be combined with DP strategy.</p><h4 id="gshard">GShard</h4><p>Additional experts increase the amount of model parameters significantly. Basically MoE layer requires $E$ times more parameters than a single FFN layer (plus gating tensor $\mathbf{W}_{\text{G}}$). But what if we had each expert reside on its own device? <a href="https://arxiv.org/pdf/2006.16668.pdf"><strong>GShard</strong> (Lepikhin et al., 2020)</a> uses the idea of sharding across expert dimension to scale up the MoE transformer model up to 600B parameters. The MoE transformer replaces every other FFN with a MoE layer. All MoE layers are different across devices, while other layers are duplicated.</p><p><img data-proofer-ignore data-src="/assets/img/gshard.png" alt="GShard" /> <em>Illustration of scaling of Transformer Encoder with MoE Layers. (a) The encoder of a standard Transformer model is a stack of self-attention and feed forward layers interleaved with residual connections and layer normalization. (b) By replacing every other feed forward layer with a MoE layer, we get the model structure of the MoE Transformer Encoder. (c) When scaling to multiple devices, the MoE layer is sharded across devices, while all other layers are replicated.</em></p><p>Authors chose to let each token $x$ dispatched to at most two experts. Besides that, there are several improvements for the gating function in GShard:</p><ul><li>To ensure that expert load is balanced, the number of tokens processed by one expert is restricted by threshold $C$ named <strong>expert capacity</strong>. If a token $x$ is routed to an expert that has reached its capacity, the token be considered overflowed and gating output $\mathcal{G}(x)$ degenerates into a zero vector. Such token has its representation $x$ passed on to the next layer via residual connection.<li>Local group dispatching: $\mathcal{G}(\cdot)$ partitions all tokens in a training batch evenly into $G$ groups, each of size $S$ and the expert capacity is enforced on the group level.<li>Random routing: since MoE layer output $y$ is a weighted average, if the 2nd best expert gating weight is small enough, we can skip expert computation. To comply with the capacity constraint, token is dispatched to the expert with with a probability proportional to its weight.<li>Auxiliary loss to avoid experts under-utilization is modified. Let $p(x)$ be dense gating function:</ul>\[p(x) = \operatorname{softmax}(x\mathbf{W}_{\text{G}}).\]<p>Also let $f_i$ be the fraction of tokens in a group $\mathcal{S}$ dispatched to expert $i$:</p>\[f_i = \frac{1}{S} \sum_{x \in \mathcal{S}} \mathbb{1}_{ \lbrace \operatorname{argmax} \mathcal{G}(x) = i \rbrace },\]<p>The goal is to minimize mean squared ratio of tokens per expert: $\frac{1}{E} \sum_{i=1}^E f_i^2$. But since this value is derived from $\operatorname{topk}$ function, it’s non-differentiable, so authors propose to use mean gating weights as differentiable approximation:</p>\[\bar{p}_i = \frac{1}{S} \sum_{x \in \mathcal{S}} p(x)_i \approx f_i.\]<p>Then $\ell_{\text{aux}} = \sum_{i=1}^E f_i \cdot \bar{p}_i$.</p><h4 id="switch-transformer">Switch Transformer</h4><p><a href="https://arxiv.org/pdf/2101.03961.pdf"><strong>Switch Transformer</strong></a> scales the model size even more, up to 1.6 trillion parameters, by replacing FFN layer with a sparse MoE layer in which each input is routed to only one expert network. Authors refer to such routing strategy as a <strong>Switch layer</strong>. Similar to how it is done in GShard each expert has its capacity $C$, which depends on batch size $B$ and number of experts $E$ by formula</p>\[C = \frac{B}{E} \cdot c,\]<p>where $c$ is a capacity factor. A capacity factor greater than 1.0 creates additional buffer to accommodate for when tokens are not perfectly balanced across experts.</p><p><img data-proofer-ignore data-src="/assets/img/switch_capacity.png" alt="Switch capacity factor" /> <em>Each token $x$ is routed to the expert with the highest router probability $p(x)$, but each expert has a fixed batch size $C$. Smaller capacity factor can lead to experts overflow, while larger factor increases computation and communication costs.</em></p><p>Let’s dive into implementation details and describe step-by-step how to combine Switch layer with Data Parallel strategy.</p><ul><li>Switch Transformer is allocated on $G$ devices, which will also correspond to the number of experts $E$. For each token per device gating function locally computes assignments to the experts.</ul><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="c1"># Probabilities for each token of what expert it should be sent to.
# gating_probs shape: [G, S, E]
</span><span class="n">gating_probs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">gate_params</span><span class="p">)</span>

<span class="c1"># Get the top−1 expert for each token. 
# expert_gate is the probability from the gating to top-1 expert
# expert_index is what expert each token is going to be routed to
# expert_gate shape: [G, S]
# expert_index shape: [G, S]
</span><span class="n">expert_gate</span><span class="p">,</span> <span class="n">expert_index</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">top_k</span><span class="p">(</span><span class="n">gating_probs</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">expert_gate</span><span class="p">,</span> <span class="n">expert_index</span> <span class="o">=</span> <span class="n">expert_gate</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(),</span> <span class="n">expert_index</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">()</span>
</pre></table></code></div></div><ul><li>The output is a <strong>dispatch mask</strong>, a 4-D binary tensor of shape $[G, S, E, C]$, which is partitioned across the first dimension and determines expert assignment. On each device $g$ for each token $s$ 2-D slice of dispatch mask contains at most one non-zero element.</ul><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre><span class="c1"># expert_mask shape: [G, S, E]
</span><span class="n">expert_mask</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">expert_index</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">gating_probs</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
    
<span class="c1"># Experts have a fixed capacity C, ensure we do not exceed it. 
# Construct the batch indices, to each expert, with position in expert
# make sure that not more that C examples can be routed to each expert.
</span><span class="n">position_in_expert</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">expert_mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">expert_mask</span>
    
<span class="c1"># Keep only tokens that fit within expert capacity.
</span><span class="n">expert_mask_trunc</span> <span class="o">=</span> <span class="n">expert_mask</span> <span class="o">*</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">less</span><span class="p">(</span><span class="n">position_in_expert</span><span class="p">,</span> <span class="n">expert_capacity</span><span class="p">)</span>
<span class="n">expert_mask_flat</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">expert_mask_trunc</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Mask out the experts that have overflowed the expert capacity.
</span><span class="n">expert_gate</span> <span class="o">*=</span> <span class="n">expert_mask_flat</span>

<span class="c1"># combine_tensor used for combining expert outputs and scaling with gating probability.
# combine_tensor shape: [G, S, E, C]
</span><span class="n">expert_capacity_int</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">ceil</span><span class="p">(</span><span class="n">expert_capacity</span><span class="p">))</span>
<span class="n">combine_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">expert_gate</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span>
                  <span class="n">expert_mask</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">*</span>
                  <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">position_in_expert</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">expert_capacity_int</span><span class="p">))</span>
<span class="n">combine_tensor</span> <span class="o">=</span> <span class="n">combine_tensor</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">:]</span> <span class="c1"># cut 0-dimension which is always 0s
</span><span class="n">dispatch_mask</span> <span class="o">=</span> <span class="n">combine_tensor</span><span class="p">.</span><span class="nf">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
</pre></table></code></div></div><ul><li>This mask is then used to do a gather via Einstein summation with the partitioned input tensor $x$ of size $[G, S, d]$, resulting in the final tensor of shape $[E, G, C, d]$, which is sharded across second dimension. Because each device has its own expert, we do an all-to-all communication of size $[E, C, d]$ to now shard the $E$ dimension instead of the $G$-dimension.</ul><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="c1"># Matmul with large boolean tensor to assign tokens to the correct expert.
# device layout: [G, 1, 1], −&gt; [1, G, 1, 1]
# expert inputs shape: [E, G, C, d]
</span><span class="n">expert_inputs</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">GSEC,GSd-&gt;EGCd</span><span class="sh">"</span><span class="p">,</span> <span class="n">dispatch_mask</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># All−to−All communication. Cores split across G and now we want to split
# across E. This sends tokens, routed locally, to the correct expert now
# split across different cores.
# device layout: [1, G, 1, 1] −&gt; [G, 1, 1, 1]
</span><span class="n">sharding</span> <span class="o">=</span> <span class="nc">PositionalSharding</span><span class="p">(</span><span class="n">jax</span><span class="p">.</span><span class="nf">devices</span><span class="p">())</span>
<span class="n">expert_inputs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">expert_inputs</span><span class="p">,</span> <span class="n">sharding</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></table></code></div></div><ul><li>Next, we run experts computation with re-sharded inputs and perform all-to-all communication once again to shard expert outputs back along $G$ dimension.</ul><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="c1"># Standard FFN computation, where each expert has
# its own unique set of parameters.
# Total unique parameters created: E * (d * h * 2).
# expert_outputs shape: [E, G, C, d]
</span><span class="n">expert_outputs</span> <span class="o">=</span> <span class="nf">ffn</span><span class="p">(</span><span class="n">expert_inputs</span><span class="p">,</span> <span class="n">ffn_params</span><span class="p">)</span>
    
<span class="c1"># All−to−All communication. Cores are currently split across the experts
# dimension, which needs to be switched back to being split across num cores.
# device layout: [G, 1, 1, 1] −&gt; [1, G, 1, 1]
</span><span class="n">expert_outputs</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="nf">device_put</span><span class="p">(</span><span class="n">expert_outputs</span><span class="p">,</span> <span class="n">sharding</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
</pre></table></code></div></div><ul><li>And finally, to get $y$, we average expert outputs based on gating probabilities:</ul><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="c1"># Convert back to input shape and multiply outputs of experts by the gating probability
# expert_outputs shape: [E, G, C, d]
# expert_outputs_combined shape: [G, S, d]
# device layout: [1, G, 1, 1] −&gt; [G, 1, 1]
</span><span class="n">expert_outputs_combined</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">EGCd,GSEC-&gt;GSd</span><span class="sh">"</span><span class="p">,</span> <span class="n">expert_outputs</span><span class="p">,</span> <span class="n">combine_tensor</span><span class="p">)</span>
</pre></table></code></div></div><div id="expert_prll_dsptch" class="svg-container" align="center"></div><script> d3.select("#expert_prll_dsptch").style("position", "relative"); function mark_cell(group, token) { return (token == 0) && (group == 1); } function dispatch_tensor(svg, x_sh, y_sh, add_probs=true) { tensor(svg, x_sh, y_sh, 4, 5, 'white'); cell(svg, x_sh, y_sh, colors[4]); cell(svg, x_sh + 16, y_sh + 16, colors[4]); cell(svg, x_sh + 16, y_sh + 32, colors[4]); cell(svg, x_sh + 48, y_sh + 48, colors[4]); cell(svg, x_sh, y_sh + 64, colors[4]); if (add_probs) { cell(svg, x_sh, y_sh + 32, colors[4], 0.2); cell(svg, x_sh + 32, y_sh + 16, colors[4], 0.2); cell(svg, x_sh + 32, y_sh + 32, colors[4], 0.2); cell(svg, x_sh + 32, y_sh + 64, colors[4], 0.2); cell(svg, x_sh + 48, y_sh + 32, colors[4], 0.2); cell(svg, x_sh, y_sh + 64, colors[4], 0.2); } else { svg.append('text') .attr('x', x_sh + 5) .attr('y', y_sh + 11) .text("1") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_sh + 21) .attr('y', y_sh + 27) .text("1") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_sh + 20) .attr('y', y_sh + 43) .text("2") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_sh + 53) .attr('y', y_sh + 59) .text("1") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_sh + 4) .attr('y', y_sh + 75) .text("2") .style("font-size", "11px") .attr("font-family", "Arvo"); } } function expert_parallel_dispatch() { var svg = d3.select("#expert_prll_dsptch") .append("svg") .attr("width", 600) .attr("height", 615); const num_experts = 4; const expert_capacity = 2; const experts_mapping = { 0: {0: [0, 0], 1: [1, 0], 2: [1, 1], 3: [3, 0], 4: [0, 1]}, 1: {0: [3, 0], 1: [2, 0], 2: [3, 1], 3: [2, 1], 4: [0, 0]}, 2: {0: [3, 0], 1: [0, 0], 2: [0, 1], 3: [3, 1], 4: [2, 0]}, 3: {0: [1, 0], 1: [0, 0], 2: [2, 0], 3: [2, 1], 4: [1, 1]} }; x_start = 85; y_shift = 100; y_start = 215; k = 4; x_sh = x_start - 35; y_sh = y_start - 199; svg.append('text') .attr('x', x_sh - 45) .attr('y', y_sh + 50) .text("Gating:") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('rect') .attr('x', x_sh + 10) .attr('y', y_sh - 15) .attr('width', 530) .attr('height', 145) .attr('stroke', 'black') .attr("rx", 3) .attr('fill', 'none'); svg.append('text') .attr('x', x_sh + 31.5) .attr('y', y_sh + 5) .text("x") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_sh + 87) .attr('y', y_sh + 25) .text("W") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_sh + 101) .attr('y', y_sh + 27) .text("G") .style("font-size", "7px") .attr("font-family", "Arvo"); tensor(svg, x_sh + 20, y_sh + 10, 2, 5, 'lightgray'); cdot(svg, x_sh + 52, y_sh + 56); tensor(svg, x_sh + 64, y_sh + 35, 4, 2, colors[4]); softmax(svg, x_sh + 145, y_sh + 50); right_arrow(svg, x_sh + 160, y_sh + 51); dispatch_tensor(svg, x_sh + 180, y_sh + 10, true); long_right_arrow(svg, x_sh + 250, y_sh + 50); svg.append('text') .attr('x', x_sh + 252) .attr('y', y_sh + 45) .text("Argmax") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_sh + 249) .attr('y', y_sh + 62) .text("CumSum") .style("font-size", "11px") .attr("font-family", "Arvo"); dispatch_tensor(svg, x_sh + 308, y_sh + 10, false); long_right_arrow(svg, x_sh + 380, y_sh + 50); svg.append('text') .attr('x', x_sh + 380) .attr('y', y_sh + 45) .text("Truncate") .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_sh + 382) .attr('y', y_sh + 62) .text("One-hot") .style("font-size", "11px") .attr("font-family", "Arvo"); tensor(svg, x_sh + 449, y_sh + 7, 4, 5, 'white'); cell(svg, x_sh + 465, y_sh + 37, colors[4]); cell(svg, x_sh + 449, y_sh + 71, colors[4]); tensor(svg, x_sh + 444, y_sh + 12, 4, 5, 'white'); cell(svg, x_sh + 444, y_sh + 12, colors[4]); cell(svg, x_sh + 460, y_sh + 28, colors[4]); cell(svg, x_sh + 492, y_sh + 60, colors[4]); svg.append('text') .attr('x', x_sh + 475) .attr('y', y_sh) .text("E") .style("font-size", "13px") .attr("font-family", "Arvo"); long_right_arrow(svg, x_sh + 455, y_sh + 3); svg.append('text') .attr('x', x_sh + 525) .attr('y', y_sh + 55) .text("S") .style("font-size", "13px") .attr("font-family", "Arvo"); long_down_arrow(svg, x_sh + 520, y_sh + 25); svg.append('text') .attr('x', x_sh + 513) .attr('y', y_sh + 103) .text("C") .style("font-size", "13px") .attr("font-family", "Arvo"); upright_arrow(svg, x_sh + 507, y_sh + 97); svg.append('text') .attr('x', x_sh + 158) .attr('y', y_sh + 117) .text("Combine weights") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_sh + 430) .attr('y', y_sh + 117) .text("Dispatch mask") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 11.5) .attr('y', y_start - 35) .text("x") .style("font-size", "13px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 261.5) .attr('y', y_start - 35) .text("x") .style("font-size", "13px") .attr("font-family", "Arvo"); shrd_tensor(svg, x_start, y_start - 25, 2, 5 * k, 1, k); shaded_cell(svg, x_start, y_start + y_shift - 25, colors[1], true); shaded_cell(svg, x_start + 16, y_start + y_shift - 25, colors[1], true); for (var i = 0; i < k; i += 1) { svg.append('text') .attr('x', x_start - 50) .attr('y', y_start + 19 + i * y_shift) .text("GPU " + i) .style("font-size", "13px") .attr("font-family", "Arvo"); right_arrow(svg, x_start + 40, y_start + i * y_shift + 16); gate(svg, x_start + 65, y_start + i * y_shift); svg.append('text') .attr('x', x_start + 133) .attr('y', y_start + i * y_shift + 10) .text("T") .style("font-size", "8px") .attr("font-family", "Arvo"); right_arrow(svg, x_start + 130, y_start + i * y_shift + 16); x_sh = x_start + 155; y_sh = y_start + i * y_shift - 20; if (i == 0) { svg.append('text') .attr('x', x_sh + 35) .attr('y', y_sh - 10) .text("S") .style("font-size", "13px") .attr("font-family", "Arvo"); long_right_arrow(svg, x_sh + 10, y_sh - 5, 60); } for (var j = 0; j < expert_capacity; j += 1) { tensor(svg, x_sh - j * 5, y_sh + j * 5, 5, num_experts, 'white'); for (const [token, value] of Object.entries(experts_mapping[i])) { const [expert, capacity] = value; if (capacity == expert_capacity - 1 - j) { shaded_cell(svg, x_sh - j * 5 + token * 16, y_sh + j * 5 + expert * 16, colors[i], mark_cell(i, token)); } } } cdot(svg, x_start + 236, y_start + i * y_shift + 21); } svg.append('rect') .attr('x', x_start + 60) .attr('y', y_start - 6) .attr('width', 66) .attr('height', 340) .attr('stroke', 'black') .attr("rx", 3) .style('stroke-dasharray', ('2,3')) .attr('fill', 'none'); shrd_tensor(svg, x_start + 250, y_start - 25, 2, 5 * k, 1, k); shaded_cell(svg, x_start + 250, y_start + y_shift - 25, colors[1], true); shaded_cell(svg, x_start + 266, y_start + y_shift - 25, colors[1], true); x_sh = x_start + 320; for (var i = 0; i < k; i += 1) { y_sh = y_start + i * y_shift - 20; right_arrow(svg, x_sh - 30, y_start + i * y_shift + 16); for (var j = 0; j < expert_capacity; j += 1) { tensor(svg, x_sh - j * 5, y_sh + j * 5, 2, num_experts, 'white'); for (const [token, value] of Object.entries(experts_mapping[i])) { const [expert, capacity] = value; if (capacity == expert_capacity - 1 - j) { shaded_cell(svg, x_sh - j * 5, y_sh + j * 5 + expert * 16, colors[i], mark_cell(i, token)); shaded_cell(svg, x_sh - j * 5 + 16, y_sh + j * 5 + expert * 16, colors[i], mark_cell(i, token)); } } } circle_arrow(svg, x_sh + 39, y_start + i * y_shift + 16); } x_sh = x_start + 380; for (var i = 0; i < num_experts; i += 1) { y_sh = y_start + i * y_shift - 20; for (var j = 0; j < expert_capacity; j += 1) { tensor(svg, x_sh - j * 5, y_sh + j * 5, 2, 4, 'white'); for (var ii = 0; ii < k; ii += 1) { for (const [token, value] of Object.entries(experts_mapping[ii])) { const [expert, capacity] = value; if ((expert == i) && (capacity == expert_capacity - 1 - j)) { shaded_cell(svg, x_sh - j * 5, y_sh + j * 5 + ii * 16, colors[ii], mark_cell(ii, token)); shaded_cell(svg, x_sh - j * 5 + 16, y_sh + j * 5 + ii * 16, colors[ii], mark_cell(ii, token)); } } } } right_arrow(svg, x_sh + 37, y_start + i * y_shift + 16); } x_sh = x_start + 440; for (var i = 0; i < num_experts; i += 1) { y_sh = y_start + i * y_shift - 20; for (var j = 0; j < expert_capacity; j += 1) { tensor(svg, x_sh - j * 5, y_sh + j * 5, 2, k, 'white'); for (var ii = 0; ii < k; ii += 1) { for (const [token, value] of Object.entries(experts_mapping[ii])) { const [expert, capacity] = value; if ((expert == i) && (capacity == expert_capacity - 1 - j)) { shaded_cell(svg, x_sh - j * 5, y_sh + j * 5 + ii * 16, colors[i], mark_cell(ii, token)); shaded_cell(svg, x_sh - j * 5 + 16, y_sh + j * 5 + ii * 16, colors[i], mark_cell(ii, token)); } } } } if (i == 0) { svg.append('text') .attr('x', x_sh + 10) .attr('y', y_sh - 10) .text("d") .style("font-size", "13px") .attr("font-family", "Arvo"); long_right_arrow(svg, x_sh, y_sh - 5, 25); svg.append('text') .attr('x', x_sh + 44) .attr('y', y_sh + 35) .text("G") .style("font-size", "13px") .attr("font-family", "Arvo"); long_down_arrow(svg, x_sh + 40, y_sh + 8, 45); svg.append('text') .attr('x', x_sh + 33) .attr('y', y_sh + 78) .text("C") .style("font-size", "13px") .attr("font-family", "Arvo"); upright_arrow(svg, x_sh + 27, y_sh + 72); } } svg.append("path") .datum([{x: x_start - 50, y: y_start + 370}, {x: x_start + 500, y: y_start + 370}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .datum([{x: x_start + 497, y: y_start + 369}, {x: x_start + 500, y: y_start + 370}, {x: x_start + 497, y: y_start + 371}]) .attr("fill", "none") .attr("stroke-width", 1) .attr("stroke", "black") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('text') .attr('x', x_start + 160) .attr('y', y_start + 395) .text("Dispatch") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x_start + 333) .attr('y', y_start + 395) .text("All-to-all reshard") .style("font-size", "14px") .attr("font-family", "Arvo"); } expert_parallel_dispatch(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>A schematic representation of top-1 expert parallel dispatching with data parallel and batch size per device $S=5$. Number of experts $E$ is equal to the number of devices $G$ and capacity factor $c$ is set to 2 (therefore expert capacity $C = \frac{S}{E} \cdot c = 2.5$). Any white cell represents zero element. Vectors associated with the first token placed on GPU-1 (initially) are shaded as an example so that the flow of elements in the image can be easily followed. Note that the embedding of this token was dispatched to the last expert by <code class="language-plaintext highlighter-rouge">Gating</code> and therefore put to the last device accordingly.</em></p><p>The auxiliary loss $\ell_{\text{aux}}$ is similar to GShard, except that $f_i$ is derived through dense gating function and aggregated over whole batch:</p>\[f_i = \frac{1}{B} \sum_{x \in \mathcal{B}} \mathbb{1}_{ \lbrace \operatorname{argmax} \mathcal{p}(x) = i \rbrace }.\]<p>The same goes for $\bar{p}_i$.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">load_balance_loss</span><span class="p">(</span><span class="n">gating_probs</span><span class="p">,</span> <span class="n">expert_mask</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
        Calculate load−balancing loss to ensure diverse expert routing.
    </span><span class="sh">'''</span>
    <span class="c1"># gating probs is the probability assigned for each expert per token
</span>    <span class="c1"># gating probs shape: [G, S, E]
</span>    <span class="c1"># expert index contains the expert with the highest gating
</span>    <span class="c1"># probability in one−hot format
</span>    <span class="c1"># expert mask shape: [G, S, E]
</span>    <span class="c1"># For each core, get the fraction of tokens routed to each expert
</span>    <span class="c1"># density_1 shape: [G, E]
</span>    <span class="n">density_1</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">expert_mask</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># For each core, get fraction of probability mass assigned to each expert
</span>    <span class="c1"># from the router across all tokens.
</span>    <span class="c1"># density_1_proxy shape: [G, E]
</span>    <span class="n">density_1_proxy</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">gating_probs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># density_1 for a single device: vector of length E that sums to 1.
</span>    <span class="c1"># density_1_proxy for a single device: vector of length E that sums to 1.
</span>    <span class="c1"># Want both vectors to have uniform allocation (1/E) across all E elements.
</span>    <span class="c1"># The two vectors will be pushed towards uniform allocation when the dot product 
</span>    <span class="c1"># is minimized.
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">density_1_proxy</span> <span class="o">*</span> <span class="n">density_1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">density_1</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></table></code></div></div><p>Full code for Switch Layer can be found <a href="https://github.com/astralord/jax_parallel/blob/main/6_switch.py">here</a>.</p><h4 id="mixtral-of-experts">Mixtral of Experts</h4><p>Recently an open-source language model called Mixtral 8x7B was introduced in <a href="https://arxiv.org/pdf/2401.04088.pdf">Mixture of Experts</a> paper and is claimed to outperform Llama-2 70B and GPT-3.5 on many benchmarks. As the name suggests, inference requires running only through 7B parameters, which is possible thanks to 8 distinct experts for each layer. For Mixtral 8x7B authors use deterministic sparse gating function, routing to top 2 experts:</p>\[\mathcal{G}(x) = \operatorname{softmax}(\operatorname{topk} (x\mathbf{W}_{\text{G}}, 2)).\]<p>Another key point is that the SwiGLU layer is chosen as the expert function. It was introduced by Noam Shazeer in <a href="https://arxiv.org/pdf/2002.05202v1.pdf">GLU Variants Improve Transformers</a> and has two main differences from standard FFN layer. The first one is Swish (also called SiLU) activation function instead of ReLU:</p>\[\operatorname{Swish}(x) = x \sigma (x) = \frac{x}{1+e^{-x}}.\]<p>The second one is the gating mechanism, <strong>Gated Linear Unit (GLU)</strong>, introduced by <a href="https://arxiv.org/pdf/1612.08083.pdf">Dauphin et al., 2016</a>:</p>\[\operatorname{SwiGLU}(x) = \big(\operatorname{Swish}(x \mathbf{W}_1) \otimes x \mathbf{V} \big)\mathbf{W}_2,\]<p>where $\otimes$ is the element-wise product between matrices.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">SwiGLUParams</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">w1</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span>
    <span class="n">w2</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span>
    <span class="n">v</span><span class="p">:</span>  <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span>

<span class="nd">@jit</span>
<span class="k">def</span> <span class="nf">swiglu</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="p">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">SwiGLUParams</span><span class="p">):</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">params</span><span class="p">.</span><span class="n">v</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">swish</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">params</span><span class="p">.</span><span class="n">w1</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">z</span> <span class="o">*</span> <span class="n">y</span><span class="p">)</span> <span class="o">@</span> <span class="n">params</span><span class="p">.</span><span class="n">w2</span>
</pre></table></code></div></div><p><img data-proofer-ignore data-src="/assets/img/mixtral-of-experts.png" alt="Mixtral-of-Experts" /> <em>Routing analysis: each token is colored with the first expert choice in Mixtral 8x7B. Authors notice that the selection of experts appears to be more aligned with the syntax rather than the domain, especially at the initial and final layers.</em></p><h3 id="strategies-worth-considering-but-beyond-the-scope-of-this-post">Strategies worth considering but beyond the scope of this post</h3><p><a href="https://arxiv.org/pdf/1910.02054.pdf"><strong>Zero Redundancy Optimizer (ZeRO)</strong></a> - it also performs sharding of the tensors somewhat similar to TP, except the whole tensor gets reconstructed in time for a forward or backward computation, therefore the model doesn’t need to be modified. It also supports various offloading techniques to compensate for limited GPU memory.</p><p><a href="https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/"><strong>Fully sharded data parallel (FSDP)</strong></a> - is a type of data parallel training, but unlike traditional DP strategy, which maintains a per-GPU copy of a model’s parameters, gradients and optimizer states, it shards all of these states across data parallel workers and can optionally offload the sharded model parameters to CPUs.</p><h3 id="conclusion">Conclusion</h3><p>Training ever-larger neural networks requires creative parallelization techniques to distribute computation and memory efficiently across accelerators. In this post, we explored some of the predominant strategies used today like data, tensor, pipeline, and mixture-of-experts parallelism.</p><p>While simple data parallelism can work for smaller models, combining it with model parallel approaches becomes essential to scale up to the massive architectures used in LLMs. Each strategy makes tradeoffs between computation efficiency, communication overheads, and implementation complexity.</p><p>Hybrid schemes are often needed in practice, tailored to optimize parallelism for a specific model architecture. As models continue growing in size, new innovations in efficient distributed training will be crucial to unlock further breakthroughs in AI. The insights from this post can guide decisions on parallelization strategies when training large neural networks.</p><p>Sources:</p><ul><li>Parallelization with Jax<ul><li><a href="https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html">Parallel evaluation in Jax</a><li><a href="https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html">Distributed arrays and automatic parallelization in Jax</a></ul><li>Data/model parallel strategies:<ul><li><a href="https://arxiv.org/pdf/1909.08053.pdf">Megatron-LM (Shoeybi et al. 2020)</a><li><a href="https://arxiv.org/pdf/2201.11990.pdf">Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B (Smith et al. 2022)</a><li><a href="https://arxiv.org/pdf/2105.04663.pdf">GSPMD: General and Scalable Parallelization for ML Computation Graphs (Xu et al. 2021)</a><li><a href="https://arxiv.org/pdf/1811.06965.pdf">GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism (Huang et al. 2019)</a><li>How to Parallelize Deep Learning on GPUs.<ul><li><a href="https://timdettmers.com/2014/10/09/deep-learning-data-parallelism/">Part 1: Data Parallelism</a><li><a href="https://timdettmers.com/2014/11/09/model-parallelism-deep-learning/">Part 2: Model Parallelism</a></ul></ul><li>Mixture of Experts:<ul><li><a href="https://arxiv.org/pdf/1701.06538.pdf">Sparse MoE Layer (Shazeer et al. 2017)</a><li><a href="https://arxiv.org/pdf/2006.16668.pdf">GShard (Lepikhin et al. 2020)</a><li><a href="https://arxiv.org/pdf/2101.03961.pdf">Switch Transformers (Fedus et al. 2021)</a></ul></ul><p><a href="https://github.com/astralord/jax_parallel"><strong>Supplementary code on GitHub</strong></a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/ml-engineering/'>ML Engineering</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/jax/" class="post-tag no-text-decoration" >jax</a> <a href="/tags/data-parallel/" class="post-tag no-text-decoration" >data-parallel</a> <a href="/tags/model-parallel/" class="post-tag no-text-decoration" >model-parallel</a> <a href="/tags/pipeline-paralell/" class="post-tag no-text-decoration" >pipeline-paralell</a> <a href="/tags/tensor-parallel/" class="post-tag no-text-decoration" >tensor-parallel</a> <a href="/tags/mixture-of-experts/" class="post-tag no-text-decoration" >mixture-of-experts</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Exploring Parallel Strategies with Jax - AstraBlog&url=https://astralord.github.io/posts/exploring-parallel-strategies-with-jax/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Exploring Parallel Strategies with Jax - AstraBlog&u=https://astralord.github.io/posts/exploring-parallel-strategies-with-jax/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Exploring Parallel Strategies with Jax - AstraBlog&url=https://astralord.github.io/posts/exploring-parallel-strategies-with-jax/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://astralord.github.io/posts/exploring-parallel-strategies-with-jax/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/">Building Aligned Intelligence System. Part I: Creating GPT Assistant</a><li><a href="/posts/exploring-parallel-strategies-with-jax/">Exploring Parallel Strategies with Jax</a><li><a href="/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/">Building Aligned Intelligence System. Part II. Improving Large Language Models</a><li><a href="/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/">Visual Guide to Statistics. Part I: Basics of Point Estimation</a><li><a href="/posts/visual-guide-to-statistics-part-ii-bayesian-statistics/">Visual Guide to Statistics. Part II: Bayesian Statistics</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/parameter-estimation/">parameter estimation</a> <a class="post-tag" href="/tags/asymptotic-efficiency/">asymptotic efficiency</a> <a class="post-tag" href="/tags/bartlett-test/">bartlett test</a> <a class="post-tag" href="/tags/bayes-estimator/">bayes estimator</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/central-limit-theorem/">central limit theorem</a> <a class="post-tag" href="/tags/chain-of-thought/">chain-of-thought</a> <a class="post-tag" href="/tags/chatgpt/">chatgpt</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/power-of-diffusion-models/"><div class="card-body"> <span class="timeago small" >Sep 25, 2022<i class="unloaded">2022-09-25T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Power of Diffusion Models</h3><div class="text-muted small"><p> In 2022, insanely beautiful and original images created with generative neural networks are taking the internet by storm. This post focuses on the theory behind diffusion models that underpin th...</p></div></div></a></div><div class="card"> <a href="/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/"><div class="card-body"> <span class="timeago small" >Jul 23, 2023<i class="unloaded">2023-07-23T19:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Building Aligned Intelligence System. Part II. Improving Large Language Models</h3><div class="text-muted small"><p> In this post we will look at different techniques for steering LLMs behaviour to get desired outcomes, starting with some basic general principles such as writing a good prompt and ending ...</p></div></div></a></div><div class="card"> <a href="/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/"><div class="card-body"> <span class="timeago small" >Jul 3, 2023<i class="unloaded">2023-07-03T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Building Aligned Intelligence System. Part I: Creating GPT Assistant</h3><div class="text-muted small"><p> In recent years, the field of natural language processing has witnessed a remarkable breakthrough with the advent of Large Language Models (LLMs). These models have demonstrated unprecedented pe...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/" class="btn btn-outline-primary" prompt="Older"><p>Building Aligned Intelligence System. Part II. Improving Large Language Models</p></a> <span class="btn btn-outline-primary disabled" prompt="Newer"><p>-</p></span></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/astralord">Aleksandr Samarin</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/parameter-estimation/">parameter estimation</a> <a class="post-tag" href="/tags/asymptotic-efficiency/">asymptotic efficiency</a> <a class="post-tag" href="/tags/bartlett-test/">bartlett test</a> <a class="post-tag" href="/tags/bayes-estimator/">bayes estimator</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/central-limit-theorem/">central limit theorem</a> <a class="post-tag" href="/tags/chain-of-thought/">chain of thought</a> <a class="post-tag" href="/tags/chatgpt/">chatgpt</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://astralord.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
