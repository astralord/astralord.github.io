<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="days ago"><meta name="hour-prompt" content="hours ago"><meta name="minute-prompt" content="minutes ago"><meta name="justnow-prompt" content="just now"><meta name="generator" content="Jekyll v4.3.3" /><meta property="og:title" content="Building Aligned Intelligence System. Part II. Improving Large Language Models" /><meta property="og:locale" content="en" /><meta name="description" content="A minimal, portfolio, sidebar, bootstrap Jekyll theme with responsive web design and focuses on text presentation." /><meta property="og:description" content="A minimal, portfolio, sidebar, bootstrap Jekyll theme with responsive web design and focuses on text presentation." /><link rel="canonical" href="https://astralord.github.io/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/" /><meta property="og:url" content="https://astralord.github.io/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/" /><meta property="og:site_name" content="AstraBlog" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-07-23T19:00:00+03:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Building Aligned Intelligence System. Part II. Improving Large Language Models" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-01-28T01:08:08+03:00","datePublished":"2023-07-23T19:00:00+03:00","description":"A minimal, portfolio, sidebar, bootstrap Jekyll theme with responsive web design and focuses on text presentation.","headline":"Building Aligned Intelligence System. Part II. Improving Large Language Models","mainEntityOfPage":{"@type":"WebPage","@id":"https://astralord.github.io/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/"},"url":"https://astralord.github.io/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/"}</script><title>Building Aligned Intelligence System. Part II. Improving Large Language Models | AstraBlog</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="AstraBlog"><meta name="application-name" content="AstraBlog"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="en"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/marvel-icon.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">AstraBlog</a></div><div class="site-subtitle font-italic">A place to learn and share knowledge about AI-related things</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/astralord" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['samarin_ad','mail.ru'].join('@')" aria-label="email" class="order-4" > <i class="fas fa-envelope"></i> </a> <a href="https://www.linkedin.com/in/aleksandr-samarin-b8a35496" aria-label="linkedin" class="order-5" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://t.me/astrlrd" aria-label="telegram" class="order-6" target="_blank" rel="noopener"> <i class="fab fa-telegram"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Building Aligned Intelligence System. Part II. Improving Large Language Models</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Building Aligned Intelligence System. Part II. Improving Large Language Models</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> Aleksandr Samarin </span> on <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Sun, Jul 23, 2023, 7:00 PM +0300" >Jul 23, 2023<i class="unloaded">2023-07-23T19:00:00+03:00</i> </span></div><div> <span> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, Jan 28, 2024, 1:08 AM +0300" >Jan 28<i class="unloaded">2024-01-28T01:08:08+03:00</i> </span> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="4266 words">23 min read</span></div></div><div class="post-content"> <script src="https://d3js.org/d3.v4.min.js"></script><link href="https://fonts.googleapis.com/css?family=Arvo" rel="stylesheet" /> <script> function line(svg, x1, y1, x2, y2, opacity=1.0, width=2) { svg.append('line') .attr('x1', x1) .attr('y1', y1) .attr('x2', x2) .attr('y2', y2) .style("stroke-width", width) .attr("opacity", opacity) .attr('stroke', 'black'); } function triangle(svg, x, y, rotate=0) { var triangleSize = 25; var triangle_symb = d3.symbol() .type(d3.symbolTriangle) .size(triangleSize); svg.append("path") .attr("d", triangle_symb) .attr("stroke", "black") .attr("fill", "gray") .attr("transform", function(d) { return "translate(" + x + "," + y + ") rotate(" + rotate + ")"; }); } function up_arrow(svg, x1, y1, y2, opacity=1.0) { line(svg, x1, y1, x1, y2, opacity=opacity); triangle(svg, x1, y1 + 5, 0); } function bckg_block(svg, x, y, height=280) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 198) .attr('height', height) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#F3F3F4'); } function emb_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1) .attr('fill', '#F7E1E1'); svg.append('text') .attr('x', x + 20) .attr('y', y + 20) .text("Embeddings") .style("font-size", "14px") .attr("font-family", "Arvo"); } function softmax_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#D1E6D1'); svg.append('text') .attr('x', x + 33) .attr('y', y + 20) .text("Softmax") .style("font-size", "14px") .attr("font-family", "Arvo"); } function linear_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#DCDFEE'); svg.append('text') .attr('x', x + 40) .attr('y', y + 20) .text("Linear") .style("font-size", "14px") .attr("font-family", "Arvo"); } function transformer_block(svg, x, y) { bckg_block(svg, x, y, height=30); svg.append('text') .attr('x', x + 34) .attr('y', y + 20) .text("Transformer block") .style("font-size", "14px") .attr("font-family", "Arvo"); } function linear_block_2(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 80) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#E9EEEB'); svg.append('text') .attr('x', x + 20) .attr('y', y + 20) .text("Linear") .style("font-size", "14px") .attr("font-family", "Arvo"); } function addnorm_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#F3F3C6'); svg.append('text') .attr('x', x + 20) .attr('y', y + 20) .text("Add & Norm") .style("font-size", "14px") .attr("font-family", "Arvo"); } function ff_block(svg, x, y, frozen=false) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#C9E7F5'); svg.append('text') .attr('x', x + 15) .attr('y', y + 20) .text("Feed Forward") .style("font-size", "14px") .attr("font-family", "Arvo"); if (frozen) { svg.append('text') .attr('x', x + 108) .attr('y', y + 12) .text("❄") .style("font-size", "14px") .attr("font-family", "Arvo"); } } function mha_block(svg, x, y, frozen=false) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 70) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#FBE2C0'); svg.append('text') .attr('x', x + 35) .attr('y', y + 20) .text("Masked") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 20) .attr('y', y + 40) .text("Multi-Head") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 30) .attr('y', y + 60) .text("Attention") .style("font-size", "14px") .attr("font-family", "Arvo"); if (frozen) { svg.append('text') .attr('x', x + 108) .attr('y', y + 12) .text("❄") .style("font-size", "14px") .attr("font-family", "Arvo"); } } </script><blockquote><p>In this post we will look at different techniques for steering LLMs behaviour to get desired outcomes, starting with some basic general principles such as <em>writing a good prompt</em> and ending with fine-tuning and augmenting models with external knowledge. Methods discussed in this post are mainly aimed at improving LLMs reliability and ensuring the consistency and factual accuracy of their outputs.</p></blockquote><h3 id="prompt-design">Prompt design</h3><p>Smart prompt design essentially produces efficient context that can lead to desired completion. Such approach is important, because it does not require to change model weights and with a single model checkpoint one can perform many tasks. The general advice for writing a smart prompt is to start simple and iteratively adding more elements and context as you aim for better results.</p><p>Although some of the techniques are specific to certain types of problems, many of them are built upon general principles that can be applied to a wide range of tasks.</p><ul><li>Give specific instructions. If you want it to say “I don’t know” when it doesn’t know the answer, tell it ‘Say “I don’t know” if you do not know the answer.’<li>Split complex tasks into simpler subtasks.<li>Supply high quality and diverse examples.<li>Generate multiple possible answer, ask for justifications and then pick the best one.<li>Prompt the model to explain before answering. Ask it to write down the series of steps explaining its reasoning.</ul><h4 id="chain-of-thought">Chain-of-Thought</h4><p>In <a href="https://astralord.github.io/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/#supervised-fine-tuning-sft-for-dialogue">previous part</a> we saw such techniques as <strong>zero-shot</strong>, when user gives a direct instruction, and <strong>few-shot</strong>, when instruction is followed by examples in the prompt. But this simple methods could be still unreliable on tasks that require reasoning abilities. As an example, if you simply ask <code class="language-plaintext highlighter-rouge">text-davinci-002</code> (GPT-3.5 model trained with SFT) the arithmetic problem, in most cases the answer will be wrong.</p><p><em>Standard few-shot example:</em></p><pre><code><b>Q</b>: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can
has 3 tennis balls. How many tennis balls does he have now?
<b>A</b>: The answer is 11.

<b>Q</b>: A juggler can juggle 16 balls. Half of the balls are golf balls, and half 
of the golf balls are blue. How many blue golf balls are there?
<b>A</b>:
</code></pre><pre><code>The answer is 8.
</code></pre><p>A technique known as <strong>chain-of-thought (CoT)</strong> introduced in <a href="https://arxiv.org/pdf/2201.11903.pdf">Wei et al. (2022)</a> is to prompt the model to explain step-by-step how it arrives at an answer.</p><p><em>CoT few-shot example:</em></p><pre><code><b>Q</b>: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can
has 3 tennis balls. How many tennis balls does he have now?
<b>A</b>: <span style="color:Salmon">Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 
5 + 6 = 11.</span> The answer is 11.

<b>Q</b>: A juggler can juggle 16 balls. Half of the balls are golf balls, and half 
of the golf balls are blue. How many blue golf balls are there?
<b>A</b>:</code></pre><pre><code>The juggler can juggle 16 balls. Half of the balls are golf balls. So there are 
16 / 2 = 8 golf balls. Half of the golf balls are blue. So there are 8 / 2 = 4
blue golf balls. The answer is 4.</code></pre><p>Tests with <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">PaLM 540B</a> on <a href="https://paperswithcode.com/dataset/gsm8k">GSM8K</a> benchmark have shown that chain-of-thought outperforms few-shot prompting by a large margin: 57% solving rate on math problems, compared to near 18% with standard prompting. In addition to math problems, CoT also lifted performance on questions related to sports understanding, coin flip tracking, and last letter concatenation.</p><p>CoT enables complex reasoning capabilities, but it comes with a price: the increase in both latency and cost due to the increased number of input and output tokens. Also, most prompt examples are task-specific and require extra effort to generate. A recent technique, called <strong>zero-shot CoT</strong> <a href="https://arxiv.org/pdf/2205.11916.pdf">(Kojima et al. 2022)</a>, simplifies CoT by essentially adding “Let’s think step by step” to the original prompt without providing any examples:</p><p><em>Zero-shot CoT example:</em></p><pre><code><b>Q</b>: A juggler can juggle 16 balls. Half of the balls are golf balls, and half 
of the golf balls are blue. How many blue golf balls are there?
<b>A</b>: Let’s think step by step.</code></pre><pre><code>There are 16 balls in total. Half of the balls are golf balls. That means that
there are 8 golf balls. Half of the golf balls are blue. That means that there 
are 4 blue golf balls.</code></pre><p>On GSM8K benchmark the “Let’s think step by step” trick raised solving rate up to 41% with InstructGPT. Similar magnitudes of improvements have been acheived with PaLM 540B as well. At the same time, while this trick works on math problems, it’s not effective in general. The authors found that it was most helpful for multi-step arithmetic problems, symbolic reasoning problems, strategy problems, and other reasoning problems. It didn’t help with simple math problems or common sense questions, and presumably wouldn’t help with many other non-reasoning tasks either.</p><p>Also, if you apply this technique to your own tasks, don’t be afraid to experiment with customizing the instruction. “Let’s think step by step” is rather generic prompt, so you may find better performance with instructions that hew to a stricter format customized to your use case.</p><h4 id="self-consistency">Self-consistency</h4><p>The idea of <strong>self-consistency</strong> proposed by <a href="https://arxiv.org/pdf/2203.11171.pdf">Wang et al. (2022)</a> is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer. There are generally different thought processes for the same problem (e.g. different ways to prove the same theorem), and the output decision can be more faithful by exploring a richer set of thoughts. The output response can be picked either by majority vote, or by language model itself.</p><p><img data-proofer-ignore data-src="/assets/img/self-consistency-gpt.png" alt="GPT self-consistency" /> <em>The self-consistency method contains three steps: (1) prompt a language model using chain-of-thought (CoT) prompting; (2) replace the “greedy decode” in CoT prompting by sampling from the language model’s decoder to generate a diverse set of reasoning paths; and (3) marginalize out the reasoning paths and aggregate by choosing the most consistent answer in the final answer set.</em></p><p>Self-consistency technique helps to boost the performance of CoT prompting on tasks involving arithmetic and commonsense reasoning. In particular, on arithmetic tasks self-consistency method applied to PaLM 540B and GPT-3 gave an increase in accuracy up to 17.9% compared to CoT-prompting, surpassing many SoTA solutions. On GSM8K benchmark <code class="language-plaintext highlighter-rouge">code-davinci-002</code> (GPT 3.5 model optimized for code-completion tasks) with self-consistency reached 78%.</p><p>Although this technique is simple to implement, it can be costly. Remember, that generating a set of <em>N</em> answers will increase your costs <em>N</em> times. Another limitation is that the “most frequent” heuristic only applies when the output space is limited (e.g. multi-choice QA).</p><h4 id="tree-of-thoughts">Tree of Thoughts</h4><p>Simple prompting techniques can fall short in tasks that require exploration, strategic lookahead, or where initial decisions play a pivotal role. To overcome these challenges <a href="https://arxiv.org/pdf/2305.10601.pdf">Yao et el. (2023)</a> proposed <strong>Tree of Thoughts (ToT)</strong>, a framework that generalizes over chain-of-thought prompting and encourages exploration over thoughts<sup id="fnref:ToT" role="doc-noteref"><a href="#fn:ToT" class="footnote" rel="footnote">1</a></sup>. ToT allows LLMs to perform deliberate decision making by searching over multiple different reasoning paths and self-evaluating choices to decide the next course of action, as well as looking ahead or backtracking when necessary to make global choices.</p><p>To explain how ToT works first let’s formalize previous techniques. We’ll use $\pi_\theta$ to denote a language model, parameterized by $\theta$. Sampling response $y$ from LLM, by giving it prompt $x$ then can be formulated as</p>\[y \sim \pi_\theta(y \mid x).\]<p>CoT introduces sequence of thoughts $z_{1 \dots n} = (z_1, \dots, z_n)$ to bridge $x$ and $y$. To solve problems with CoT, each thought</p>\[z_i \sim \pi_\theta(z_i \mid x, z_1, \dots z_{i-1})\]<p>is sampled sequentially and then the output $y \sim \pi_\theta(y \mid x, z_{1 \dots n})$. In practice $z$ is sampled as a continuous language sequence, and the decomposition of thoughts (e.g. is each $z_i$ a phrase, a sentence, or a paragraph) is left ambiguous.</p><p>Self-consistency with CoT is an ensemble approach that samples $k$ i.i.d. chains of thought</p>\[[z_{1 \dots n}^{(j)}, y^{(j)}] \sim \pi_\theta(z_{1 \dots n}, y \mid x), \quad j = 1, \dots k,\]<p>then returns the most frequent output or the one with the largest score.</p><p>ToT frames any problem as a search over a tree, where each node is a state $s=[x, z_{1 \dots i}]$ representing a partial solution with the input and the sequence of generated thoughts.</p><div id="tot" class="svg-container" align="center"></div><script> function right_arrow(svg, x1, x2, y1, opacity=1.0) { line(svg, x1, y1, x2, y1, opacity=opacity); triangle(svg, x2, y1, 90); } function prompt_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 80) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', 'none'); svg.append('text') .attr('x', x + 15) .attr('y', y + 20) .text("Prompt") .style("font-size", "14px") .attr("font-family", "Arvo"); } function thought_block(svg, x, y, color) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 80) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', color); svg.append('text') .attr('x', x + 11) .attr('y', y + 20) .text("Thought") .style("font-size", "14px") .attr("font-family", "Arvo"); } function output_block(svg, x, y, color) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 80) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', color); svg.append('text') .attr('x', x + 16) .attr('y', y + 20) .text("Output") .style("font-size", "14px") .attr("font-family", "Arvo"); } function right_dash(svg, x1, x2, y1) { svg.append('line') .attr('x1', x1) .attr('y1', y1) .attr('x2', x2) .attr('y2', y1) .style("stroke-width", 2) .attr("stroke-dasharray", ("2, 10")) .attr('stroke', 'black'); } function tot() { var svg = d3.select("#tot") .append("svg") .attr("width", 700) .attr("height", 572); x_start = 65; x_end = 565; fs_level = 25; svg.append('text') .attr('x', 5) .attr('y', 15) .text("Standard") .style("font-size", "14px") .attr("font-family", "Arvo"); prompt_block(svg, x_start, fs_level); right_arrow(svg, x_start + 80, x_end - 5, fs_level + 15); output_block(svg, x_end, fs_level, '#92C37D'); svg.append("path") .attr("stroke", "black") .datum([{x: 0, y: fs_level + 50}, {x: 670, y: fs_level + 50}]) .attr("fill", "none") .attr("stroke-width", 4) .attr("opacity", 0.2) .attr("stroke", "gray") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('text') .attr('x', 5) .attr('y', fs_level + 75) .text("CoT") .style("font-size", "14px") .attr("font-family", "Arvo"); cot_level = 100; prompt_block(svg, x_start, cot_level); thought_shift = x_start + 170; right_arrow(svg, x_start + 80, thought_shift - 5, cot_level + 15); thought_block(svg, thought_shift, cot_level, '#D9E9D3'); thought_shift_2 = thought_shift + 170; right_dash(svg, thought_shift + 100, thought_shift_2 - 20, cot_level + 15); thought_block(svg, thought_shift_2, cot_level, '#D9E9D3'); right_arrow(svg, thought_shift_2 + 80, x_end - 5, cot_level + 15); output_block(svg, x_end, cot_level, '#92C37D'); svg.append("path") .attr("stroke", "black") .datum([{x: 0, y: cot_level + 50}, {x: 670, y: cot_level + 50}]) .attr("fill", "none") .attr("stroke-width", 4) .attr("opacity", 0.2) .attr("stroke", "gray") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('text') .attr('x', 5) .attr('y', cot_level + 75) .text("Self-consistency") .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 5) .attr('y', cot_level + 95) .text("with CoT") .style("font-size", "14px") .attr("font-family", "Arvo"); sc_level = 220; prompt_block(svg, x_start, sc_level); thought_shift = x_start + 170; right_arrow(svg, x_start + 80, thought_shift - 5, sc_level + 15); line(svg, x_start + 80, sc_level + 15, thought_shift - 5, sc_level - 35); triangle(svg, thought_shift - 5, sc_level - 35, 50); line(svg, x_start + 80, sc_level + 15, thought_shift - 5, sc_level + 65); triangle(svg, thought_shift - 5, sc_level + 65, 0); thought_block(svg, thought_shift, sc_level, '#F5CBCC'); thought_block(svg, thought_shift, sc_level - 50, '#D9E9D3'); thought_block(svg, thought_shift, sc_level + 50, '#D9E9D3'); thought_shift_2 = thought_shift + 170; right_dash(svg, thought_shift + 100, thought_shift_2 - 20, sc_level + 15); right_dash(svg, thought_shift + 100, thought_shift_2 - 20, sc_level - 35); right_dash(svg, thought_shift + 100, thought_shift_2 - 20, sc_level + 65); output_block(svg, thought_shift_2, sc_level, '#F5CBCC'); output_block(svg, thought_shift_2, sc_level - 50, '#D9E9D3'); output_block(svg, thought_shift_2, sc_level + 50, '#D9E9D3'); line(svg, thought_shift_2 + 80, sc_level - 35, x_end - 5, sc_level + 10); triangle(svg, x_end - 5, sc_level + 10, 0); line(svg, thought_shift_2 + 80, sc_level + 65, x_end - 5, sc_level + 20); triangle(svg, x_end - 5, sc_level + 20, 50); output_block(svg, x_end, sc_level, '#92C37D'); svg.append("path") .attr("stroke", "black") .datum([{x: 0, y: sc_level + 100}, {x: 670, y: sc_level + 100}]) .attr("fill", "none") .attr("stroke-width", 4) .attr("opacity", 0.2) .attr("stroke", "gray") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append('text') .attr('x', 5) .attr('y', sc_level + 125) .text("ToT") .style("font-size", "14px") .attr("font-family", "Arvo"); tot_level = 440; prompt_block(svg, x_start, tot_level); thought_shift = x_start + 170; right_arrow(svg, x_start + 80, thought_shift - 5, tot_level + 15); line(svg, x_start + 80, tot_level + 15, thought_shift - 5, tot_level - 35); triangle(svg, thought_shift - 5, tot_level - 35, 50); line(svg, x_start + 80, tot_level + 15, thought_shift - 5, tot_level + 65); triangle(svg, thought_shift - 5, tot_level + 65, 0); thought_block(svg, thought_shift, tot_level - 50, '#D9E9D3'); thought_block(svg, thought_shift, tot_level, '#F5CBCC'); thought_block(svg, thought_shift, tot_level + 50, '#D9E9D3'); line(svg, thought_shift + 80, tot_level - 35, thought_shift_2 - 5, tot_level - 85); triangle(svg, thought_shift_2 - 5, tot_level - 85, 50); right_arrow(svg, thought_shift + 80, thought_shift_2 - 5, tot_level - 35); line(svg, thought_shift + 80, tot_level - 35, thought_shift_2 - 5, tot_level + 15); triangle(svg, thought_shift_2 - 5, tot_level + 15, 0); right_arrow(svg, thought_shift + 80, thought_shift_2 - 5, tot_level + 65); line(svg, thought_shift + 80, tot_level + 65, thought_shift_2 - 5, tot_level + 115); triangle(svg, thought_shift_2 - 5, tot_level + 115, 0); thought_shift_2 = thought_shift + 170; thought_block(svg, thought_shift_2, tot_level - 100, '#F5CBCC'); thought_block(svg, thought_shift_2, tot_level - 50, '#D9E9D3'); thought_block(svg, thought_shift_2, tot_level, '#F5CBCC'); thought_block(svg, thought_shift_2, tot_level + 50, '#92C37D'); thought_block(svg, thought_shift_2, tot_level + 100, '#D9E9D3'); line(svg, thought_shift_2 + 80, tot_level - 35, thought_shift_2 + 100, tot_level - 35); line(svg, thought_shift_2 + 80, tot_level + 65, thought_shift_2 + 100, tot_level + 65); line(svg, thought_shift_2 + 80, tot_level + 115, thought_shift_2 + 100, tot_level + 115); svg.append('line') .attr('x1', thought_shift_2 + 120) .attr('y1', tot_level - 20) .attr('x2', thought_shift_2 + 120) .attr('y2', tot_level + 60) .style("stroke-width", 2) .attr("stroke-dasharray", ("2, 10")) .attr('stroke', 'black'); right_arrow(svg, x_end - 25, x_end - 5, tot_level + 15); output_block(svg, x_end, tot_level, '#92C37D'); } tot(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Schematic illustrating various approaches to problem solving with LLMs. A thought is a coherent language sequence that serves as an intermediate step toward problem solving.</em></p><p>First, given the state $s$ a thought generator $G(\pi_\theta, s, k)$ generates $k$ next states. It can be done either of two ways:</p><ul><li>Sample $k$ i.i.d. samples:</ul>\[z_{i+1}^j \sim \pi_\theta(z \mid s), \quad j=1, \dots k.\]<p>This works better when the thought space is rich (e.g. each thought is a paragraph), and i.i.d. samples lead to diversity.</p><ul><li>Propose multiple thoughts at once, by giving model a <em>propose prompt</em> $w$, e.g. “What are the next steps?”:</ul>\[[z_{i+1}^1, \cdots z_{i+1}^k] \sim \pi_\theta(z \mid s, w).\]<p>This works better when the thought space is more constrained (e.g. each thought is just a word or a line), so proposing different thoughts in the same context avoids duplication.</p><p>Then, given the set of states $S$, each state in this set is evaluated with a state evaluator $V(\pi_\theta, s, S)$. Authors of ToT propose to use LLM to reason about states. Evaluation can be done either by giving a model a <em>value prompt</em> such as “Evaluate if…” and asking to output a scalar, or by voting out of given set $S$ by giving a model a <em>vote prompt</em>, e.g. “Which state to explore?”</p><p>Finally, search algorithm over a tree is applied. Authors of ToT explored two classic search algorithms on graphs in their work: depth-first search and breadth-first search. Also, pruning is iteratively applied to trade exploration for exploitation, and these algorithms are more like beam searches.</p><p><img data-proofer-ignore data-src="/assets/img/tot.png" alt="ToT example" /> <em>A step of deliberate search in a randomly picked Creative Writing task. Given the input, the LM samples 5 different plans, then votes 5 times to decide which plan is best. The majority choice is used to consequently write the output passage with the same sample-vote procedure.</em></p><p>ToT can substantially outperform simple sampling methods, but it requires more resources and effort to implement. Although, the modular flexibility of ToT allows users to customize such performance-cost tradeoffs.</p><h3 id="parameter-efficient-fine-tuning">Parameter Efficient Fine-Tuning</h3><p>Smart prompting is an important tool, but it doesn’t help when the model has not learned how to solve the problems that will be given to it. A huge performance gains over using the pretrained LLMs out-of-the-box can be achieved via fine-tuning on downstream tasks. However, training the entire model, which has billions of parameters, is computationally expensive and time-consuming, not to mention impossible on most consumer hardware.</p><p>This is where <strong>Parameter-Efficient Fine-tuning (PEFT)</strong> comes in handy. PEFT approaches only fine-tune a small number of (extra) model parameters while freezing most parameters of the pretrained LLMs, thereby greatly decreasing the computational and storage costs.</p><h4 id="prompt-tuning">Prompt-tuning</h4><p><strong>Prompt-tuning</strong> <a href="https://arxiv.org/pdf/2104.08691.pdf">(Lester et al. 2021)</a> is a simple mechanism, which allows to condition frozen LLM to perform specific downstream task. The idea is to prepend different trainable tensors $\mathbf{P_\theta}$ (so called <strong>soft prompts</strong>) to input embeddings per each task. Unlike the discrete text prompts, soft prompts do not tie to any embeddings associated with the real words and thus they are more expressive for steering the context.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">jax.numpy</span> <span class="k">as</span> <span class="n">jnp</span>

<span class="k">def</span> <span class="nf">prompt_tuned_model</span><span class="p">(</span><span class="n">token_ids</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nf">embedding</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">soft_prompt</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span>
    <span class="k">return</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></table></code></div></div><p>Prompt-tuning only requires storing a small task-specific prompt for each task, and enables mixed-task inference using the original pretrained model. Experiments have shown that for large models prompt-tuning produces competitive results as model fine-tuning and its efficiency grows with model size. Also with learned task-specific parameters, prompt-tuning achieves better resilience on domain shift problems. In addition, authors showed that prompt ensembling of multiple prompts beats or matches singular prompts.</p><p>Overall soft prompts are incredibly parameter-efficient at the cost of inference overhead (given the quadratic complexity of transformer) and more applicable to larger models (&gt; 10B).</p><h4 id="prefix-tuning">Prefix-tuning</h4><p>In <strong>prefix-tuning</strong> <a href="https://arxiv.org/pdf/2101.00190.pdf">(Li &amp; Liang 2021)</a> instead of adding a soft prompt to the model input, trainable embeddings are prepended to the hidden states of all transformer layers. In practice, directly updating $\mathbf{P_\theta}$ leads to unstable optimization and poor performance. To reduce the difficulty associated with high dimensionality training, the matrix $\mathbf{P_\theta}$ is reparameterized by a smaller matrix $\mathbf{P_\theta’}$ composed with a large linear layer $\mathbf{W}$:</p>\[\mathbf{P_\theta} = \mathbf{P_\theta' W}.\]<div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">prefix_tuned_model</span><span class="p">(</span><span class="n">token_ids</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nf">embedding</span><span class="p">(</span><span class="n">token_ids</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">soft_prompt</span> <span class="o">@</span> <span class="n">W</span>
    <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="n">transformer_blocks</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">block</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">p</span><span class="p">,</span> <span class="n">x</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">x</span>
</pre></table></code></div></div><div id="prompt_tuning" class="svg-container" align="center"></div><script> function trapezoid(svg, x, y, lw=80, uw=40, h=30) { svg.append("path") .attr("stroke", "black") .datum([{x: x - uw / 2, y: y}, {x: x + uw / 2, y: y}, {x: x + uw / 2, y: y}, {x: x + lw / 2, y: y + h}, {x: x + lw / 2, y: y + h}, {x: x - lw / 2, y: y + h}, {x: x - lw / 2, y: y + h}, {x: x - uw / 2, y: y}]) .attr("fill", "#DCDFEE") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); } function emb_prefix_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 100) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr('fill', '#EA9999'); svg.append('text') .attr('x', x + 10) .attr('y', y + 20) .text("Soft Prompt") .style("font-size", "14px") .attr("font-family", "Arvo"); } function concat_path(svg, x, y) { svg.append("path") .attr("stroke", "black") .datum([{x: x, y: y}, {x: x, y: y - 30}, {x: x + 125, y: y - 30}, {x: x + 125, y: y - 60}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); } function prompt_tuning() { var svg = d3.select("#prompt_tuning") .append("svg") .attr("width", 600) .attr("height", 400); svg.append("path") .attr("stroke", "black") .datum([{x: 275, y: 400}, {x: 275, y: 5}]) .attr("fill", "none") .attr("stroke-width", 4) .attr("opacity", 0.2) .attr("stroke", "gray") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); bckg_block(svg, 5, 25, 195); svg.append('text') .attr('x', 63) .attr('y', 115) .text("Pretrained") .style("font-size", "17px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 55) .attr('y', 135) .text("Transformer") .style("font-size", "17px") .attr("font-family", "Arvo"); emb_prefix_block(svg, 5, 340); concat_path(svg, 55, 340); emb_block(svg, 120, 340); up_arrow(svg, 180, 220, 340); concat_path(svg, 355, 290); trapezoid(svg, 355, 290, 40, 80); svg.append('text') .attr('x', 347) .attr('y', 310) .text("W") .style("font-size", "14px") .attr("font-family", "Arvo"); line(svg, 355, 320, 355, 340); emb_prefix_block(svg, 305, 340); emb_block(svg, 420, 340); up_arrow(svg, 480, 220, 340); transformer_block(svg, 320, 190); concat_path(svg, 355, 90); trapezoid(svg, 355, 90, 40, 80); svg.append('text') .attr('x', 347) .attr('y', 110) .text("W") .style("font-size", "14px") .attr("font-family", "Arvo"); line(svg, 355, 120, 355, 140); emb_prefix_block(svg, 305, 140); svg.append('line') .attr('x1', 480) .attr('y1', 0) .attr('x2', 480) .attr('y2', 20) .style("stroke-width", 2) .attr("stroke-dasharray", ("2, 5")) .attr('stroke', 'black'); up_arrow(svg, 480, 20, 190); } prompt_tuning(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Prompt-tuning (left) vs prefix-tuning. Note that after training, only $\mathbf{P_\theta}$ is needed for inference, and tensor $\mathbf{W}$ can be discarded.</em></p><h4 id="lora">LoRA</h4><p><strong>Low-Rank Adaptation (LoRA)</strong> <a href="https://arxiv.org/pdf/2106.09685.pdf">(Hu et. al 2021)</a> freezes the pretrained model weights and injects trainable rank decomposition matrices into each layer of the transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. The core idea is to modify linear transformation of input vector $x$</p>\[h = x\mathbf{W},\]<p>parameterized with a pretrained weight matrix $\mathbf{W}$, with an additional parameter update $\Delta \mathbf{W}$, which can be decomposed into a product of two low-rank matrices:</p>\[h \leftarrow h + x \Delta\mathbf{W} = x(\mathbf{W} + \Delta \mathbf{W})=x\mathbf{W}+x\mathbf{W_d W_u}.\]<p>Here $\mathbf{W}_d \in \mathbb{R}^{d \times r}$ is down-projection, $\mathbf{W}_u \in \mathbb{R}^{r \times k}$ is up-projection and $r \ll \min(d, k)$ is a bottleneck dimension.</p><div id="lora_svg" class="svg-container" align="center"></div><script> function adapter_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 120) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#e6d1e4'); svg.append('text') .attr('x', x + 31) .attr('y', y + 20) .text("Adapter") .style("font-size", "14px") .attr("font-family", "Arvo"); } function add_block(svg, x, y) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 80) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#F3F3C6'); svg.append('text') .attr('x', x + 27) .attr('y', y + 20) .text("Add") .style("font-size", "14px") .attr("font-family", "Arvo"); } function text_block_1(svg, x, y, txt) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 80) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#E9EEEB'); svg.append('text') .attr('x', x + 31) .attr('y', y + 20) .text(txt) .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 67) .attr('y', y + 12) .text("❄") .style("font-size", "14px") .attr("font-family", "Arvo"); } function lora_block(svg, x, y) { trapezoid(svg, x, y + 70); svg.append('text') .attr('x', x - 10) .attr('y', y + 90) .text('W') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 5) .attr('y', y + 95) .text('d') .style("font-size", "10px") .attr("font-family", "Arvo"); line(svg, x, y + 30, x, y + 70); trapezoid(svg, x, y, lw=40, uw=80); svg.append('text') .attr('x', x - 10) .attr('y', y + 20) .text('W') .style("font-size", "14px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', x + 5) .attr('y', y + 25) .text('u') .style("font-size", "10px") .attr("font-family", "Arvo"); } function lora() { var svg = d3.select("#lora_svg") .append("svg") .attr("width", 400) .attr("height", 270); up_arrow(svg, 120, 170, 270); text_block_1(svg, 80, 140, "W"); line(svg, 120, 85, 120, 140); add_block(svg, 80, 55); up_arrow(svg, 120, 10, 55); svg.append("path") .attr("stroke", "black") .datum([{x: 120, y: 240}, {x: 230, y: 240}, {x: 270, y: 240}, {x: 270, y:200}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); svg.append("path") .attr("stroke", "black") .datum([{x: 270, y: 110}, {x: 270, y: 70}, {x: 230, y: 70}, {x: 160, y: 70}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 165, 70, 270); lora_block(svg, 270, 105); svg.append('text') .attr('x', 0) .attr('y', 130) .text("Pretrained") .style("font-size", "17px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 0) .attr('y', 150) .text("weights") .style("font-size", "17px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 330) .attr('y', 130) .text("LoRA") .style("font-size", "17px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 330) .attr('y', 150) .text("weights") .style("font-size", "17px") .attr("font-family", "Arvo"); } lora(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Low rank adaptation. Frozen layers are marked with ❄.</em></p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">linear_with_lora</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W</span> <span class="c1"># regular linear
</span>    <span class="n">h</span> <span class="o">+=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_d</span> <span class="o">@</span> <span class="n">W_u</span> <span class="c1"># low-rank update
</span>    <span class="k">return</span> <span class="n">h</span>
</pre></table></code></div></div><p>In the paper experiments LoRA performs competitively even with a very small $r$, such as 1 or 2. Also applying LoRA to both $\mathbf{W}^Q$ and $\mathbf{W}^V$ gives the best performance overall, while adapting only $\mathbf{W}^Q$ or $\mathbf{W}^K$ results in significantly lower performance, even with larger value of $r$.</p><p>In general, LoRA possesses several key advantages:</p><ul><li><strong>Low storage requirements</strong>. A pre-trained model can be shared and used to build many small LoRA modules for different tasks. We can efficiently switch between tasks by replacing the matrices $\mathbf{W_u}$ and $\mathbf{W_d}$, while keeping base model frozen. This reduces the storage requirement and task-switching overhead significantly.<li><strong>Training efficiency</strong>. LoRA makes training more efficient and lowers the hardware barrier to entry, since it is not needed to calculate the gradients or maintain the optimizer states for frozen parameters. Instead, only the injected, much smaller low-rank matrices are optimized.<li><strong>Inference speed</strong>. Simple linear design allows to deploy model with merged trainable matrices and frozen weights: $\mathbf{W} \leftarrow \mathbf{W} + \Delta \mathbf{W},$ thus introducing no inference latency by construction.<li><strong>Orthogonality</strong>. The combination of LoRA and prefix-tuning significantly outperforms both methods applied separately on WikiSQL benchmark, which indicates that LoRA is somewhat orthogonal to prefix-tuning.</ul><p>Interestingly, studying the relationship between $\Delta \mathbf{W}$ and $\mathbf{W}$ authors concluded that the low-rank adaptation matrix potentially <em>amplifies the important features for specific downstream tasks that were learned but not emphasized in the general pre-training model</em>. Such statement suggests that LoRA can be applied to RLHF fine-tuning stage, which <a href="https://openai.com/research/instruction-following">according to OpenAI</a> is required to “unlock” model capabilities it has already learned.</p><h4 id="adapter">Adapter</h4><p><a href="https://arxiv.org/pdf/1902.00751.pdf">Houlsby et al. (2019)</a> proposed to modify transformer block with additional FFN layers, called <strong>(series) adapters</strong>. The adapter module is added twice to each transformer layer: after the projection following multi-head attention and after the two feed-forward layers. But like in LoRA, the adapter consists of a bottleneck which has smaller hidden dimension than the input and therefore contains fewer parameters relative to the attention and feed-forward layers in the original model.</p><p>Adapter transformation of vector $\mathbf{h}$ can be described as</p>\[h \leftarrow h + f(h\mathbf{W_d})\mathbf{W_u},\]<p>where $f(\cdot)$ is a nonlinear activation function, e.g. ReLU.</p><div id="adapter_svg" class="svg-container" align="center"></div><script> function adapter() { var svg = d3.select("#adapter_svg") .append("svg") .attr("width", 400) .attr("height", 400); svg.append("path") .attr("stroke", "black") .datum([{x: 200, y: 250}, {x: 370, y: 250}, {x: 370, y: 40}, {x: 200, y: 40}, {x: 200, y: 75}, {x: 170, y: 95}, {x: 200, y: 115}, {x: 200, y: 250}]) .attr("fill", "none") .attr("stroke-width", 3) .attr("opacity", 0.4) .attr("stroke", "gray") .attr("d", d3.line() .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); up_arrow(svg, 100, 10, 40); addnorm_block(svg, 40, 40); line(svg, 100, 70, 100, 80); adapter_block(svg, 40, 80); line(svg, 100, 110, 100, 120); ff_block(svg, 40, 120, frozen=true); up_arrow(svg, 100, 150, 190); svg.append("path") .attr("stroke", "black") .datum([{x: 100, y: 175}, {x: 35, y: 175}, {x: 10, y: 175}, {x: 10, y: 145}, {x: 10, y: 85}, {x: 10, y: 55}, {x: 35, y: 55}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 35, 55, 90); addnorm_block(svg, 40, 190); line(svg, 100, 220, 100, 230); adapter_block(svg, 40, 230); line(svg, 100, 260, 100, 270); mha_block(svg, 40, 270, frozen=true); up_arrow(svg, 100, 340, 400); svg.append("path") .attr("stroke", "black") .datum([{x: 65, y: 340}, {x: 65, y: 360}, {x: 135, y: 360}, {x: 135, y: 340}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 65, 345, 0); triangle(svg, 135, 345, 0); svg.append("path") .attr("stroke", "black") .datum([{x: 100, y: 375}, {x: 35, y: 375}, {x: 10, y: 375}, {x: 10, y: 345}, {x: 10, y: 235}, {x: 10, y: 205}, {x: 35, y: 205}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 35, 205, 90); up_arrow(svg, 270, 10, 55); add_block(svg, 230, 55); line(svg, 270, 85, 270, 105); lora_block(svg, 270, 105); svg.append('text') .attr('x', 225) .attr('y', 160) .text("ReLU") .style("font-size", "14px") .attr("font-family", "Arvo"); up_arrow(svg, 270, 205, 270); svg.append("path") .attr("stroke", "black") .datum([{x: 270, y: 235}, {x: 315, y: 235}, {x: 345, y: 235}, {x: 345, y: 205}, {x: 345, y: 100}, {x: 345, y: 70}, {x: 315, y: 70}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 315, 70, 270); svg.append('text') .attr('x', 300) .attr('y', 280) .text("Series") .style("font-size", "17px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 300) .attr('y', 300) .text("adapter") .style("font-size", "17px") .attr("font-family", "Arvo"); } adapter(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Architecture of the adapter module and its integration with the transformer block.</em></p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">transformer_block_with_adapter</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="nf">ffn</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="c1"># adapter
</span>    <span class="n">x</span> <span class="o">=</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># transformer FFN
</span>    <span class="n">h</span> <span class="o">=</span> <span class="n">h</span> <span class="o">+</span> <span class="nf">ffn</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="c1"># adapter
</span>    <span class="n">h</span> <span class="o">=</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">h</span>
</pre></table></code></div></div><p>Adapter tuning is highly parameter-efficient: training with adapters of sizes 0.5-5% of the original model produces strong performance, comparable to full fine-tuning. In addition to that, <a href="https://arxiv.org/pdf/2004.03829.pdf">Lin et al. (2020)</a> and <a href="https://arxiv.org/pdf/2005.00247.pdf">Pfeiffer et al. (2021)</a> proposed a more efficient design with the adapter layer applied only after the FFN “Add &amp; Norm” sub-layer, which achieves similar performance as using two adapters per transformer block.</p><h4 id="mam-adapter">MAM adapter</h4><p><strong>Mix-and-match (MAM) adapter</strong> was proposed in a paper by <a href="https://arxiv.org/pdf/2110.04366.pdf">He et al. (2022)</a>, where adapter placement and combinations with soft prompts were studied. They measured the performance of all prior methods on four different downstream tasks (summarization, translation, entailment/contradiction and classification), where they also included comparisons with <strong>parralel adapters</strong>.</p><div id="prl_adapter" class="svg-container" align="center"></div><script> function prl_adapter() { var svg = d3.select("#prl_adapter") .append("svg") .attr("width", 400) .attr("height", 330); up_arrow(svg, 100, 10, 40); addnorm_block(svg, 40, 40); line(svg, 100, 70, 100, 80); ff_block(svg, 40, 80, frozen=true); up_arrow(svg, 100, 110, 150); svg.append("path") .attr("stroke", "black") .datum([{x: 100, y: 135}, {x: 35, y: 135}, {x: 10, y: 135}, {x: 10, y: 105}, {x: 10, y: 85}, {x: 10, y: 55}, {x: 35, y: 55}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 35, 55, 90); svg.append("path") .attr("stroke", "black") .datum([{x: 100, y: 135}, {x: 225, y: 135}, {x: 250, y: 135}, {x: 250, y: 110}, {x: 250, y: 85}, {x: 250, y: 85}, {x: 250, y: 55}, {x: 225, y: 55}, {x: 165, y: 55}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 165, 55, 270); adapter_block(svg, 190, 80); addnorm_block(svg, 40, 150); line(svg, 100, 180, 100, 190); mha_block(svg, 40, 190, frozen=true); up_arrow(svg, 100, 260, 330); svg.append("path") .attr("stroke", "black") .datum([{x: 65, y: 260}, {x: 65, y: 280}, {x: 135, y: 280}, {x: 135, y: 260}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 65, 265, 0); triangle(svg, 135, 265, 0); svg.append("path") .attr("stroke", "black") .datum([{x: 100, y: 295}, {x: 35, y: 295}, {x: 10, y: 295}, {x: 10, y: 265}, {x: 10, y: 195}, {x: 10, y: 165}, {x: 35, y: 165}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 35, 165, 90); svg.append("path") .attr("stroke", "black") .datum([{x: 100, y: 295}, {x: 225, y: 295}, {x: 250, y: 295}, {x: 250, y: 270}, {x: 250, y: 205}, {x: 250, y: 165}, {x: 225, y: 165}, {x: 165, y: 165}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, 165, 165, 270); adapter_block(svg, 190, 210); svg.append('text') .attr('x', 300) .attr('y', 150) .text("Parallel") .style("font-size", "17px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', 300) .attr('y', 170) .text("adapters") .style("font-size", "17px") .attr("font-family", "Arvo"); } prl_adapter(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Parallel adapters.</em></p><p>They also considered scaling adapter output with tunable parameter $s$. Changing $s$ is roughly the same as changing the learning rate for adapter block if we scale the initialization appropriately. Experiments have shown that scaled parallel adapters outperform series adapters and that placing an adapter in parallel to FFN outperforms adapters parallel to multi-head attention. Finally, they propose MAM adapter, which is a combination of scaled parallel adapter for FFN layer and soft prompt.</p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">transformer_block_with_mam</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">concatenate</span><span class="p">([</span><span class="n">soft_prompt</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nf">self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span>
    <span class="n">h1</span> <span class="o">=</span> <span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># transformer FFN
</span>    <span class="n">h2</span> <span class="o">=</span> <span class="n">scale</span> <span class="o">*</span> <span class="nf">ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># MAM adapter
</span>    <span class="n">h</span> <span class="o">=</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">h1</span> <span class="o">+</span> <span class="n">h2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">h</span>
</pre></table></code></div></div><h4 id="ia">(IA)³</h4><p><a href="https://arxiv.org/pdf/2205.05638.pdf">Liu et al. (2022)</a> proposed another PEFT technique, called <strong>(IA)³</strong>, which stands for “<strong>I</strong>nfused <strong>A</strong>dapter by <strong>I</strong>nhibiting and <strong>A</strong>mplifying <strong>I</strong>nner <strong>A</strong>ctivations”. (IA)³ introduces new parameters $l_v$ and $l_k$, which rescale key and value in attention mechanism:</p>\[\operatorname{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) =\operatorname{softmax} \bigg( \frac{\mathbf{Q} (l_k \odot \mathbf{K}^T)}{\sqrt{d_k}} \bigg) \cdot (l_v \odot \mathbf{V}),\]<p>and $l_{ff}$, which rescales hidden FFN activations:</p>\[\operatorname{FFN}(x) = (l_{ff} \odot f(x\mathbf{W}_1))\mathbf{W}_2\]<p>where $f$ is a nonlinearity activation function. Authors also experimented with <a href="https://arxiv.org/pdf/2110.08207.pdf">T0 model</a>, a variant of famous <a href="https://arxiv.org/pdf/1910.10683.pdf">T5 model</a> by Google, adding auxiliary loss terms to discourage the model from predicting tokens from incorrect target sequences. The called their training pipeline “T-Few” recipe.</p><p><img data-proofer-ignore data-src="/assets/img/IA3.png" alt="IA3" /> <em>Diagram of (IA)³ and the loss terms used in the T-Few recipe. Left: (IA)³ introduces the learned vectors $l_k$, $l_v$, and $l_{ff}$ which respectively rescale (via element-wise multiplication, visualized as $\odot$) the keys and values in attention mechanisms and the inner activations in position-wise feed-forward networks. Right: In addition to a standard cross-entropy loss $L_{LM}$, an unlikelihood loss $L_{UL}$ and length-normalized loss $L_{LN}$ are introduced. Former lowers the probability of incorrect outputs while latter applies a standard softmax cross-entropy loss to length-normalized log-probabilities of all output choices.</em></p><div class="language-python highlighter-rouge"><div class="code-header" text-data="python"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">scaled_self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_k</span><span class="p">,</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_q</span><span class="p">,</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_v</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">l_k</span> <span class="o">*</span> <span class="n">k</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">l_v</span> <span class="o">*</span> <span class="n">v</span>
    <span class="k">return</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">@</span> <span class="n">V</span>
    
<span class="k">def</span> <span class="nf">scaled_ffn</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_1</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">l_ff</span> <span class="o">*</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># f is nonlinear activation
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">W_2</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">transformer_block_with_ia3</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nf">scaled_self_attention</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nf">scaled_ffn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">h</span> <span class="o">=</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">h</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">h</span>
</pre></table></code></div></div><p>(IA)³ adds smaller overhead compared to adapter methods as scale vectors $l_v$ and $l_k$ can be merged into $\mathbf{W}^V$ and $\mathbf{W}^K$ respectively, thus leaving the only overhead from $l_{ff}$. With minimal number of training parameters it achieves comparable results with LoRA and outperforms prompt- and prefix-tuning methods on multiple benchmarks.</p><h3 id="providing-external-knowledge">Providing external knowledge</h3><p>Language models show remarkable abilities to solve new problems with just a few examples or textual instructions. At the same time, they struggle with basic functionality, such as arithmetic or factual lookup, where they are outperformed by much simpler and smaller models. They are also unable to solve tasks that require access to changing or private data that was unavailable at training time. To be able to do that LLM must be augmented with additional tools that can provide an external information.</p><h4 id="internet-augmented-language-models">Internet-augmented language models</h4><p><a href="https://arxiv.org/pdf/2203.05115.pdf">Lazaridou et. al (2022)</a> proposed to use few-shot prompting to condition LMs on information returned from a broad and constantly updated knowledge source, for example, Google Search. Such approach does not involve fine-tuning or learning additional parameters, thus making it applicable to any language model.</p><p>Given a query $q$, clean text is extracted out of multiple URLs returned by search, resulting in a set of documents. Each document is split into $m$ paragraphs $(\mathcal{P})_m$, which are ranked by TF-IDF cosine similarity with a query. Top $n$ paragraphs are selected and each one is inserted separately into the following $k$-shot prompt as the last <code class="language-plaintext highlighter-rouge">Evidence</code>:</p><div class="language-plaintext highlighter-rouge"><div class="code-header" text-data="plaintext"><button data-original-title="Copied!"><i class="far fa-clone"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre>Evidence: ...
Question: ...
Answer: ...

... (k times)

Evidence: Paragraph
Question: Query
Answer:
</pre></table></code></div></div><p>This produces $n$ candidate answers $(a)_n$, which can be re-ranked with conditional probabilities $\pi(a \mid q, \mathcal{P})$ and $\pi(q \mid \mathcal{P})$, measured by language model and TF-IDF scores.</p><p><img data-proofer-ignore data-src="/assets/img/retrieval.png" alt="Retrieval pipeline" /> <em>Schematic representation of Internet-augmented LM</em></p><h4 id="talm">TALM</h4><p><strong>Tool Augmented Language Model (TALM)</strong> <a href="https://arxiv.org/pdf/2205.12255.pdf">Parisi et al. 2022</a> is a LLM augmented with text-to-text API calls. It learns two subtasks at the same time: calling a tool and generating an answer based on tool results.</p><div id="talm_svg" class="svg-container" align="center"></div><script> function tool_block(svg, x, y, text, shift) { svg.append('rect') .attr('x', x) .attr('y', y) .attr('width', 90) .attr('height', 30) .attr('stroke', 'black') .attr("rx", 3) .attr("stroke-width", 2) .attr("opacity", 1.0) .attr('fill', '#95D1E6'); svg.append('text') .attr('x', x + shift) .attr('y', y + 20) .text(text) .style("font-size", "14px") .attr("font-family", "Arvo"); } function talm() { var svg = d3.select("#talm_svg") .append("svg") .attr("width", 700) .attr("height", 131); x_start = 65; x_end = 565; fs_level = 25; tool_shift = x_start + 165; tool_shift_2 = tool_shift + 165; prompt_block(svg, x_start, fs_level); right_arrow(svg, x_start + 80, tool_shift - 5, fs_level + 15); tool_block(svg, tool_shift, fs_level, "Tool input", 12); right_arrow(svg, tool_shift_2 + 80, x_end - 5, fs_level + 15); tool_block(svg, tool_shift_2, fs_level, "Tool result", 10); svg.append("path") .attr("stroke", "black") .datum([{x: tool_shift + 45, y: fs_level + 30}, {x: tool_shift + 45, y: fs_level + 70}, {x: tool_shift + 45, y: fs_level + 100}, {x: tool_shift + 75, y: fs_level + 100}, {x: tool_shift + 85, y: fs_level + 100}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, tool_shift + 85, fs_level + 100, 90); svg.append("path") .attr("stroke", "black") .datum([{x: tool_shift_2, y: fs_level + 100}, {x: tool_shift_2 + 15, y: fs_level + 100}, {x: tool_shift_2 + 45, y: fs_level + 100}, {x: tool_shift_2 + 45, y: fs_level + 70}, {x: tool_shift_2 + 45, y: fs_level + 30}]) .attr("fill", "none") .attr("stroke-width", 2) .attr("d", d3.line() .curve(d3.curveBasis) .x(function(d) { return d.x; }) .y(function(d) { return d.y; })); triangle(svg, tool_shift_2 + 45, fs_level + 35, 0); svg.append('text') .attr('x', tool_shift + 110) .attr('y', fs_level + 105) .text('Tool') .style("font-size", "17px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', tool_shift - 52) .attr('y', fs_level + 70) .text('Call external tool') .style("font-size", "11px") .attr("font-family", "Arvo"); svg.append('text') .attr('x', tool_shift_2 + 52) .attr('y', fs_level + 70) .text('Append tool result') .style("font-size", "11px") .attr("font-family", "Arvo"); output_block(svg, x_end, fs_level, '#92C37D'); } talm(); </script><p><img data-proofer-ignore data-src="." alt="" /> <em>Tool Augmented LMs.</em></p><p>TALM is guided to generate a <code class="language-plaintext highlighter-rouge">tool call</code> and <code class="language-plaintext highlighter-rouge">tool input text</code> conditioned on the task input text and invokes a tool’s API by generating a delimiter, such as <code class="language-plaintext highlighter-rouge">|result</code>. Whenever this delimiter is detected, the tool API is called and its result appended to the text sequence. TALM then continues to generate the final task output, following <code class="language-plaintext highlighter-rouge">|output</code> token:</p><pre><code>Input text 
|<span style="color:SteelBlue"><b>tool-call</b> tool input text 
|<b>result</b> tool output text</span>
|<span style="color:#508450"><b>output</b> Output text</span>
</code></pre><p>A weather task example:</p><pre><code>How hot will it get in NYC today? 
|<span style="color:SteelBlue"><b>weather</b> lookup region=NYC
|<b>result</b> precipitation chance: 10, high temp: 20°C, low-temp: 12°C</span>
|<span style="color:#508450"><b>output</b> Today’s high will be 20°C</span>
</code></pre><p>To train TALM authors propose to iteratively fine-tune model on a dataset of tool use examples. Each round model interacts with a tool, then expands the dataset based on whether a newly added tool can improve the generated outputs. Such technique helps to boost the model performance on knowledge and reasoning tasks drastically.</p><h4 id="toolformer">Toolformer</h4><p><strong>Toolformer</strong> <a href="https://arxiv.org/pdf/2302.04761.pdf">(Schick et al. 2023)</a> approach is similar to TALM in that they both aimed for LLMs to teach themselves how to use external tools via simple APIs. Toolformer is trained as follows:</p><ul><li><strong>Sample API calls</strong>. First, we annotate a dataset with API call usage examples. It can be done by prompting a pre-trained LM via few-shot learning. An exemplary prompt to generate API calls:</ul><pre><code><i>Your task is to add calls to a Question Answering API to a piece of text.
The questions should help you get information required to complete the text. You
can call the API by writing "[QA(question)]" where "question" is the question you
want to ask. Here are some examples of API calls:</i>
<br /><b>Input:</b> Joe Biden was born in Scranton, Pennsylvania.
<b>Output:</b> Joe Biden was born in <b><span style="color:#008bda;background-color:#cce8f7">[QA("Where was Joe Biden born?")]</span></b> Scranton, <b><span style="color:#008bda;background-color:#cce8f7">[QA("In
which state is Scranton?")]</span></b> Pennsylvania.
<br /><b>Input:</b> Coca-Cola, or Coke, is a carbonated soft drink manufactured by
the Coca-Cola Company.
<b>Output:</b> Coca-Cola, or <b><span style="color:#008bda;background-color:#cce8f7">[QA("What other name is Coca-Cola known by?")]</span></b> Coke, is
a carbonated soft drink manufactured by <b><span style="color:#008bda;background-color:#cce8f7">[QA("Who manufactures Coca-Cola?")]</span></b>
the Coca-Cola Company.
<br /><b>Input: x</b>
<b>Output:</b></code></pre><ul><li><strong>Execute API calls</strong> to obtain the corresponding results. The response for each API call $c_i$ needs to be a single text sequence $r_i$.<li><strong>Filter annotations</strong> based on whether API calls help model predict future tokens. Let $i$ be the position of the API call $c_i$ in the sequence $(x_1, \dots x_n)$ and let $r_i$ be the response from the API. Let also</ul>\[L_i(\mathbf{z}) = \sum_{j=i}^n w_{j-i} \log \pi(x_j \mid z, x_{1:j-1})\]<p>be a weighted cross-entropy loss with condition $\mathbf{z}$, given as a prefix. Then to decide which API calls are actually helpful, we compare the difference of losses $L_i^- - L_i^+$ to some threshold, where</p>\[\begin{aligned} L_i^+ &amp;= L_i(c_i \rightarrow r_i),\\ L_i^- &amp;= \min(L_i(\varepsilon), L_i(c_i \rightarrow \varepsilon)) \end{aligned}\]<p>and $\varepsilon$ is an empty sequence. Only API calls with $L_i^- - L_i^+$ larger than some threshold are kept.</p><ul><li><strong>Fine-tune LM on this annotated dataset</strong>.</ul><p><img data-proofer-ignore data-src="/assets/img/toolformer_pipeline.png" alt="Toolformer pipeline" /> <em>Key steps in Toolformer approach, illustrated for a question answering tool: Given an input text $x$, we first sample a position $i$ and corresponding API call candidates $c_i^1, c_i^2, \dots, c_i^k$. We then execute these API calls and filter out all calls which do not reduce the loss $L_i$ over the next tokens. All remaining API calls are interleaved with the original text, resulting in a new text $x^\ast$.</em></p><p>At inference time, decoding runs until the model produces “$\rightarrow$” token, indicating that it is expecting response from an API call next. At this point, the decoding process is interrupted, the appropriate API is called to get a response, and the decoding process continues after inserting the response.</p><p>Toolformer considerably improves zero-shot performance of language model, e.g. augmented GPT-J (6.3B) even outperformed a much larger GPT-3 model on a range of different downstream tasks.</p><h3 id="conclusion">Conclusion</h3><p>The list of techniques in this post is far from complete, but it provides direction for those who are looking for a way to make their language model more useful. Assistants based on Large Language Models are relatively new and, while extremely powerful, still face many limitations that can be worked around in a variety of ways. It is only a matter of time before language models cease to be used for entertainment purposes and enter our daily lives as a complete and useful tool.</p><hr /><div class="footnotes" role="doc-endnotes"><ol><li id="fn:ToT" role="doc-endnote"><p>A similar idea but with use of reinforcement learning instead of tree search was proposed by <a href="https://arxiv.org/pdf/2305.08291.pdf">Long (2023)</a>. <a href="#fnref:ToT" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/generative-ai/'>Generative AI</a>, <a href='/categories/large-language-models/'>Large Language Models</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/prompt-engineering/" class="post-tag no-text-decoration" >prompt engineering</a> <a href="/tags/chain-of-thought/" class="post-tag no-text-decoration" >chain-of-thought</a> <a href="/tags/tree-of-thoughts/" class="post-tag no-text-decoration" >tree of thoughts</a> <a href="/tags/peft/" class="post-tag no-text-decoration" >peft</a> <a href="/tags/prompt-tuning/" class="post-tag no-text-decoration" >prompt-tuning</a> <a href="/tags/prefix-tuning/" class="post-tag no-text-decoration" >prefix-tuning</a> <a href="/tags/lora/" class="post-tag no-text-decoration" >lora</a> <a href="/tags/ia3/" class="post-tag no-text-decoration" >ia3</a> <a href="/tags/talm/" class="post-tag no-text-decoration" >talm</a> <a href="/tags/toolformer/" class="post-tag no-text-decoration" >toolformer</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Building Aligned Intelligence System. Part II. Improving Large Language Models - AstraBlog&url=https://astralord.github.io/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Building Aligned Intelligence System. Part II. Improving Large Language Models - AstraBlog&u=https://astralord.github.io/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Building Aligned Intelligence System. Part II. Improving Large Language Models - AstraBlog&url=https://astralord.github.io/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <a href="https://www.linkedin.com/sharing/share-offsite/?url=https://astralord.github.io/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/" data-toggle="tooltip" data-placement="top" title="Linkedin" target="_blank" rel="noopener" aria-label="Linkedin"> <i class="fa-fw fab fa-linkedin"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" title-succeed="Link copied successfully!"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/">Building Aligned Intelligence System. Part I: Creating GPT Assistant</a><li><a href="/posts/exploring-parallel-strategies-with-jax/">Exploring Parallel Strategies with Jax</a><li><a href="/posts/building-aligned-intelligence-systems-part-ii-applying-large-language-models/">Building Aligned Intelligence System. Part II. Improving Large Language Models</a><li><a href="/posts/visual-guide-to-statistics-part-i-basics-of-point-estimation/">Visual Guide to Statistics. Part I: Basics of Point Estimation</a><li><a href="/posts/visual-guide-to-statistics-part-ii-bayesian-statistics/">Visual Guide to Statistics. Part II: Bayesian Statistics</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/parameter-estimation/">parameter estimation</a> <a class="post-tag" href="/tags/asymptotic-efficiency/">asymptotic efficiency</a> <a class="post-tag" href="/tags/bartlett-test/">bartlett test</a> <a class="post-tag" href="/tags/bayes-estimator/">bayes estimator</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/central-limit-theorem/">central limit theorem</a> <a class="post-tag" href="/tags/chain-of-thought/">chain-of-thought</a> <a class="post-tag" href="/tags/chatgpt/">chatgpt</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/"><div class="card-body"> <span class="timeago small" >Jul 3, 2023<i class="unloaded">2023-07-03T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Building Aligned Intelligence System. Part I: Creating GPT Assistant</h3><div class="text-muted small"><p> In recent years, the field of natural language processing has witnessed a remarkable breakthrough with the advent of Large Language Models (LLMs). These models have demonstrated unprecedented pe...</p></div></div></a></div><div class="card"> <a href="/posts/power-of-diffusion-models/"><div class="card-body"> <span class="timeago small" >Sep 25, 2022<i class="unloaded">2022-09-25T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Power of Diffusion Models</h3><div class="text-muted small"><p> In 2022, insanely beautiful and original images created with generative neural networks are taking the internet by storm. This post focuses on the theory behind diffusion models that underpin th...</p></div></div></a></div><div class="card"> <a href="/posts/exploring-parallel-strategies-with-jax/"><div class="card-body"> <span class="timeago small" >Jan 27<i class="unloaded">2024-01-27T06:00:00+03:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Exploring Parallel Strategies with Jax</h3><div class="text-muted small"><p> Training large language models either like GPT, LlaMa or Mixtral requires immense computational resources. With model sizes ballooning into the billions or sometimes even trillions of parameters...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/building-aligned-intelligence-systems-part-i-creating-gpt-assistant/" class="btn btn-outline-primary" prompt="Older"><p>Building Aligned Intelligence System. Part I: Creating GPT Assistant</p></a> <a href="/posts/exploring-parallel-strategies-with-jax/" class="btn btn-outline-primary" prompt="Newer"><p>Exploring Parallel Strategies with Jax</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/astralord">Aleksandr Samarin</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/jax/">jax</a> <a class="post-tag" href="/tags/parameter-estimation/">parameter estimation</a> <a class="post-tag" href="/tags/asymptotic-efficiency/">asymptotic efficiency</a> <a class="post-tag" href="/tags/bartlett-test/">bartlett test</a> <a class="post-tag" href="/tags/bayes-estimator/">bayes estimator</a> <a class="post-tag" href="/tags/bayesian-inference/">bayesian inference</a> <a class="post-tag" href="/tags/central-limit-theorem/">central limit theorem</a> <a class="post-tag" href="/tags/chain-of-thought/">chain of thought</a> <a class="post-tag" href="/tags/chatgpt/">chatgpt</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://astralord.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script>
